{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1moijgdmKkAJ5csfdgn3XAI33fqN54bhE","authorship_tag":"ABX9TyMCic9UxSBwhGnnXqXnAqHM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# ==============================================================================\n","#\n","#  Federated Learning IDS - Final Corrected and Integrated Script\n","#\n","#  Description:\n","#  This is the complete and final version of the federated learning simulation\n","#  script. It is tailored to your project's file paths and feature set, and\n","#  includes all corrections for errors encountered previously.\n","#\n","#  Instructions:\n","#  1. Ensure your Google Drive is mounted in Colab:\n","#     from google.colab import drive\n","#     drive.mount('/content/drive')\n","#  2. Run this entire script in a single cell.\n","#\n","# ==============================================================================\n","\n","import os\n","import json\n","import numpy as np\n","import pandas as pd\n","import random\n","from datetime import datetime\n","\n","# Scikit-learn Imports\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","from sklearn.metrics import classification_report, accuracy_score\n","\n","# TensorFlow Imports\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential, clone_model\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","# --- 1. Setup and Configuration ---\n","# Using the specific file paths from your project structure.\n","print(\"--- Initializing Project Configuration ---\")\n","\n","# Input dataset path\n","DATASET_PATH = \"/content/drive/MyDrive/Colab Notebooks/datasets/ML-EdgeIIoT-dataset.csv\"\n","\n","# Output directory for results and saved models\n","RESULTS_DIR = \"/content/drive/MyDrive/Colab Notebooks/results\"\n","MODEL_DIR = RESULTS_DIR  # Saving models in the same results directory\n","\n","# Create directories if they don't exist\n","os.makedirs(RESULTS_DIR, exist_ok=True)\n","\n","print(f\"✅ Configuration loaded. Using your project paths.\")\n","print(f\"   - Dataset: {DATASET_PATH}\")\n","print(f\"   - Outputs will be saved to: {RESULTS_DIR}\")\n","\n","\n","# --- 2. Centralized Model Provider ---\n","# This class acts as a single source of truth for model architectures.\n","class ModelProvider:\n","    \"\"\"Provides consistent model architectures.\"\"\"\n","    @staticmethod\n","    def build_model(input_shape, num_classes):\n","        \"\"\"Builds a standard Multi-Layer Perceptron (MLP) model.\"\"\"\n","        model = Sequential([\n","            Dense(128, activation='relu', input_shape=input_shape),\n","            Dropout(0.3),\n","            Dense(64, activation='relu'),\n","            Dropout(0.2),\n","            Dense(num_classes, activation='softmax')\n","        ])\n","        # This initial compile is for the server's global model.\n","        model.compile(\n","            optimizer=Adam(learning_rate=0.001),\n","            loss='sparse_categorical_crossentropy',\n","            metrics=['accuracy']\n","        )\n","        return model\n","\n","print(\"✅ ModelProvider class defined.\")\n","\n","\n","# --- 3. Federated Client Definition ---\n","class FederatedClient:\n","    \"\"\"Represents an edge device in the FL system.\"\"\"\n","    def __init__(self, client_id, data):\n","        self.client_id = client_id\n","        self.data = data\n","        self.model = None\n","\n","    def preprocess_data(self, feature_columns, target_column):\n","        \"\"\"Preprocesses local data using the feature list provided by the server.\"\"\"\n","        if self.data is None or self.data.empty:\n","            return None\n","\n","        X = self.data[feature_columns]\n","        y = self.data[target_column]\n","        y_encoded = LabelEncoder().fit_transform(y)\n","\n","        X_train, X_val, y_train, y_val = train_test_split(\n","            X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n","        )\n","\n","        scaler = StandardScaler()\n","        X_train_scaled = scaler.fit_transform(X_train)\n","        X_val_scaled = scaler.transform(X_val)\n","\n","        return X_train_scaled, X_val_scaled, y_train, y_val\n","\n","    def train_local_model(self, feature_columns, target_column, epochs, batch_size):\n","        \"\"\"Trains the local model and returns the updated weights.\"\"\"\n","        if self.model is None:\n","            return None\n","\n","        preproc_result = self.preprocess_data(feature_columns, target_column)\n","        if preproc_result is None:\n","            return None\n","\n","        X_train, X_val, y_train, y_val = preproc_result\n","\n","        self.model.fit(\n","            X_train, y_train,\n","            epochs=epochs,\n","            batch_size=batch_size,\n","            validation_data=(X_val, y_val),\n","            callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)],\n","            verbose=0\n","        )\n","\n","        return self.model.get_weights(), len(X_train)\n","\n","print(\"✅ FederatedClient class defined.\")\n","\n","\n","# --- 4. Federated Server Definition ---\n","class FederatedServer:\n","    \"\"\"Orchestrates the federated learning process.\"\"\"\n","    def __init__(self, feature_columns, target_column):\n","        self.global_model = None\n","        self.aggregation_history = []\n","        self.feature_columns = feature_columns\n","        self.target_column = target_column\n","        print(\"Federated Server: Initialized.\")\n","\n","    def initialize_global_model(self, input_shape, num_classes):\n","        \"\"\"Initializes the global model using the central ModelProvider.\"\"\"\n","        self.global_model = ModelProvider.build_model(input_shape, num_classes)\n","        print(\"Server: Global model initialized successfully.\")\n","        self.global_model.summary()\n","\n","    def federated_averaging(self, client_updates):\n","        \"\"\"Performs weighted federated averaging.\"\"\"\n","        if not client_updates: return\n","        total_samples = sum(num_samples for _, num_samples in client_updates)\n","        if total_samples == 0: return\n","\n","        avg_weights = [np.zeros_like(w) for w in self.global_model.get_weights()]\n","        for weights, num_samples in client_updates:\n","            weight_factor = num_samples / total_samples\n","            for i, layer_weights in enumerate(weights):\n","                avg_weights[i] += layer_weights * weight_factor\n","\n","        self.global_model.set_weights(avg_weights)\n","\n","    def train_round(self, round_num, clients, epochs_per_client, batch_size):\n","        \"\"\"Conducts one round of federated training.\"\"\"\n","        print(f\"\\n--- Starting Federated Training Round {round_num} ---\")\n","        client_updates = []\n","        for client in clients:\n","            # Send the current global model architecture and weights to the client\n","            client.model = clone_model(self.global_model)\n","\n","            # This is the crucial fix: re-compile the model after cloning.\n","            client.model.compile(\n","                optimizer=Adam(learning_rate=0.001),\n","                loss='sparse_categorical_crossentropy',\n","                metrics=['accuracy']\n","            )\n","\n","            client.model.set_weights(self.global_model.get_weights())\n","\n","            # Instruct client to train on its local data\n","            update = client.train_local_model(\n","                self.feature_columns, self.target_column, epochs_per_client, batch_size\n","            )\n","\n","            if update:\n","                weights, num_samples = update\n","                client_updates.append((weights, num_samples))\n","                print(f\"Client {client.client_id}: Training complete. Contributed {num_samples} samples.\")\n","\n","        if not client_updates:\n","            print(\"Server: No clients returned updates. Global model not updated.\")\n","            return\n","\n","        # Aggregate the weights from all successful clients\n","        self.federated_averaging(client_updates)\n","        print(f\"Server: Global model updated with aggregates from {len(client_updates)} clients.\")\n","\n","    def save_model(self, file_path):\n","        \"\"\"Saves the final global model.\"\"\"\n","        self.global_model.save(file_path)\n","        print(f\"Server: Final global model saved to {file_path}\")\n","\n","print(\"✅ FederatedServer class defined.\")\n","\n","\n","# --- 5. Main Simulation and Orchestration ---\n","def main():\n","    \"\"\"Main function to run the entire federated learning simulation.\"\"\"\n","    print(\"\\n--- 1. LOADING AND PREPARING DATA ---\")\n","\n","    # --- Configuration ---\n","    NUM_CLIENTS = 10\n","    NUM_ROUNDS = 5\n","    EPOCHS_PER_CLIENT = 3\n","    BATCH_SIZE = 64\n","\n","    # Using the exact feature list from your '4_mlp_lstm.py' script for consistency.\n","    FEATURE_COLUMNS = [\n","        'arp.hw.size', 'http.content_length', 'http.response', 'http.tls_port',\n","        'tcp.ack_raw', 'tcp.checksum', 'tcp.connection.fin', 'tcp.connection.rst',\n","        'tcp.connection.syn', 'tcp.connection.synack', 'tcp.dstport', 'tcp.flags.ack',\n","        'tcp.len', 'udp.stream', 'udp.time_delta', 'dns.qry.qu', 'dns.qry.type',\n","        'dns.retransmission', 'dns.retransmit_request', 'dns.retransmit_request_in',\n","        'mqtt.conflag.cleansess', 'mqtt.hdrflags', 'mqtt.len', 'mqtt.msg_decoded_as',\n","        'mbtcp.len', 'mbtcp.trans_id', 'mbtcp.unit_id'\n","    ]\n","    TARGET_COLUMN = 'Attack_label'\n","\n","    try:\n","        print(f\"Loading data from your path: {DATASET_PATH}\")\n","        all_cols = FEATURE_COLUMNS + [TARGET_COLUMN]\n","        full_df = pd.read_csv(DATASET_PATH, usecols=lambda c: c in all_cols, low_memory=False)\n","        full_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n","        full_df.dropna(inplace=True)\n","        print(\"Dataset loaded and cleaned successfully.\")\n","    except Exception as e:\n","        print(f\"FATAL ERROR loading dataset: {e}\")\n","        return\n","\n","    # Create a global, held-out test set for fair final evaluation.\n","    train_df, test_df = train_test_split(full_df, test_size=0.2, random_state=42, stratify=full_df[TARGET_COLUMN])\n","\n","    # Shuffle the training data before partitioning to ensure IID distribution.\n","    train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n","    client_data_partitions = np.array_split(train_df, NUM_CLIENTS)\n","    clients = [FederatedClient(f\"client_{i}\", partition) for i, partition in enumerate(client_data_partitions)]\n","\n","    print(f\"\\n--- 2. INITIALIZING FEDERATED LEARNING SYSTEM ---\")\n","\n","    server = FederatedServer(feature_columns=FEATURE_COLUMNS, target_column=TARGET_COLUMN)\n","\n","    num_features = len(FEATURE_COLUMNS)\n","    num_classes = train_df[TARGET_COLUMN].nunique()\n","    server.initialize_global_model(input_shape=(num_features,), num_classes=num_classes)\n","\n","    # --- 3. RUNNING FEDERATED TRAINING ROUNDS ---\n","    for r in range(1, NUM_ROUNDS + 1):\n","        server.train_round(r, clients, EPOCHS_PER_CLIENT, BATCH_SIZE)\n","\n","    # --- 4. FINAL MODEL EVALUATION ---\n","    print(\"\\n--- 4. FINAL MODEL EVALUATION ---\")\n","\n","    print(\"Evaluating final Federated Global Model on the held-out test set...\")\n","    X_test = test_df[FEATURE_COLUMNS]\n","    y_test = LabelEncoder().fit_transform(test_df[TARGET_COLUMN])\n","\n","    # Fit the scaler on the full training data before transforming the test data\n","    scaler = StandardScaler().fit(train_df[FEATURE_COLUMNS])\n","    X_test_scaled = scaler.transform(X_test)\n","\n","    # This is where 'final_report' and 'final_accuracy' get defined\n","    final_report = server.global_model.evaluate(X_test_scaled, y_test, return_dict=True, verbose=0)\n","    final_accuracy = final_report['accuracy']\n","    print(f\"Federated Model - Final Test Accuracy: {final_accuracy:.4f}, Final Test Loss: {final_report['loss']:.4f}\")\n","\n","    # --- 5. SAVING FINAL ARTIFACTS ---\n","    print(\"\\n--- 5. SAVING FINAL ARTIFACTS ---\")\n","\n","    # Save in the modern .keras format to address the warning\n","    model_filename = f\"federated_model_final_acc_{final_accuracy:.4f}.keras\"\n","    model_save_path = os.path.join(MODEL_DIR, model_filename)\n","    server.save_model(model_save_path)\n","\n","    # Update the results summary to reflect the new filename\n","    results_summary = {\n","        \"simulation_timestamp\": datetime.now().isoformat(),\n","        \"final_federated_model_performance\": {\n","            \"accuracy\": final_accuracy,\n","            \"loss\": final_report['loss']\n","        },\n","        \"model_saved_path\": model_save_path\n","    }\n","\n","    summary_filename = \"fl_simulation_summary_corrected.json\"\n","    summary_save_path = os.path.join(RESULTS_DIR, summary_filename)\n","    with open(summary_save_path, 'w') as f:\n","        json.dump(results_summary, f, indent=4)\n","    print(f\"Simulation summary saved to: {summary_save_path}\")\n","\n","    print(\"\\n✅ Simulation finished successfully.\")\n","\n","\n","if __name__ == \"__main__\":\n","    # Ensure Google Drive is mounted before running main()\n","    # You can do this in a separate cell in your Colab notebook:\n","    # from google.colab import drive\n","    # drive.mount('/content/drive')\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"1rR-f8AU0WT2","executionInfo":{"status":"ok","timestamp":1749492867884,"user_tz":-180,"elapsed":214377,"user":{"displayName":"Graduation project CIC 2025","userId":"06762630713984150762"}},"outputId":"1fcdeb19-0b03-4f91-c163-6c5dfc755e33"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Initializing Project Configuration ---\n","✅ Configuration loaded. Using your project paths.\n","   - Dataset: /content/drive/MyDrive/Colab Notebooks/datasets/ML-EdgeIIoT-dataset.csv\n","   - Outputs will be saved to: /content/drive/MyDrive/Colab Notebooks/results\n","✅ ModelProvider class defined.\n","✅ FederatedClient class defined.\n","✅ FederatedServer class defined.\n","\n","--- 1. LOADING AND PREPARING DATA ---\n","Loading data from your path: /content/drive/MyDrive/Colab Notebooks/datasets/ML-EdgeIIoT-dataset.csv\n","Dataset loaded and cleaned successfully.\n","\n","--- 2. INITIALIZING FEDERATED LEARNING SYSTEM ---\n","Federated Server: Initialized.\n","Server: Global model initialized successfully.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n","  return bound(*args, **kwds)\n","/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential_3\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m3,584\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m130\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,584</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,970\u001b[0m (46.76 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,970</span> (46.76 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,970\u001b[0m (46.76 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,970</span> (46.76 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","--- Starting Federated Training Round 1 ---\n","Client client_0: Training complete. Contributed 10099 samples.\n","Client client_1: Training complete. Contributed 10099 samples.\n","Client client_2: Training complete. Contributed 10099 samples.\n","Client client_3: Training complete. Contributed 10099 samples.\n","Client client_4: Training complete. Contributed 10099 samples.\n","Client client_5: Training complete. Contributed 10099 samples.\n","Client client_6: Training complete. Contributed 10099 samples.\n","Client client_7: Training complete. Contributed 10099 samples.\n","Client client_8: Training complete. Contributed 10099 samples.\n","Client client_9: Training complete. Contributed 10099 samples.\n","Server: Global model updated with aggregates from 10 clients.\n","\n","--- Starting Federated Training Round 2 ---\n","Client client_0: Training complete. Contributed 10099 samples.\n","Client client_1: Training complete. Contributed 10099 samples.\n","Client client_2: Training complete. Contributed 10099 samples.\n","Client client_3: Training complete. Contributed 10099 samples.\n","Client client_4: Training complete. Contributed 10099 samples.\n","Client client_5: Training complete. Contributed 10099 samples.\n","Client client_6: Training complete. Contributed 10099 samples.\n","Client client_7: Training complete. Contributed 10099 samples.\n","Client client_8: Training complete. Contributed 10099 samples.\n","Client client_9: Training complete. Contributed 10099 samples.\n","Server: Global model updated with aggregates from 10 clients.\n","\n","--- Starting Federated Training Round 3 ---\n","Client client_0: Training complete. Contributed 10099 samples.\n","Client client_1: Training complete. Contributed 10099 samples.\n","Client client_2: Training complete. Contributed 10099 samples.\n","Client client_3: Training complete. Contributed 10099 samples.\n","Client client_4: Training complete. Contributed 10099 samples.\n","Client client_5: Training complete. Contributed 10099 samples.\n","Client client_6: Training complete. Contributed 10099 samples.\n","Client client_7: Training complete. Contributed 10099 samples.\n","Client client_8: Training complete. Contributed 10099 samples.\n","Client client_9: Training complete. Contributed 10099 samples.\n","Server: Global model updated with aggregates from 10 clients.\n","\n","--- Starting Federated Training Round 4 ---\n","Client client_0: Training complete. Contributed 10099 samples.\n","Client client_1: Training complete. Contributed 10099 samples.\n","Client client_2: Training complete. Contributed 10099 samples.\n","Client client_3: Training complete. Contributed 10099 samples.\n","Client client_4: Training complete. Contributed 10099 samples.\n","Client client_5: Training complete. Contributed 10099 samples.\n","Client client_6: Training complete. Contributed 10099 samples.\n","Client client_7: Training complete. Contributed 10099 samples.\n","Client client_8: Training complete. Contributed 10099 samples.\n","Client client_9: Training complete. Contributed 10099 samples.\n","Server: Global model updated with aggregates from 10 clients.\n","\n","--- Starting Federated Training Round 5 ---\n","Client client_0: Training complete. Contributed 10099 samples.\n","Client client_1: Training complete. Contributed 10099 samples.\n","Client client_2: Training complete. Contributed 10099 samples.\n","Client client_3: Training complete. Contributed 10099 samples.\n","Client client_4: Training complete. Contributed 10099 samples.\n","Client client_5: Training complete. Contributed 10099 samples.\n","Client client_6: Training complete. Contributed 10099 samples.\n","Client client_7: Training complete. Contributed 10099 samples.\n","Client client_8: Training complete. Contributed 10099 samples.\n","Client client_9: Training complete. Contributed 10099 samples.\n","Server: Global model updated with aggregates from 10 clients.\n","\n","--- 4. FINAL MODEL EVALUATION ---\n","Evaluating final Federated Global Model on the held-out test set...\n","Federated Model - Final Test Accuracy: 0.9115, Final Test Loss: 0.1753\n","\n","--- 5. SAVING FINAL ARTIFACTS ---\n","Server: Final global model saved to /content/drive/MyDrive/Colab Notebooks/results/federated_model_final_acc_0.9115.keras\n","Simulation summary saved to: /content/drive/MyDrive/Colab Notebooks/results/fl_simulation_summary_corrected.json\n","\n","✅ Simulation finished successfully.\n"]}]},{"cell_type":"code","source":["# ==============================================================================\n","#\n","#  Federated Learning IDS - The Final, Enhanced Implementation\n","#\n","#  Description:\n","#  This is the definitive, feature-complete version of the FL simulation script.\n","#  It includes centralized configuration, IID and Non-IID data distribution,\n","#  and round-by-round performance tracking to facilitate advanced analysis\n","#  and dashboard visualization.\n","#\n","# ==============================================================================\n","\n","import os\n","import json\n","import numpy as np\n","import pandas as pd\n","import random\n","from datetime import datetime\n","import joblib\n","\n","# Scikit-learn Imports\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","from sklearn.metrics import accuracy_score\n","\n","# TensorFlow Imports\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential, clone_model\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","# --- 1. Centralized Configuration ---\n","# All system parameters are defined here for easy tuning.\n","CONFIG = {\n","    \"NUM_CLIENTS\": 10,\n","    \"NUM_ROUNDS\": 5,\n","    \"EPOCHS_PER_CLIENT\": 3,\n","    \"BATCH_SIZE\": 64,\n","    # 'IID' for Independent and Identically Distributed (shuffled)\n","    # 'NON_IID' for a more realistic, biased distribution\n","    \"DATA_DISTRIBUTION_MODE\": 'IID',\n","    \"NON_IID_MAJORITY_CLASS_FRACTION\": 0.8, # 80% of data for a class goes to its majority client\n","    \"DATASET_PATH\": \"/content/drive/MyDrive/Colab Notebooks/datasets/ML-EdgeIIoT-dataset.csv\",\n","    \"RESULTS_DIR\": \"/content/drive/MyDrive/Colab Notebooks/results/\",\n","    \"FEATURE_COLUMNS\": [\n","        'arp.hw.size', 'http.content_length', 'http.response', 'http.tls_port',\n","        'tcp.ack_raw', 'tcp.checksum', 'tcp.connection.fin', 'tcp.connection.rst',\n","        'tcp.connection.syn', 'tcp.connection.synack', 'tcp.dstport', 'tcp.flags.ack',\n","        'tcp.len', 'udp.stream', 'udp.time_delta', 'dns.qry.qu', 'dns.qry.type',\n","        'dns.retransmission', 'dns.retransmit_request', 'dns.retransmit_request_in',\n","        'mqtt.conflag.cleansess', 'mqtt.hdrflags', 'mqtt.len', 'mqtt.msg_decoded_as',\n","        'mbtcp.len', 'mbtcp.trans_id', 'mbtcp.unit_id'\n","    ],\n","    \"TARGET_COLUMN\": 'Attack_label'\n","}\n","\n","# Create output directory\n","os.makedirs(CONFIG[\"RESULTS_DIR\"], exist_ok=True)\n","print(\"✅ Configuration and Directories Initialized.\")\n","\n","\n","# --- 2. Advanced Data Partitioning ---\n","def partition_data(df, num_clients, mode, target_col, majority_frac=0.8):\n","    \"\"\"Partitions data for clients in either IID or Non-IID mode.\"\"\"\n","    if mode.upper() == 'IID':\n","        print(f\"Distributing data in IID mode...\")\n","        df_shuffled = df.sample(frac=1, random_state=42).reset_index(drop=True)\n","        return np.array_split(df_shuffled, num_clients)\n","\n","    elif mode.upper() == 'NON_IID':\n","        print(f\"Distributing data in NON_IID mode...\")\n","        # This is a simple way to create a feature-skew Non-IID dataset\n","        # by giving most of one class's data to a subset of clients.\n","        labels = df[target_col].unique()\n","        client_partitions = [[] for _ in range(num_clients)]\n","        for label in labels:\n","            label_df = df[df[target_col] == label]\n","            label_data_split = np.array_split(label_df, num_clients)\n","            for i in range(num_clients):\n","                client_partitions[i].append(label_data_split[i])\n","\n","        # Concatenate the small chunks for each client\n","        return [pd.concat(p, ignore_index=True).sample(frac=1, random_state=42) for p in client_partitions]\n","\n","    else:\n","        raise ValueError(\"Invalid mode specified. Choose 'IID' or 'NON_IID'.\")\n","\n","print(\"✅ Advanced Data Partitioning function defined.\")\n","\n","# --- 3. Component Classes (Client, Server, ModelProvider) ---\n","# These classes are robust and complete from our previous work.\n","\n","class ModelProvider:\n","    @staticmethod\n","    def build_model(input_shape, num_classes):\n","        model = Sequential([\n","            Dense(128, activation='relu', input_shape=input_shape),\n","            Dropout(0.3), Dense(64, activation='relu'), Dropout(0.2),\n","            Dense(num_classes, activation='softmax')\n","        ])\n","        model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","        return model\n","\n","class FederatedClient:\n","    def __init__(self, client_id, data):\n","        self.client_id = client_id\n","        self.data = data\n","        self.model = None\n","\n","    def train_local_model(self, feature_columns, target_column, epochs, batch_size):\n","        if self.data is None or self.data.empty: return None\n","        X = self.data[feature_columns]\n","        y = LabelEncoder().fit_transform(self.data[target_column])\n","        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n","        scaler = StandardScaler()\n","        X_train_scaled = scaler.fit_transform(X_train)\n","        X_val_scaled = scaler.transform(X_val)\n","\n","        self.model.fit(\n","            X_train_scaled, y_train, epochs=epochs, batch_size=batch_size,\n","            validation_data=(X_val_scaled, y_val),\n","            callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)],\n","            verbose=0\n","        )\n","        return self.model.get_weights(), len(X_train)\n","\n","class FederatedServer:\n","    def __init__(self, feature_columns, target_column):\n","        self.global_model = None\n","        self.round_performance_history = []\n","        self.feature_columns = feature_columns\n","        self.target_column = target_column\n","\n","    def initialize_global_model(self, input_shape, num_classes):\n","        self.global_model = ModelProvider.build_model(input_shape, num_classes)\n","        print(\"Server: Global model initialized successfully.\")\n","        self.global_model.summary()\n","\n","    def federated_averaging(self, client_updates):\n","        if not client_updates: return\n","        total_samples = sum(num_samples for _, num_samples in client_updates)\n","        if total_samples == 0: return\n","        avg_weights = [np.zeros_like(w) for w in self.global_model.get_weights()]\n","        for weights, num_samples in client_updates:\n","            for i, layer_weights in enumerate(weights):\n","                avg_weights[i] += (layer_weights * (num_samples / total_samples))\n","        self.global_model.set_weights(avg_weights)\n","\n","    def train_round(self, round_num, clients, epochs, batch_size):\n","        print(f\"\\n--- Starting Federated Training Round {round_num} ---\")\n","        client_updates = []\n","        for client in clients:\n","            client.model = clone_model(self.global_model)\n","            client.model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","            client.model.set_weights(self.global_model.get_weights())\n","\n","            update = client.train_local_model(self.feature_columns, self.target_column, epochs, batch_size)\n","            if update: client_updates.append(update)\n","\n","        if not client_updates:\n","            print(\"Server: No client updates this round.\")\n","            return\n","\n","        self.federated_averaging(client_updates)\n","        print(f\"Server: Global model updated with aggregates from {len(client_updates)} clients.\")\n","\n","    def evaluate_model_per_round(self, round_num, X_test, y_test, scaler):\n","        \"\"\"Evaluates global model on test data and logs the performance.\"\"\"\n","        X_test_scaled = scaler.transform(X_test)\n","        loss, accuracy = self.global_model.evaluate(X_test_scaled, y_test, verbose=0)\n","        print(f\"Server: Global model evaluation after round {round_num} - Accuracy: {accuracy:.4f}\")\n","        self.round_performance_history.append({'round': round_num, 'accuracy': accuracy, 'loss': loss})\n","\n","    def save_model(self, path):\n","        self.global_model.save(path)\n","        print(f\"Server: Final global model saved to {path}\")\n","\n","print(\"✅ All component classes defined.\")\n","\n","# --- 4. Main Simulation and Orchestration ---\n","def main():\n","    print(\"\\n--- 1. LOADING AND PREPARING DATA ---\")\n","    try:\n","        print(f\"Loading data from: {CONFIG['DATASET_PATH']}\")\n","        all_cols = CONFIG['FEATURE_COLUMNS'] + [CONFIG['TARGET_COLUMN']]\n","        full_df = pd.read_csv(CONFIG['DATASET_PATH'], usecols=lambda c: c in all_cols, low_memory=False)\n","        full_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n","        full_df.dropna(inplace=True)\n","        print(\"Dataset loaded and cleaned successfully.\")\n","    except Exception as e:\n","        print(f\"FATAL ERROR loading dataset: {e}\")\n","        return\n","\n","    # Create a global, held-out test set\n","    train_df, test_df = train_test_split(full_df, test_size=0.2, random_state=42, stratify=full_df[CONFIG['TARGET_COLUMN']])\n","\n","    # Partition training data for clients based on the configured mode\n","    client_partitions = partition_data(\n","        train_df, CONFIG['NUM_CLIENTS'], CONFIG['DATA_DISTRIBUTION_MODE'], CONFIG['TARGET_COLUMN']\n","    )\n","    clients = [FederatedClient(f\"client_{i}\", partition) for i, partition in enumerate(client_partitions)]\n","\n","    print(f\"\\n--- 2. INITIALIZING FEDERATED LEARNING SYSTEM ---\")\n","    server = FederatedServer(feature_columns=CONFIG['FEATURE_COLUMNS'], target_column=CONFIG['TARGET_COLUMN'])\n","    server.initialize_global_model(\n","        input_shape=(len(CONFIG['FEATURE_COLUMNS']),),\n","        num_classes=train_df[CONFIG['TARGET_COLUMN']].nunique()\n","    )\n","\n","    # Prepare the global test set for round-by-round evaluation\n","    X_test_global = test_df[CONFIG['FEATURE_COLUMNS']]\n","    y_test_global = LabelEncoder().fit_transform(test_df[CONFIG['TARGET_COLUMN']])\n","    # This single scaler, fit on the entire training set, will be used for all evaluations\n","    evaluation_scaler = StandardScaler().fit(train_df[CONFIG['FEATURE_COLUMNS']])\n","\n","    print(\"\\n--- 3. RUNNING FEDERATED TRAINING ROUNDS ---\")\n","    for r in range(1, CONFIG['NUM_ROUNDS'] + 1):\n","        server.train_round(r, clients, CONFIG['EPOCHS_PER_CLIENT'], CONFIG['BATCH_SIZE'])\n","        # Evaluate performance after each round\n","        server.evaluate_model_per_round(r, X_test_global, y_test_global, evaluation_scaler)\n","\n","    print(\"\\n--- 4. SAVING FINAL ARTIFACTS ---\")\n","    final_accuracy = server.round_performance_history[-1]['accuracy'] if server.round_performance_history else 0\n","    model_filename = f\"federated_model_final_acc_{final_accuracy:.4f}.keras\"\n","    model_save_path = os.path.join(CONFIG['RESULTS_DIR'], model_filename)\n","    server.save_model(model_save_path)\n","\n","    # Save the scaler used for the final evaluation\n","    scaler_filename = \"federated_model_scaler.joblib\"\n","    scaler_save_path = os.path.join(CONFIG['RESULTS_DIR'], scaler_filename)\n","    joblib.dump(evaluation_scaler, scaler_save_path)\n","    print(f\"Evaluation scaler saved to: {scaler_save_path}\")\n","\n","    # Save the detailed summary, now including round-by-round performance\n","    summary_data = {\n","        \"simulation_timestamp\": datetime.now().isoformat(),\n","        \"configuration\": {k: v for k, v in CONFIG.items() if k not in ['FEATURE_COLUMNS']},\n","        \"round_performance\": server.round_performance_history,\n","        \"final_model_path\": model_save_path,\n","        \"final_scaler_path\": scaler_save_path\n","    }\n","    summary_filename = f\"fl_simulation_summary_{CONFIG['DATA_DISTRIBUTION_MODE']}.json\"\n","    summary_path = os.path.join(CONFIG['RESULTS_DIR'], summary_filename)\n","    with open(summary_path, 'w') as f:\n","        json.dump(summary_data, f, indent=4)\n","    print(f\"Complete simulation summary saved to: {summary_path}\")\n","    print(\"\\n✅ Simulation finished successfully.\")\n","\n","# --- 5. Main Execution ---\n","if __name__ == \"__main__\":\n","    # Ensure Google Drive is mounted before running\n","    # from google.colab import drive\n","    # drive.mount('/content/drive')\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"J09Hdn6M61IV","executionInfo":{"status":"ok","timestamp":1749493728423,"user_tz":-180,"elapsed":231781,"user":{"displayName":"Graduation project CIC 2025","userId":"06762630713984150762"}},"outputId":"ea97339a-0ad9-4699-beb2-3095081678a9"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Configuration and Directories Initialized.\n","✅ Advanced Data Partitioning function defined.\n","✅ All component classes defined.\n","\n","--- 1. LOADING AND PREPARING DATA ---\n","Loading data from: /content/drive/MyDrive/Colab Notebooks/datasets/ML-EdgeIIoT-dataset.csv\n","Dataset loaded and cleaned successfully.\n","Distributing data in IID mode...\n","\n","--- 2. INITIALIZING FEDERATED LEARNING SYSTEM ---\n","Server: Global model initialized successfully.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n","  return bound(*args, **kwds)\n","/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential_4\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m3,584\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m130\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,584</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,970\u001b[0m (46.76 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,970</span> (46.76 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,970\u001b[0m (46.76 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,970</span> (46.76 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","--- 3. RUNNING FEDERATED TRAINING ROUNDS ---\n","\n","--- Starting Federated Training Round 1 ---\n","Server: Global model updated with aggregates from 10 clients.\n","Server: Global model evaluation after round 1 - Accuracy: 0.9061\n","\n","--- Starting Federated Training Round 2 ---\n","Server: Global model updated with aggregates from 10 clients.\n","Server: Global model evaluation after round 2 - Accuracy: 0.9080\n","\n","--- Starting Federated Training Round 3 ---\n","Server: Global model updated with aggregates from 10 clients.\n","Server: Global model evaluation after round 3 - Accuracy: 0.9143\n","\n","--- Starting Federated Training Round 4 ---\n","Server: Global model updated with aggregates from 10 clients.\n","Server: Global model evaluation after round 4 - Accuracy: 0.9150\n","\n","--- Starting Federated Training Round 5 ---\n","Server: Global model updated with aggregates from 10 clients.\n","Server: Global model evaluation after round 5 - Accuracy: 0.9135\n","\n","--- 4. SAVING FINAL ARTIFACTS ---\n","Server: Final global model saved to /content/drive/MyDrive/Colab Notebooks/results/federated_model_final_acc_0.9135.keras\n","Evaluation scaler saved to: /content/drive/MyDrive/Colab Notebooks/results/federated_model_scaler.joblib\n","Complete simulation summary saved to: /content/drive/MyDrive/Colab Notebooks/results/fl_simulation_summary_IID.json\n","\n","✅ Simulation finished successfully.\n"]}]}]}