{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"gpuType":"V28","mount_file_id":"1ffxiC7Qj0E5fez9fphfLVE-POGDDE1IB","authorship_tag":"ABX9TyOUs1AAQiJ1BmHaijyoH0na"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","source":["import joblib"],"metadata":{"id":"Jiae9FyXKN-y","executionInfo":{"status":"ok","timestamp":1748725461788,"user_tz":-180,"elapsed":128,"user":{"displayName":"Graduation project CIC 2025","userId":"06762630713984150762"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from sklearn.metrics import classification_report, accuracy_score\n","from sklearn.preprocessing import StandardScaler\n","import os\n","import joblib\n","import pickle # For .pkl saving and serializing objects to bytes for H5\n","import h5py # For HDF5 file handling\n","\n","# Define the path to the dataset and output\n","DATASET_PATH = \"/content/drive/MyDrive/Colab Notebooks/datasets/ML-EdgeIIoT-dataset.csv\"\n","OUTPUT_PATH = \"/content/drive/MyDrive/Colab Notebooks/results\"\n","os.makedirs(OUTPUT_PATH, exist_ok=True)\n","\n","# Define path for the saved model (joblib)\n","MODEL_SAVE_PATH_JOBLIB = os.path.join(OUTPUT_PATH, \"svm_model.joblib\")\n","SCALER_SAVE_PATH_JOBLIB = os.path.join(OUTPUT_PATH, \"svm_scaler.joblib\")\n","\n","# Define path for the saved model (HDF5)\n","MODEL_SAVE_PATH_H5 = os.path.join(OUTPUT_PATH, \"svm_model_and_scaler.h5\")\n","\n","# Define path for the saved model (pickle .pkl)\n","MODEL_SAVE_PATH_PKL = os.path.join(OUTPUT_PATH, \"svm_model.pkl\")\n","SCALER_SAVE_PATH_PKL = os.path.join(OUTPUT_PATH, \"svm_scaler.pkl\")\n","\n","\n","print(f\"Loading dataset: {DATASET_PATH}\")\n","df = pd.read_csv(DATASET_PATH, low_memory=False)\n","print(\"Dataset loaded successfully.\")\n","\n","# Preprocessing\n","df.replace([float(\"inf\"), float(\"-inf\")], pd.NA, inplace=True)\n","df.dropna(inplace=True)\n","print(f\"Dataset shape after dropping NA: {df.shape}\")\n","\n","if df.empty:\n","    print(\"Dataset is empty after dropping NA values. Exiting.\")\n","    exit()\n","\n","# Feature Selection\n","drop_cols = [\"frame.time\", \"ip.src_host\", \"ip.dst_host\", \"arp.src.proto_ipv4\",\n","             \"arp.dst.proto_ipv4\", \"http.file_data\", \"http.request.full_uri\",\n","             \"icmp.transmit_timestamp\", \"tcp.options\", \"tcp.payload\",\n","             \"mqtt.conack.flags\", \"mqtt.msg\", \"mqtt.protoname\", \"mqtt.topic\",\n","             \"mqtt.uuid\", \"mqtt.conflags\",\n","             \"Attack_label\", \"Attack_type\", \"Label\",\n","             \"icmp.unused\", \"http.request.method\", \"http.referer\", \"http.request.version\",\n","             \"dns.qry.name\", \"dns.resp.name\", \"tcp.flags\", \"udp.port\", \"tcp.port\",\n","             \"mqtt.conack.flags_tree\",\n","             \"tcp.options.mss\", \"tcp.window_size\", \"tcp.hdr_len\", \"tcp.seq\", \"tcp.ack\",\n","             \"ip.src\", \"ip.dst\", \"arp.opcode\", \"arp.hw.type\", \"arp.src.hw_mac\",\n","             \"arp.dst.hw_mac\", \"icmp.type\", \"icmp.code\", \"icmp.checksum\",\n","             \"icmp.ident\", \"icmp.seq_le\", \"udp.srcport\", \"udp.dstport\", \"udp.checksum\",\n","             \"dns.id\", \"dns.flags.response\", \"dns.flags.opcode\", \"dns.flags.authoritative\",\n","             \"dns.flags.truncated\", \"dns.flags.recursion_desired\", \"dns.flags.recursion_available\",\n","             \"dns.flags.z\", \"dns.flags.authenticated\", \"dns.flags.checking_disabled\", \"dns.flags.rcode\",\n","             \"dns.count.queries\", \"dns.count.answers\", \"dns.count.auth_rr\", \"dns.count.add_rr\",\n","             \"mqtt.clientid\", \"mqtt.qos\", \"mqtt.retain\", \"mqtt.dupflag\", \"mqtt.sessionpresent\",\n","             \"mqtt.proto_len\", \"mqtt.topic_len\", \"mqtt.ver\", \"mqtt.willmsg\", \"mqtt.willtopic\",\n","             \"mqtt.dup\", \"mqtt.msgtype\", \"mqtt.kalive\", \"mqtt.msgid\", \"mqtt.password\",\n","             \"mqtt.username\", \"mqtt.client_id_len\",\n","             \"mqtt.topic_val\", \"mqtt.msg_len\",\n","             \"mqtt.payload\", \"mqtt.ciphersuite\", \"mqtt.pk_id\", \"mqtt.reason_code\", \"mqtt.session_expiry_interval\",\n","             \"mqtt.will_flag\", \"mqtt.will_qos\", \"mqtt.will_retain\", \"mqtt.will_message_len\", \"mqtt.will_message\",\n","             \"mqtt.will_topic_len\", \"mqtt.will_topic\", \"mqtt.var_header.length\", \"mqtt.var_header.qos\",\n","             \"mqtt.var_header.retain\", \"mqtt.var_header.dup\", \"mqtt.var_header.message_identifier\",\n","             \"mqtt.var_header.topic_name_length\", \"mqtt.var_header.topic_name\", \"mqtt.var_header.packet_identifier\",\n","             \"mqtt.var_header.properties.message_expiry_interval\", \"mqtt.var_header.properties.content_type\",\n","             \"mqtt.var_header.properties.correlation_data\", \"mqtt.var_header.properties.payload_format_indicator\",\n","             \"mqtt.var_header.properties.request_response_information\", \"mqtt.var_header.properties.response_topic\",\n","             \"mqtt.var_header.properties.session_expiry_interval\", \"mqtt.var_header.properties.subscription_identifier\",\n","             \"mqtt.var_header.properties.topic_alias\", \"mqtt.var_header.properties.user_property\",\n","             \"mqtt.var_header.properties.will_delay_interval\", \"mqtt.var_header.properties.will_payload_format_indicator\",\n","             \"mqtt.var_header.properties.will_content_type\", \"mqtt.var_header.properties.will_response_topic\",\n","             \"mqtt.var_header.properties.will_correlation_data\", \"mqtt.var_header.properties.will_user_property\",\n","             \"mqtt.var_header.properties.will_subscription_identifier\", \"mqtt.var_header.properties.will_topic_alias\",\n","             \"mqtt.var_header.properties.will_retained_message\", \"mqtt.var_header.properties.will_message_expiry_interval\",\n","             \"mqtt.var_header.properties.will_content_type_len\", \"mqtt.var_header.properties.will_content_type_val\",\n","             \"mqtt.var_header.properties.will_response_topic_len\", \"mqtt.var_header.properties.will_response_topic_val\",\n","             \"mqtt.var_header.properties.will_correlation_data_len\", \"mqtt.var_header.properties.will_correlation_data_val\",\n","             \"mqtt.var_header.properties.will_user_property_len\", \"mqtt.var_header.properties.will_user_property_val\",\n","             \"mqtt.var_header.properties.will_subscription_identifier_len\", \"mqtt.var_header.properties.will_subscription_identifier_val\",\n","             \"mqtt.var_header.properties.will_topic_alias_len\", \"mqtt.var_header.properties.will_topic_alias_val\",\n","             \"mqtt.var_header.properties.will_retained_message_len\", \"mqtt.var_header.properties.will_retained_message_val\",\n","             \"mqtt.var_header.properties.will_message_expiry_interval_len\", \"mqtt.var_header.properties.will_message_expiry_interval_val\"\n","             ]\n","drop_cols = sorted(list(set(drop_cols)))\n","\n","\n","if \"Attack_label\" in df.columns:\n","    y = df[\"Attack_label\"]\n","    cols_to_drop_for_X = [\"Attack_label\", \"Attack_type\", \"Label\"] + [col for col in drop_cols if col in df.columns]\n","    cols_to_drop_for_X = sorted(list(set(cols_to_drop_for_X)))\n","    X_candidate_features = df.drop(columns=cols_to_drop_for_X, errors=\"ignore\")\n","    X = X_candidate_features.select_dtypes(include=np.number)\n","else:\n","    print(\"Target variable Attack_label not found.\")\n","    exit()\n","\n","if X.empty:\n","    print(\"No numeric features available after selection. Exiting.\")\n","    exit()\n","\n","print(f\"Selected features for training: {X.columns.tolist()}\")\n","print(f\"Number of selected features: {len(X.columns)}\")\n","\n","\n","# Data Splitting\n","if y.empty or len(y.unique()) < 2:\n","    print(\"Target variable y is empty or has only one class. Stratified splitting not possible.\")\n","    exit()\n","\n","X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n","X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=(0.15/0.85), random_state=42, stratify=y_train_val)\n","\n","# Feature Scaling\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_val = scaler.transform(X_val)\n","X_test = scaler.transform(X_test)\n","\n","# --- Save Scaler in all formats ---\n","# Joblib\n","joblib.dump(scaler, SCALER_SAVE_PATH_JOBLIB)\n","print(f\"Scaler saved to {SCALER_SAVE_PATH_JOBLIB} (joblib format)\")\n","# Pickle\n","with open(SCALER_SAVE_PATH_PKL, 'wb') as f:\n","    pickle.dump(scaler, f)\n","print(f\"Scaler saved to {SCALER_SAVE_PATH_PKL} (pickle .pkl format)\")\n","\n","\n","print(f\"Training set size: {X_train.shape[0]}\")\n","print(f\"Validation set size: {X_val.shape[0]}\")\n","print(f\"Test set size: {X_test.shape[0]}\")\n","\n","# Initialize and train SVM Classifier\n","print(\"Training SVM model...\")\n","svm_model = SVC(kernel=\"rbf\", C=1, gamma=\"scale\", random_state=42, probability=True)\n","svm_model.fit(X_train, y_train)\n","print(\"SVM Model training complete.\")\n","\n","# --- Save Model in all formats ---\n","# Joblib\n","joblib.dump(svm_model, MODEL_SAVE_PATH_JOBLIB)\n","print(f\"Trained SVM model saved to {MODEL_SAVE_PATH_JOBLIB} (joblib format)\")\n","# Pickle\n","with open(MODEL_SAVE_PATH_PKL, 'wb') as f:\n","    pickle.dump(svm_model, f)\n","print(f\"Trained SVM model saved to {MODEL_SAVE_PATH_PKL} (pickle .pkl format)\")\n","\n","# HDF5 (containing pickled model and scaler)\n","print(f\"Saving SVM model and scaler to HDF5 format: {MODEL_SAVE_PATH_H5}...\")\n","try:\n","    model_bytes = pickle.dumps(svm_model)\n","    scaler_bytes = pickle.dumps(scaler) # Already done above, but for clarity in this block\n","\n","    with h5py.File(MODEL_SAVE_PATH_H5, 'w') as h5f:\n","        h5f.create_dataset('svm_model', data=np.void(model_bytes))\n","        h5f.create_dataset('scaler', data=np.void(scaler_bytes))\n","        h5f.attrs['feature_names'] = pickle.dumps(X.columns.tolist())\n","\n","    print(f\"SVM model and scaler successfully saved to {MODEL_SAVE_PATH_H5}\")\n","except Exception as e:\n","    print(f\"Error saving to HDF5: {e}\")\n","\n","\n","# Evaluate on validation set\n","print(\"Evaluating SVM on validation set...\")\n","y_val_pred_svm = svm_model.predict(X_val)\n","val_accuracy_svm = accuracy_score(y_val, y_val_pred_svm)\n","print(f\"SVM Validation Accuracy: {val_accuracy_svm:.4f}\")\n","print(\"SVM Validation Classification Report:\")\n","print(classification_report(y_val, y_val_pred_svm, zero_division=0))\n","\n","# Evaluate on test set\n","print(\"Evaluating SVM on test set...\")\n","y_test_pred_svm = svm_model.predict(X_test)\n","test_accuracy_svm = accuracy_score(y_test, y_test_pred_svm)\n","print(f\"SVM Test Accuracy: {test_accuracy_svm:.4f}\")\n","print(\"SVM Test Classification Report:\")\n","print(classification_report(y_test, y_test_pred_svm, zero_division=0))\n","\n","# Save the results\n","results_summary_svm = f\"SVM Model Results:\\n\"\n","results_summary_svm += f\"Models saved to:\\n\"\n","results_summary_svm += f\"  Joblib (model): {MODEL_SAVE_PATH_JOBLIB}\\n\"\n","results_summary_svm += f\"  Joblib (scaler): {SCALER_SAVE_PATH_JOBLIB}\\n\"\n","results_summary_svm += f\"  Pickle (model): {MODEL_SAVE_PATH_PKL}\\n\"\n","results_summary_svm += f\"  Pickle (scaler): {SCALER_SAVE_PATH_PKL}\\n\"\n","results_summary_svm += f\"  HDF5 (model & scaler pickled bytes): {MODEL_SAVE_PATH_H5}\\n\\n\"\n","results_summary_svm += f\"Validation Accuracy: {val_accuracy_svm:.4f}\\n\"\n","results_summary_svm += f\"Test Accuracy: {test_accuracy_svm:.4f}\\n\\n\"\n","results_summary_svm += f\"Validation Classification Report:\\n{classification_report(y_val, y_val_pred_svm, zero_division=0)}\\n\\n\"\n","results_summary_svm += f\"Test Classification Report:\\n{classification_report(y_test, y_test_pred_svm, zero_division=0)}\\n\"\n","\n","output_file_path_svm = os.path.join(OUTPUT_PATH, \"svm_results.txt\")\n","with open(output_file_path_svm, \"w\") as f:\n","    f.write(results_summary_svm)\n","\n","print(f\"SVM Results saved to {output_file_path_svm}\")\n","print(\"SVM script finished successfully.\")"],"metadata":{"id":"IQyHv2hZHpgz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748729565509,"user_tz":-180,"elapsed":2915651,"user":{"displayName":"Graduation project CIC 2025","userId":"06762630713984150762"}},"outputId":"5fd47893-795f-42f9-f1e5-c90ba8a27745"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading dataset: /content/drive/MyDrive/Colab Notebooks/datasets/ML-EdgeIIoT-dataset.csv\n","Dataset loaded successfully.\n","Dataset shape after dropping NA: (157800, 63)\n","Selected features for training: ['arp.hw.size', 'http.content_length', 'http.response', 'http.tls_port', 'tcp.ack_raw', 'tcp.checksum', 'tcp.connection.fin', 'tcp.connection.rst', 'tcp.connection.syn', 'tcp.connection.synack', 'tcp.dstport', 'tcp.flags.ack', 'tcp.len', 'udp.stream', 'udp.time_delta', 'dns.qry.qu', 'dns.qry.type', 'dns.retransmission', 'dns.retransmit_request', 'dns.retransmit_request_in', 'mqtt.conflag.cleansess', 'mqtt.hdrflags', 'mqtt.len', 'mqtt.msg_decoded_as', 'mbtcp.len', 'mbtcp.trans_id', 'mbtcp.unit_id']\n","Number of selected features: 27\n","Scaler saved to /content/drive/MyDrive/Colab Notebooks/results/svm_scaler.joblib (joblib format)\n","Scaler saved to /content/drive/MyDrive/Colab Notebooks/results/svm_scaler.pkl (pickle .pkl format)\n","Training set size: 110460\n","Validation set size: 23670\n","Test set size: 23670\n","Training SVM model...\n","SVM Model training complete.\n","Trained SVM model saved to /content/drive/MyDrive/Colab Notebooks/results/svm_model.joblib (joblib format)\n","Trained SVM model saved to /content/drive/MyDrive/Colab Notebooks/results/svm_model.pkl (pickle .pkl format)\n","Saving SVM model and scaler to HDF5 format: /content/drive/MyDrive/Colab Notebooks/results/svm_model_and_scaler.h5...\n","Error saving to HDF5: VLEN strings do not support embedded NULLs\n","Evaluating SVM on validation set...\n","SVM Validation Accuracy: 0.8998\n","SVM Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.92      0.38      0.54      3645\n","           1       0.90      0.99      0.94     20025\n","\n","    accuracy                           0.90     23670\n","   macro avg       0.91      0.69      0.74     23670\n","weighted avg       0.90      0.90      0.88     23670\n","\n","Evaluating SVM on test set...\n","SVM Test Accuracy: 0.9048\n","SVM Test Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.92      0.42      0.58      3645\n","           1       0.90      0.99      0.95     20025\n","\n","    accuracy                           0.90     23670\n","   macro avg       0.91      0.71      0.76     23670\n","weighted avg       0.91      0.90      0.89     23670\n","\n","SVM Results saved to /content/drive/MyDrive/Colab Notebooks/results/svm_results.txt\n","SVM script finished successfully.\n"]}]}]}