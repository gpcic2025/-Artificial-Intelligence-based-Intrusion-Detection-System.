{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"15Qa4FVLvHQVDj3jRmSPFJdg5Ak2qiRhB","authorship_tag":"ABX9TyOWk7G0shDmB/5HeTTXcZzc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"52_tdX-Pw1o6","executionInfo":{"status":"ok","timestamp":1749060603714,"user_tz":-180,"elapsed":7590,"user":{"displayName":"Graduation project CIC 2025","userId":"06762630713984150762"}},"outputId":"db94f685-992e-4e43-caae-50c46853d713","colab":{"base_uri":"https://localhost:8080/","height":90}},"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: line 1: ngrok: command not found\n","\n","üìä Access your dashboard at: https://8501-m-s-2o85h5vwfbbuk-b.us-west3-0.prod.colab.dev\n","If the link doesn't work, try the ngrok section below (uncomment and configure).\n"]}],"source":["\n","\"\"\"Dashboard\n","\n","Automatically generated by Colab.\n","\n","Original file is located at\n","    https://colab.research.google.com/drive/1xQzHjBGJvAS2lhIWQbs1lXpHJW4UiDkY\n","\"\"\"\n","\n","# rt_sensor_simulator.py and rt_attack_simulator.py definitions would be here\n","# For brevity, I'm omitting them as the main app content below doesn't directly call their generators.\n","# If you want to integrate them as data sources, that would be an additional step.\n","\n","# ... (Keep the rt_sensor_simulator.py and rt_attack_simulator.py code from your file) ...\n","\n","# dashboard.py (main Streamlit app)\n","\n","# 1. First verify and clean up existing ngrok configurations\n","!rm -rf /root/.config/ngrok # This is a shell command, typically run in a notebook cell\n","!mkdir -p /root/.config/ngrok # This is a shell command\n","\n","# 2. Set up your ngrok authtoken\n","# Replace YOUR_NGROK_AUTH_TOKEN with your actual token\n","!ngrok config add-authtoken YOUR_NGROK_AUTH_TOKEN\n","\n","\n","# 4. Create the enhanced Streamlit app file with CSV data pipeline\n","import os\n","app_content = \"\"\"\n","import streamlit as st\n","import pandas as pd\n","import numpy as np\n","import time\n","from datetime import datetime\n","import altair as alt\n","from collections import defaultdict, deque\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import MinMaxScaler # For LSTM, original scaler\n","from sklearn.metrics import confusion_matrix, classification_report\n","import joblib # For loading scikit-learn models\n","\n","# Conditional import for TensorFlow\n","try:\n","    from tensorflow.keras.models import load_model\n","    TENSORFLOW_AVAILABLE = True\n","except ImportError:\n","    TENSORFLOW_AVAILABLE = False\n","    st.sidebar.warning(\"TensorFlow/Keras not found. LSTM model will be unavailable.\")\n","\n","# --- Configuration ---\n","# Paths to your trained models and scaler\n","BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/\" # Adjust if your Drive is mounted differently\n","RESULTS_PATH = os.path.join(BASE_PATH, \"results\")\n","DATA_PATH = os.path.join(BASE_PATH, \"datasets/ML-EdgeIIoT-dataset.csv\")\n","\n","# Scikit-learn models and common scaler\n","SKLEARN_MODEL_FILES = {\n","    \"RandomForest\": os.path.join(RESULTS_PATH, \"randomforest_model.joblib\"),\n","    \"DecisionTree\": os.path.join(RESULTS_PATH, \"decisiontree_model.joblib\"),\n","    \"LogisticRegression\": os.path.join(RESULTS_PATH, \"logisticregression_model.joblib\"),\n","    \"XGBoost\": os.path.join(RESULTS_PATH, \"xgboost_model.joblib\"),\n","    \"SVM\": os.path.join(RESULTS_PATH, \"svm_model.joblib\") # Assuming you have an SVM model saved\n","}\n","COMMON_SCALER_PATH = os.path.join(RESULTS_PATH, \"common_scaler.joblib\") # Scaler for scikit-learn models\n","\n","# LSTM Model\n","LSTM_MODEL_PATH = os.path.join(BASE_PATH, \"lstm_model.h5\") # Path to your trained LSTM model\n","\n","# Features: CRITICAL - This list MUST match the features your models were trained on.\n","# These are the 27 features from our previous scikit-learn model training.\n","# The LSTM model might require a different set or preprocessing. This example assumes\n","# we can extract these 27 for all models for demonstration, or that LSTM is adapted.\n","FEATURE_COLUMNS = [\n","    'arp.hw.size', 'http.content_length', 'http.response', 'http.tls_port',\n","    'tcp.ack_raw', 'tcp.checksum', 'tcp.connection.fin', 'tcp.connection.rst',\n","    'tcp.connection.syn', 'tcp.connection.synack', 'tcp.dstport',\n","    'tcp.flags.ack', 'tcp.len', 'udp.stream', 'udp.time_delta', 'dns.qry.qu',\n","    'dns.qry.type', 'dns.retransmission', 'dns.retransmit_request',\n","    'dns.retransmit_request_in', 'mqtt.conflag.cleansess', 'mqtt.hdrflags',\n","    'mqtt.len', 'mqtt.msg_decoded_as', 'mbtcp.len', 'mbtcp.trans_id',\n","    'mbtcp.unit_id'\n","]\n","LSTM_SEQUENCE_LENGTH = 1 # Example: LSTM looks at one timestep at a time\n","\n","# --- Session State Initialization ---\n","if 'data' not in st.session_state:\n","    st.session_state.data = {\n","        'events': deque(maxlen=500), # Store recent raw samples\n","        'predictions_log': deque(maxlen=500), # Store dicts with {'actual':, 'predicted':, 'proba':}\n","        'attack_stats': defaultdict(int), # Counts for attack types\n","        'model_metrics': {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0},\n","        'common_scaler_sklearn': None, # Scaler for scikit-learn models\n","        'lstm_scaler': MinMaxScaler(), # Separate scaler for LSTM if needed, or use common one\n","        'data_iterator': None,\n","        'selected_model_name': None,\n","        'loaded_models': {} # Cache for loaded model objects\n","    }\n","if 'page_loaded' not in st.session_state: # To fit scaler only once for LSTM example\n","    st.session_state.page_loaded = False\n","\n","\n","# --- Model and Scaler Loading ---\n","@st.cache_resource\n","def load_sklearn_models_and_scaler():\n","    models = {}\n","    for name, path in SKLEARN_MODEL_FILES.items():\n","        if os.path.exists(path):\n","            try:\n","                models[name] = joblib.load(path)\n","            except Exception as e:\n","                st.sidebar.error(f\"Failed to load {name}: {e}\")\n","        else:\n","            st.sidebar.warning(f\"Scikit-learn model not found: {path}\")\n","\n","    if os.path.exists(COMMON_SCALER_PATH):\n","        try:\n","            scaler = joblib.load(COMMON_SCALER_PATH)\n","            st.sidebar.success(\"‚úÖ Common Scaler (for scikit-learn) loaded.\")\n","            return models, scaler\n","        except Exception as e:\n","            st.sidebar.error(f\"Failed to load Common Scaler: {e}\")\n","            return models, None\n","    else:\n","        st.sidebar.warning(f\"Common Scaler not found: {COMMON_SCALER_PATH}\")\n","        return models, None\n","\n","@st.cache_resource\n","def load_lstm_model_keras():\n","    if not TENSORFLOW_AVAILABLE:\n","        return None\n","    if os.path.exists(LSTM_MODEL_PATH):\n","        try:\n","            model = load_model(LSTM_MODEL_PATH)\n","            st.sidebar.success(\"‚úÖ LSTM Model loaded.\")\n","            return model\n","        except Exception as e:\n","            st.sidebar.error(f\"LSTM Model loading failed: {e}\")\n","            return None\n","    else:\n","        st.sidebar.warning(f\"LSTM Model not found: {LSTM_MODEL_PATH}\")\n","        return None\n","\n","# Load models into session state's \"loaded_models\" dictionary\n","sklearn_models_loaded, common_scaler_loaded = load_sklearn_models_and_scaler()\n","if common_scaler_loaded:\n","    st.session_state.data['common_scaler_sklearn'] = common_scaler_loaded\n","for name, model in sklearn_models_loaded.items():\n","    st.session_state.data['loaded_models'][name] = model\n","\n","if TENSORFLOW_AVAILABLE:\n","    lstm_model_loaded = load_lstm_model_keras()\n","    if lstm_model_loaded:\n","        st.session_state.data['loaded_models']['LSTM'] = lstm_model_loaded\n","\n","\n","# --- Data Pipeline ---\n","class EdgeIIoTDataIterator:\n","    def __init__(self, file_path, feature_columns):\n","        self.feature_columns = feature_columns\n","        try:\n","            # Attempt to load only necessary columns to save memory\n","            # This requires knowing which original CSV columns map to your FEATURE_COLUMNS\n","            # For simplicity, loading all then selecting. Optimize if memory is an issue.\n","            self.df = pd.read_csv(file_path, low_memory=False)\n","            st.sidebar.info(f\"Raw CSV loaded with {len(self.df)} rows, {len(self.df.columns)} columns.\")\n","\n","            # Preprocessing: Fill missing for selected features and 'Attack_label'\n","            for col in self.feature_columns:\n","                if col not in self.df.columns:\n","                    st.warning(f\"Feature column '{col}' not found in CSV! Filling with 0.\")\n","                    self.df[col] = 0\n","                else:\n","                    # Ensure numeric, fill NaNs for feature columns\n","                    self.df[col] = pd.to_numeric(self.df[col], errors='coerce').fillna(0)\n","\n","            if 'Attack_label' in self.df.columns:\n","                self.df['is_attack'] = self.df['Attack_label'].apply(lambda x: 0 if str(x) == '0' else 1)\n","            else:\n","                st.warning(\"'Attack_label' not found. Assuming all data is normal or needs manual labeling.\")\n","                self.df['is_attack'] = 0 # Default to normal if no label\n","\n","            # Keep only necessary columns for iteration to save memory\n","            self.df_processed = self.df[self.feature_columns + ['is_attack']].copy()\n","            st.sidebar.success(f\"Dataset processed. Using {len(self.feature_columns)} features.\")\n","\n","        except FileNotFoundError:\n","            st.error(f\"Dataset CSV not found at {file_path}. Please check the path.\")\n","            self.df_processed = pd.DataFrame() # Empty df\n","        except Exception as e:\n","            st.error(f\"Error loading or processing dataset: {str(e)}\")\n","            self.df_processed = pd.DataFrame() # Empty df\n","\n","        self.current_idx = 0\n","\n","\n","    def __iter__(self):\n","        return self\n","\n","    def __next__(self):\n","        if self.df_processed.empty or self.current_idx >= len(self.df_processed):\n","            self.current_idx = 0 # Loop dataset\n","            if not self.df_processed.empty:\n","                # st.sidebar.warning(\"‚ôªÔ∏è Reached end of dataset - looping.\")\n","                pass # Avoid too many warnings\n","            else:\n","                st.error(\"No data to iterate. Stopping simulation.\")\n","                raise StopIteration # Stop if df is empty from the start\n","\n","        row = self.df_processed.iloc[self.current_idx]\n","        self.current_idx += 1\n","\n","        sample_features = {col: float(row[col]) for col in self.feature_columns}\n","        sample = {\n","            'timestamp': datetime.now().strftime('%H:%M:%S.%f')[:-3],\n","            'is_attack': int(row['is_attack']), # Ground truth\n","            **sample_features\n","        }\n","        return sample\n","\n","# Initialize data iterator if not already done or if it's None\n","if st.session_state.data.get('data_iterator') is None and DATA_PATH:\n","    st.session_state.data['data_iterator'] = EdgeIIoTDataIterator(DATA_PATH, FEATURE_COLUMNS)\n","\n","# --- Preprocessing and Prediction Functions ---\n","def preprocess_and_predict(sample_features_dict, model_name, feature_columns):\n","    # Prepare 2D array for scikit-learn and initial LSTM step\n","    features_2d = np.array([[sample_features_dict[col] for col in feature_columns]])\n","\n","    current_model = st.session_state.data['loaded_models'].get(model_name)\n","    if current_model is None:\n","        return 0, 0.0, \"Model not loaded\" # Predicted class, probability, status\n","\n","    # Preprocessing\n","    if model_name == \"LSTM\":\n","        # LSTM specific scaling (example: fit once, then transform)\n","        # In a real scenario, this scaler should be saved/loaded like common_scaler_sklearn\n","        if not st.session_state.page_loaded: # Simplified: fit on first batch seen by dashboard\n","            st.session_state.data['lstm_scaler'].fit(features_2d)\n","            # This is not ideal; LSTM scaler should be pre-fitted and loaded.\n","            # For now, we make a note or assume it's handled if a pre-trained LSTM is used.\n","\n","        model_input = st.session_state.data['lstm_scaler'].transform(features_2d)\n","        model_input = model_input.reshape(1, LSTM_SEQUENCE_LENGTH, len(feature_columns))\n","        status_message = \"LSTM Preprocessed\"\n","    else: # Scikit-learn models\n","        scaler_sklearn = st.session_state.data.get('common_scaler_sklearn')\n","        if scaler_sklearn is None:\n","            return 0, 0.0, \"Scaler for scikit-learn models not loaded\"\n","        model_input = scaler_sklearn.transform(features_2d)\n","        status_message = \"Scikit-learn Preprocessed\"\n","\n","    # Prediction\n","    try:\n","        if model_name == \"LSTM\":\n","            prediction_prob = current_model.predict(model_input, verbose=0)[0][0]\n","            predicted_class = int(prediction_prob > 0.5)\n","        else: # Scikit-learn models\n","            predicted_class = current_model.predict(model_input)[0]\n","            if hasattr(current_model, \"predict_proba\"):\n","                # Probability of the positive class (assuming class 1 is attack)\n","                prediction_prob = current_model.predict_proba(model_input)[0][1]\n","            else: # Models like SVM without probability=True\n","                prediction_prob = 1.0 if predicted_class == 1 else 0.0\n","        return int(predicted_class), float(prediction_prob), status_message\n","    except Exception as e:\n","        return 0, 0.0, f\"Prediction error: {str(e)}\"\n","\n","# --- Dashboard UI ---\n","st.set_page_config(layout=\"wide\", page_title=\"Enhanced IDS Dashboard\")\n","st.title(\"üõ°Ô∏è Edge-IIoT Intrusion Detection System Dashboard\")\n","\n","# Sidebar for controls\n","st.sidebar.header(\"‚öôÔ∏è Controls & Settings\")\n","available_model_names = [name for name in st.session_state.data['loaded_models'].keys() if st.session_state.data['loaded_models'][name] is not None]\n","if not available_model_names:\n","    st.sidebar.error(\"No models were loaded successfully!\")\n","    st.stop()\n","\n","st.session_state.data['selected_model_name'] = st.sidebar.selectbox(\n","    \"üß† Select Model:\",\n","    available_model_names,\n","    index=0 # Default to the first available model\n",")\n","active_model_name = st.session_state.data['selected_model_name']\n","st.sidebar.info(f\"Active Model: **{active_model_name}**\")\n","\n","update_interval_ms = st.sidebar.slider(\"‚è±Ô∏è Update Interval (ms)\", 100, 2000, 500)\n","history_length = st.sidebar.slider(\"üìä History Length (samples)\", 50, 500, 100)\n","\n","# Adjust deque maxlen dynamically (optional, can be fixed)\n","if st.session_state.data['events'].maxlen != history_length:\n","    st.session_state.data['events'] = deque(st.session_state.data['events'], maxlen=history_length)\n","    st.session_state.data['predictions_log'] = deque(st.session_state.data['predictions_log'], maxlen=history_length)\n","\n","\n","# Main layout\n","col1, col2 = st.columns([2, 1]) # Adjust column ratio if needed\n","\n","with col1:\n","    st.subheader(\"üö¶ Real-time Traffic & Detections\")\n","    event_plot_placeholder = st.empty()\n","\n","    st.subheader(\"üìà Prediction Confidence/Probability\")\n","    confidence_plot_placeholder = st.empty()\n","\n","with col2:\n","    st.subheader(\"üìä Detection Statistics (Recent History)\")\n","    stats_placeholder = st.empty()\n","\n","    st.subheader(\"‚ö†Ô∏è Recent Alerts\")\n","    alerts_placeholder = st.empty()\n","\n","    if active_model_name not in [\"LSTM\", \"SVM\"]: # LSTM/SVM might not have simple feature_importances_\n","        model_object_for_fi = st.session_state.data['loaded_models'].get(active_model_name)\n","        if hasattr(model_object_for_fi, 'feature_importances_') or hasattr(model_object_for_fi, 'coef_'):\n","            st.subheader(f\"Feature Importance ({active_model_name})\")\n","            fi_placeholder = st.empty()\n","\n","\n","# --- Main Processing Loop ---\n","data_iterator = st.session_state.data.get('data_iterator')\n","\n","if data_iterator is None:\n","    st.error(\"Data iterator not initialized. Please check dataset path and configuration.\")\n","    st.stop()\n","\n","# Mark page as loaded after initial setup and first pass of data processing potentially\n","if not st.session_state.page_loaded:\n","    st.session_state.page_loaded = True\n","\n","\n","# Main display loop\n","while True: # Loop indefinitely for a real-time feel\n","    try:\n","        sample = next(data_iterator) # Get next data sample\n","    except StopIteration:\n","        st.warning(\"Data source depleted (if not set to loop). Dashboard will pause.\")\n","        break # Or implement a reset/reload mechanism\n","    except Exception as e:\n","        st.error(f\"Error fetching data: {e}\")\n","        time.sleep(5) # Wait before retrying\n","        continue\n","\n","    # Extract features for model\n","    sample_features = {k: sample[k] for k in FEATURE_COLUMNS}\n","\n","    predicted_class, prediction_prob, pred_status = preprocess_and_predict(\n","        sample_features, active_model_name, FEATURE_COLUMNS\n","    )\n","\n","    if \"error\" in pred_status.lower() or \"not loaded\" in pred_status.lower():\n","        st.sidebar.error(pred_status)\n","        # Potentially skip this iteration or handle error\n","    else:\n","        if st.sidebar.checkbox(\"Show Preprocessing Status\", False):\n","             st.sidebar.caption(pred_status)\n","\n","\n","    # Log event and prediction\n","    st.session_state.data['events'].append(sample)\n","    log_entry = {\n","        'timestamp': sample['timestamp'],\n","        'actual': sample['is_attack'],\n","        'predicted': predicted_class,\n","        'probability': prediction_prob,\n","        'model': active_model_name\n","    }\n","    st.session_state.data['predictions_log'].append(log_entry)\n","\n","    # Update confusion matrix stats based on the latest prediction\n","    actual = sample['is_attack']\n","    pred = predicted_class\n","    if actual == 1 and pred == 1: st.session_state.data['model_metrics']['tp'] += 1\n","    elif actual == 0 and pred == 1: st.session_state.data['model_metrics']['fp'] += 1\n","    elif actual == 0 and pred == 0: st.session_state.data['model_metrics']['tn'] += 1\n","    elif actual == 1 and pred == 0: st.session_state.data['model_metrics']['fn'] += 1\n","\n","\n","    # --- Update Visualizations ---\n","    events_df = pd.DataFrame(list(st.session_state.data['events']))\n","    predictions_log_df = pd.DataFrame(list(st.session_state.data['predictions_log']))\n","\n","    # Event Plot (Example: showing one feature and attack indicators)\n","    if not events_df.empty:\n","        with event_plot_placeholder.container():\n","            # Choose a key feature to plot, e.g., 'flow_duration' or 'Rate' if available\n","            # For now, we plot a generic \"value\" which would be the prediction probability\n","            # and mark actual vs predicted attacks.\n","            chart_data = predictions_log_df.copy()\n","            chart_data['time_obj'] = pd.to_datetime(chart_data['timestamp'], format='%H:%M:%S.%f')\n","\n","            base_chart = alt.Chart(chart_data).encode(x='time_obj:T')\n","\n","            line_prob = base_chart.mark_line().encode(\n","                y=alt.Y('probability:Q', title='Attack Probability', scale=alt.Scale(domain=[0, 1])),\n","                tooltip=['timestamp:N', 'probability:Q', 'actual:N', 'predicted:N']\n","            ).properties(title=\"Attack Prediction Probability Over Time\")\n","\n","            # Points for actual attacks\n","            actual_attacks = base_chart.transform_filter(\n","                alt.datum.actual == 1\n","            ).mark_circle(size=100, color='red', opacity=0.7).encode(\n","                y=alt.Y('probability:Q'),\n","                tooltip=['timestamp:N', 'actual:N', 'predicted:N']\n","            )\n","            # Points for predicted attacks\n","            predicted_attacks = base_chart.transform_filter(\n","                alt.datum.predicted == 1\n","            ).mark_point(shape='diamond', size=80, color='orange', opacity=0.7, filled=True).encode(\n","                y=alt.Y('probability:Q'),\n","                tooltip=['timestamp:N', 'actual:N', 'predicted:N']\n","            )\n","            st.altair_chart(line_prob + actual_attacks + predicted_attacks, use_container_width=True)\n","\n","\n","    # Confidence Plot (if probabilities are available)\n","    if not predictions_log_df.empty:\n","        with confidence_plot_placeholder.container():\n","            conf_df = predictions_log_df[['timestamp', 'probability']].copy()\n","            conf_df['time_obj'] = pd.to_datetime(conf_df['timestamp'], format='%H:%M:%S.%f')\n","            conf_chart = alt.Chart(conf_df).mark_bar().encode(\n","                x='time_obj:T',\n","                y=alt.Y('probability:Q', title='Prediction Probability', scale=alt.Scale(domain=(0,1))),\n","                color=alt.condition(\n","                    alt.datum.probability > 0.5,\n","                    alt.value('red'),  # Color for attack\n","                    alt.value('green') # Color for normal\n","                )\n","            ).properties(title=\"Prediction Confidence Scores\", height=200)\n","            st.altair_chart(conf_chart, use_container_width=True)\n","\n","    # Stats (Confusion Matrix and Metrics)\n","    with stats_placeholder.container():\n","        tp = st.session_state.data['model_metrics']['tp']\n","        fp = st.session_state.data['model_metrics']['fp']\n","        tn = st.session_state.data['model_metrics']['tn']\n","        fn = st.session_state.data['model_metrics']['fn']\n","\n","        st.write(f\"**Confusion Matrix (Overall for session):**\")\n","        cm_data = [[tn, fp], [fn, tp]]\n","        cm_df = pd.DataFrame(cm_data, columns=['Predicted Normal', 'Predicted Attack'], index=['Actual Normal', 'Actual Attack'])\n","        st.table(cm_df)\n","\n","        accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n","        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n","        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n","        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n","\n","        st.metric(\"Accuracy\", f\"{accuracy:.2%}\")\n","        col_m1, col_m2, col_m3 = st.columns(3)\n","        col_m1.metric(\"Precision\", f\"{precision:.2%}\")\n","        col_m2.metric(\"Recall (Sensitivity)\", f\"{recall:.2%}\")\n","        col_m3.metric(\"F1-Score\", f\"{f1:.2%}\")\n","\n","    # Recent Alerts\n","    with alerts_placeholder.container():\n","        if not predictions_log_df.empty:\n","            recent_alerts_df = predictions_log_df[predictions_log_df['predicted'] == 1].tail(5)\n","            st.write(\"**Recent Detected Attacks:**\")\n","            if not recent_alerts_df.empty:\n","                st.dataframe(recent_alerts_df[['timestamp', 'probability', 'actual']], use_container_width=True)\n","            else:\n","                st.info(\"No attacks detected in recent history.\")\n","\n","    # Feature Importances / Coefficients\n","    if active_model_name not in [\"LSTM\", \"SVM\"] and ('fi_placeholder' in locals() or 'fi_placeholder' in globals()): # Check if placeholder exists\n","        model_obj = st.session_state.data['loaded_models'].get(active_model_name)\n","        importances = None\n","        if hasattr(model_obj, 'feature_importances_'):\n","            importances = model_obj.feature_importances_\n","        elif hasattr(model_obj, 'coef_'):\n","            importances = model_obj.coef_[0] # For linear models, take the first (and often only) set of coeffs\n","\n","        if importances is not None:\n","            with fi_placeholder.container():\n","                fi_df = pd.DataFrame({'feature': FEATURE_COLUMNS, 'importance': importances})\n","                fi_df = fi_df.sort_values('importance', ascending=False).head(10) # Top 10\n","                fi_chart = alt.Chart(fi_df).mark_bar().encode(\n","                    x='importance:Q',\n","                    y=alt.Y('feature:N', sort='-x')\n","                ).properties(title=f\"Top Feature Importances\", height=250)\n","                st.altair_chart(fi_chart, use_container_width=True)\n","\n","    time.sleep(update_interval_ms / 1000) # Control update speed\n","\n","\"\"\"\n","\n","# Write the app content to app.py\n","with open('app.py', 'w') as f:\n","    f.write(app_content)\n","\n","# 5. Run Streamlit with proper ngrok configuration (ensure ngrok token is set above)\n","import threading\n","import time\n","# from pyngrok import ngrok # pyngrok can be problematic in some Colab setups, using direct proxy\n","\n","def run_streamlit():\n","    # Kill existing streamlit processes to avoid port conflicts\n","    os.system('pkill -f streamlit')\n","    time.sleep(2)\n","    os.system(f'streamlit run app.py --server.port 8501 --server.headless true --server.enableCORS false --server.enableXsrfProtection false')\n","\n","# Start Streamlit in background\n","thread = threading.Thread(target=run_streamlit, daemon=True)\n","thread.start()\n","time.sleep(5)  # Wait for Streamlit to initialize\n","\n","# Create ngrok tunnel or use Colab proxy\n","# Using Colab proxy as ngrok can be flaky or require more setup\n","from google.colab.output import eval_js\n","print(f\"\\nüìä Access your dashboard at: {eval_js('google.colab.kernel.proxyPort(8501)')}\")\n","print(\"If the link doesn't work, try the ngrok section below (uncomment and configure).\")\n","\n","# # Alternative: ngrok (if Colab proxy fails or for external access)\n","# # Ensure your ngrok authtoken is configured earlier in the notebook\n","# # from pyngrok import ngrok, conf\n","# # try:\n","# #     public_url = ngrok.connect(addr='8501', proto='http')\n","# #     print(f\"\\n‚úÖ Dashboard is live via ngrok at: {public_url}\\n\")\n","# # except Exception as e:\n","# #     print(f\"\\n‚ùå Error creating ngrok tunnel: {e}\")\n","# #     print(\"Make sure your ngrok authtoken is set correctly if using this method.\")"]}]}