{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1G1LewXU1nWShhQSIkIDkThjMY_NmBx9p","timestamp":1748477508645}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j1VtEZKmaNIK","executionInfo":{"status":"ok","timestamp":1748478291959,"user_tz":-180,"elapsed":26893,"user":{"displayName":"Graduation project CIC 2025","userId":"06762630713984150762"}},"outputId":"67d6b4a9-b3eb-494e-a8d3-5568a4835b13"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5UBSKCUYUfVU","executionInfo":{"status":"ok","timestamp":1748478313879,"user_tz":-180,"elapsed":5618,"user":{"displayName":"Graduation project CIC 2025","userId":"06762630713984150762"}},"outputId":"eecce3bd-ca6e-4dd0-8202-c43e720ab13f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Attempting to set project base for benchmarking at: /content/drive/MyDrive/IDS_AI_Suite/model_benchmarking_outputs\n","\n","Project base directory for Benchmarking: /content/drive/MyDrive/IDS_AI_Suite/model_benchmarking_outputs\n","Data directory (for test data): /content/drive/MyDrive/IDS_AI_Suite/model_benchmarking_outputs/data_for_benchmarking\n","Models directory (for models to benchmark): /content/drive/MyDrive/IDS_AI_Suite/model_benchmarking_outputs/models_to_benchmark\n","Benchmark results directory: /content/drive/MyDrive/IDS_AI_Suite/model_benchmarking_outputs/benchmark_outputs\n","Plots directory: /content/drive/MyDrive/IDS_AI_Suite/model_benchmarking_outputs/benchmark_outputs/plots\n","\n","✅ Section 1 (Benchmarking - Setup and Configuration) is ready.\n"]}],"source":["import os\n","import sys\n","import json\n","import time\n","import pickle\n","import argparse\n","import numpy as np\n","import pandas as pd\n","from datetime import datetime\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Scikit-learn metrics\n","from sklearn.metrics import (\n","    accuracy_score, precision_score, recall_score, f1_score,\n","    confusion_matrix, classification_report, roc_curve, auc,\n","    precision_recall_curve, average_precision_score\n",")\n","# For TensorFlow/Keras models\n","import tensorflow as tf\n","# Ensure load_model is correctly imported from tf.keras.models\n","from tensorflow.keras.models import load_model as tf_load_model\n","\n","# For system resource monitoring\n","import psutil\n","import gc\n","\n","# --- ACTION REQUIRED: Ensure Google Drive is mounted if you want to use it ---\n","# If you haven't run the mount command in a separate cell, do it now:\n","# from google.colab import drive\n","# drive.mount('/content/drive')\n","\n","# --- ACTION REQUIRED: Define the MAIN Google Drive path for ALL your IDS-AI projects ---\n","# All specific project BASE_DIRs will be created inside this path.\n","# Example: \"/content/drive/MyDrive/My_Overall_IDS_AI_Projects_Folder\"\n","YOUR_MAIN_GOOGLE_DRIVE_PROJECT_FOLDER = \"/content/drive/MyDrive/IDS_AI_Suite\" # <<<< CHANGE THIS TO YOUR DESIRED GOOGLE DRIVE FOLDER\n","\n","# --- Define project base directory for THIS BENCHMARKING SCRIPT ---\n","# This will be a subfolder within your YOUR_MAIN_GOOGLE_DRIVE_PROJECT_FOLDER\n","BENCHMARKING_PROJECT_FOLDER_NAME = \"model_benchmarking_outputs\"\n","BASE_DIR = os.path.join(YOUR_MAIN_GOOGLE_DRIVE_PROJECT_FOLDER, BENCHMARKING_PROJECT_FOLDER_NAME)\n","\n","if \"/content/drive/\" not in YOUR_MAIN_GOOGLE_DRIVE_PROJECT_FOLDER:\n","    print(f\"⚠️ WARNING: YOUR_MAIN_GOOGLE_DRIVE_PROJECT_FOLDER ('{YOUR_MAIN_GOOGLE_DRIVE_PROJECT_FOLDER}') \"\n","          \"does not appear to be a Google Drive path. Please ensure Drive is mounted and the path is correct.\")\n","    print(f\"   Results might be saved to a temporary Colab path if not set correctly: /content/{BENCHMARKING_PROJECT_FOLDER_NAME}\")\n","    # Fallback to a temporary path if Drive path seems incorrect or not mounted.\n","    if not os.path.exists(os.path.join(\"/content/drive/MyDrive\")): # Basic check if /content/drive/MyDrive exists\n","        print(\"   Google Drive ('/content/drive/MyDrive') not detected. Using temporary storage.\")\n","        BASE_DIR = f\"/content/{BENCHMARKING_PROJECT_FOLDER_NAME}\"\n","\n","print(f\"Attempting to set project base for benchmarking at: {BASE_DIR}\")\n","os.makedirs(BASE_DIR, exist_ok=True) # Ensure the main project-specific folder is created\n","\n","# Define subdirectories\n","DATA_DIR = os.path.join(BASE_DIR, \"data_for_benchmarking\")\n","MODEL_DIR = os.path.join(BASE_DIR, \"models_to_benchmark\")\n","RESULTS_DIR = os.path.join(BASE_DIR, \"benchmark_outputs\")\n","PLOTS_DIR = os.path.join(RESULTS_DIR, \"plots\")\n","\n","for directory in [DATA_DIR, MODEL_DIR, RESULTS_DIR, PLOTS_DIR]:\n","    os.makedirs(directory, exist_ok=True)\n","\n","print(f\"\\nProject base directory for Benchmarking: {BASE_DIR}\")\n","print(f\"Data directory (for test data): {DATA_DIR}\")\n","print(f\"Models directory (for models to benchmark): {MODEL_DIR}\")\n","print(f\"Benchmark results directory: {RESULTS_DIR}\")\n","print(f\"Plots directory: {PLOTS_DIR}\")\n","\n","# Alias tf.keras.models.load_model\n","keras_load_model = tf_load_model\n","\n","print(\"\\n✅ Section 1 (Benchmarking - Setup and Configuration) is ready.\")"]},{"cell_type":"code","source":["# Imports from Section 1 should still be in effect.\n","# Ensure sklearn.preprocessing.LabelEncoder, StandardScaler are available\n","# Ensure sklearn.model_selection.train_test_split is available\n","\n","class ModelBenchmarker:\n","    \"\"\"\n","    Class for benchmarking and comparing different machine learning models\n","    for intrusion detection.\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.models = {}\n","        self.results = {}\n","        self.test_data = None\n","        self.X_columns = None\n","        self.y_column = None\n","        # print(\"ModelBenchmarker Initialized.\")\n","\n","    def load_model(self, model_path, model_name=None):\n","        if not os.path.exists(model_path):\n","            print(f\"❌ Error: Model file not found at {model_path}\")\n","            return False\n","        if model_name is None:\n","            model_name = os.path.basename(model_path).split('.')[0]\n","\n","        try:\n","            model_instance = None; model_type = 'unknown'\n","            if model_path.endswith('.h5'):\n","                model_instance = keras_load_model(model_path)\n","                model_type = 'keras'\n","            elif model_path.endswith(('.pkl', '.joblib')):\n","                with open(model_path, 'rb') as f: loaded_object = pickle.load(f)\n","                if isinstance(loaded_object, dict) and 'model' in loaded_object:\n","                    model_instance = loaded_object['model']\n","                    model_type = 'sklearn_dict' if hasattr(model_instance, 'predict') else 'unknown_pickle_dict'\n","                else:\n","                    model_instance = loaded_object\n","                    model_type = 'sklearn' if hasattr(model_instance, 'predict') else 'unknown_pickle_object'\n","            else:\n","                with open(model_path, 'rb') as f: model_instance = pickle.load(f)\n","                model_type = 'sklearn_generic' if hasattr(model_instance, 'predict') else 'unknown_generic'\n","\n","            if model_instance is None: print(f\"❌ Failed to load model instance from {model_path}.\"); return False\n","            self.models[model_name] = {'model': model_instance, 'path': model_path, 'type': model_type}\n","            # print(f\"✅ Successfully loaded model '{model_name}' (Type: {model_type}).\") # Less verbose\n","            return True\n","        except Exception as e:\n","            print(f\"❌ Error loading model '{model_name}' from {model_path}: {e}\"); return False\n","\n","    def load_test_data(self, data_path, X_columns=None, y_column=None,\n","                       test_only=True, # Defaulting to True as it's for benchmarking\n","                       test_size_if_split=0.2,\n","                       scaler_path=None,\n","                       reshape_for_lstm_load_time=False): # Renamed to avoid confusion\n","        \"\"\"\n","        Load and preprocess test data for benchmarking.\n","        If test_only is True, uses the whole file as test data.\n","        If False, splits data and uses the test portion.\n","        Applies scaling if scaler_path provided. Encodes target.\n","        Optionally reshapes X_test if reshape_for_lstm_load_time is True.\n","        \"\"\"\n","        print(f\"\\nAttempting to load test data from: {data_path}\")\n","        try:\n","            if data_path.endswith('.csv'): data_df_raw = pd.read_csv(data_path, low_memory=False)\n","            elif data_path.endswith(('.json', '.jsonl')): data_df_raw = pd.read_json(data_path, lines=data_path.endswith('.jsonl'))\n","            else: print(f\"❌ Unsupported test data file format: {data_path}\"); return False\n","\n","            data_df = data_df_raw.copy() # Work on a copy\n","            print(f\"Raw test data loaded. Shape: {data_df.shape}\")\n","            data_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n","\n","            # Determine y_column and X_columns before dropna to ensure they exist for subsetting\n","            if y_column is None: y_column = data_df.columns[-1]\n","            if X_columns is None: X_columns = [col for col in data_df.columns if col != y_column]\n","\n","            cols_to_check_dropna = [col for col in X_columns if col in data_df.columns] + \\\n","                                   ([y_column] if y_column in data_df.columns else [])\n","            if not cols_to_check_dropna: print(\"Warning: No valid X or Y columns for dropna check based on initial spec.\"); return False\n","\n","            data_df.dropna(subset=cols_to_check_dropna, inplace=True)\n","            print(f\"Test data shape after NaN drop (on relevant columns): {data_df.shape}\")\n","            if data_df.empty: print(\"❌ Test data is empty after cleaning NaNs.\"); return False\n","\n","            self.y_column = y_column\n","            self.X_columns = X_columns # Store original intended feature columns\n","\n","            current_data_for_processing = data_df\n","            if not test_only:\n","                if len(data_df[self.y_column].unique()) > 1 and len(data_df) > 1 / test_size_if_split : # Check for stratification possibility\n","                     _, current_data_for_processing = train_test_split(data_df, test_size=test_size_if_split, random_state=42, stratify=data_df[self.y_column])\n","                else: # Fallback if stratification not possible or not enough data\n","                     _, current_data_for_processing = train_test_split(data_df, test_size=test_size_if_split, random_state=42)\n","                print(f\"Data split, using test portion of size: {current_data_for_processing.shape[0]}\")\n","\n","            y_test_raw = current_data_for_processing[self.y_column]\n","\n","            missing_X_cols = [col for col in self.X_columns if col not in current_data_for_processing.columns]\n","            if missing_X_cols: print(f\"❌ Specified feature columns not found in data to be processed: {missing_X_cols}\"); return False\n","            X_test_raw_features = current_data_for_processing[self.X_columns].copy()\n","\n","            X_test_numeric = X_test_raw_features.select_dtypes(include=np.number)\n","            non_numeric_cols = X_test_raw_features.select_dtypes(exclude=np.number).columns.tolist()\n","            if non_numeric_cols: print(f\"⚠️ Warning: Non-numeric columns dropped from features: {non_numeric_cols}\")\n","            if X_test_numeric.empty: print(\"❌ No numeric features available in X_test.\"); return False\n","\n","            # Update self.X_columns to only reflect the numeric columns being processed further\n","            processed_X_columns = X_test_numeric.columns.tolist()\n","            X_test_processed = X_test_numeric.copy()\n","\n","            if scaler_path and os.path.exists(scaler_path):\n","                try:\n","                    with open(scaler_path, 'rb') as f: scaler = pickle.load(f)\n","                    if hasattr(scaler, 'mean_') and scaler.mean_ is not None:\n","                         X_test_scaled_values = scaler.transform(X_test_processed) # Scale only numeric part\n","                         X_test_processed = pd.DataFrame(X_test_scaled_values, columns=X_test_processed.columns, index=X_test_processed.index)\n","                         print(f\"✅ Test data features scaled using: {scaler_path}\")\n","                    else: print(f\"⚠️ Scaler from {scaler_path} not fitted. Using unscaled.\")\n","                except Exception as e: print(f\"❌ Error applying scaler from {scaler_path}: {e}. Using unscaled.\")\n","            elif scaler_path: print(f\"⚠️ Scaler not found at {scaler_path}. Using unscaled.\")\n","            else: print(\"ℹ️ No scaler_path provided. Using unscaled numeric data.\")\n","\n","            label_encoder = LabelEncoder(); y_test_processed = label_encoder.fit_transform(y_test_raw)\n","            print(f\"Target '{self.y_column}' label encoded. Classes: {list(label_encoder.classes_)}\")\n","\n","            if isinstance(X_test_processed, pd.DataFrame): X_test_final_np = X_test_processed.values\n","            else: X_test_final_np = X_test_processed\n","            if reshape_for_lstm_load_time:\n","                X_test_final_np = X_test_final_np.reshape((X_test_final_np.shape[0], 1, X_test_final_np.shape[1]))\n","\n","            self.test_data = {'X': X_test_final_np, 'y': y_test_processed, 'label_encoder': label_encoder, 'final_X_columns': processed_X_columns }\n","            print(f\"✅ Test data loaded & processed: {X_test_final_np.shape[0]} samples. Final X_test shape: {X_test_final_np.shape}\")\n","            return True\n","        except Exception as e:\n","            print(f\"❌ Error in load_test_data for {data_path}: {e}\"); import traceback; traceback.print_exc(); self.test_data=None; return False\n","\n","    # The preprocess_data method from your original script is not strictly needed if load_test_data handles it all.\n","    # If it was intended for a different purpose (e.g. re-applying scaler to already loaded raw X), it would need adjustment.\n","    # For now, I'm keeping it as in your original script:\n","    def preprocess_data(self, scaler=None, label_encoder=None): # This assumes self.test_data has raw X, y\n","        if self.test_data is None or 'X_raw_unscaled' not in self.test_data or 'y_raw_unencoded' not in self.test_data:\n","            print(\"preprocess_data: Raw data for X or y not found in self.test_data. \" \\\n","                  \"Ensure load_test_data stores X_raw_unscaled and y_raw_unencoded if this method is used.\")\n","            return self.test_data.get('X') if self.test_data else None, self.test_data.get('y') if self.test_data else None\n","\n","        X_test = self.test_data['X_raw_unscaled']\n","        y_test = self.test_data['y_raw_unencoded']\n","\n","        if scaler is not None: X_test = scaler.transform(X_test)\n","        if label_encoder is not None: y_test = label_encoder.transform(y_test)\n","        return X_test, y_test\n","\n","\n","# --- Test Block for ModelBenchmarker (Part 1: Init & Loading) ---\n","if __name__ == \"__main__\" and 'google.colab' in sys.modules:\n","    print(\"\\n--- Testing ModelBenchmarker (Part 1: Init & Loading) ---\")\n","\n","    benchmarker = ModelBenchmarker()\n","\n","    test_h5_model_path = \"/content/drive/MyDrive/Colab Notebooks/results/lstm_model.h5\"\n","    dummy_sklearn_model_path = os.path.join(MODEL_DIR if 'MODEL_DIR' in globals() else \".\", \"dummy_sklearn_model.pkl\")\n","\n","    if not os.path.exists(dummy_sklearn_model_path):\n","        try:\n","            from sklearn.linear_model import LogisticRegression\n","            dummy_model = LogisticRegression(); dummy_X_sk = np.array([[0,0],[1,1],[0,1],[1,0]]); dummy_y_sk = np.array([0,1,0,1])\n","            dummy_model.fit(dummy_X_sk, dummy_y_sk)\n","            with open(dummy_sklearn_model_path, 'wb') as f: pickle.dump(dummy_model, f)\n","            print(f\"Dummy sklearn model created and saved to: {dummy_sklearn_model_path}\")\n","        except Exception as e: print(f\"Could not create dummy sklearn model for testing: {e}\")\n","\n","    if os.path.exists(test_h5_model_path): benchmarker.load_model(test_h5_model_path, model_name=\"LSTM_IDS_Model\")\n","    else: print(f\"⚠️ Test Keras model not found: {test_h5_model_path}\")\n","\n","    if os.path.exists(dummy_sklearn_model_path): benchmarker.load_model(dummy_sklearn_model_path, model_name=\"Dummy_Sklearn_Model\")\n","    else: print(f\"⚠️ Dummy sklearn model not found: {dummy_sklearn_model_path}\")\n","\n","    print(f\"Models loaded: {list(benchmarker.models.keys())}\")\n","\n","    test_data_csv_path = \"/content/drive/MyDrive/Colab Notebooks/datasets/ML-EdgeIIoT-dataset.csv\"\n","    edge_iiot_features = [\n","        'arp.hw.size', 'http.content_length', 'http.response', 'http.tls_port',\n","        'tcp.ack_raw', 'tcp.checksum', 'tcp.connection.fin', 'tcp.connection.rst',\n","        'tcp.connection.syn', 'tcp.connection.synack', 'tcp.dstport', 'tcp.flags.ack',\n","        'tcp.len', 'udp.stream', 'udp.time_delta', 'dns.qry.qu', 'dns.qry.type',\n","        'dns.retransmission', 'dns.retransmit_request', 'dns.retransmit_request_in',\n","        'mqtt.conflag.cleansess', 'mqtt.hdrflags', 'mqtt.len', 'mqtt.msg_decoded_as',\n","        'mbtcp.len', 'mbtcp.trans_id', 'mbtcp.unit_id']\n","    edge_iiot_target = 'Attack_label'\n","    test_scaler_path = \"/content/drive/MyDrive/Colab Notebooks/results/scaler.pkl\"\n","\n","    if os.path.exists(test_data_csv_path):\n","        benchmarker.load_test_data(\n","            data_path=test_data_csv_path,\n","            X_columns=edge_iiot_features,\n","            y_column=edge_iiot_target,\n","            scaler_path=test_scaler_path if os.path.exists(test_scaler_path) else None,\n","            test_only=True, # Use the whole CSV as test data\n","            reshape_for_lstm_load_time=False # Reshaping will be done in benchmark_model\n","        )\n","        if benchmarker.test_data:\n","            print(f\"Test data X shape after load_test_data: {benchmarker.test_data['X'].shape}\")\n","            print(f\"Test data y shape after load_test_data: {benchmarker.test_data['y'].shape}\")\n","            print(f\"Final X_columns used by benchmarker: {benchmarker.X_columns}\")\n","    else:\n","        print(f\"⚠️ Test data file not found at {test_data_csv_path}.\")\n","    print(\"\\n--- End of ModelBenchmarker (Part 1) Test ---\")\n","\n","print(\"\\n✅ Section 2 (Benchmarking - `ModelBenchmarker` Class - Part 1: Init & Loading) is ready.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e9X0xbWlih9W","executionInfo":{"status":"ok","timestamp":1748480188182,"user_tz":-180,"elapsed":5596,"user":{"displayName":"Graduation project CIC 2025","userId":"06762630713984150762"}},"outputId":"6d025591-8f24-46c6-8686-f55b814feddb"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"output_type":"stream","name":"stdout","text":["\n","--- Testing ModelBenchmarker (Part 1: Init & Loading) ---\n","Models loaded: ['LSTM_IDS_Model', 'Dummy_Sklearn_Model']\n","\n","Attempting to load test data from: /content/drive/MyDrive/Colab Notebooks/datasets/ML-EdgeIIoT-dataset.csv\n","Raw test data loaded. Shape: (157800, 63)\n","Test data shape after NaN drop (on relevant columns): (157800, 63)\n","✅ Test data features scaled using: /content/drive/MyDrive/Colab Notebooks/results/scaler.pkl\n","Target 'Attack_label' label encoded. Classes: [np.int64(0), np.int64(1)]\n","✅ Test data loaded & processed: 157800 samples. Final X_test shape: (157800, 27)\n","Test data X shape after load_test_data: (157800, 27)\n","Test data y shape after load_test_data: (157800,)\n","Final X_columns used by benchmarker: ['arp.hw.size', 'http.content_length', 'http.response', 'http.tls_port', 'tcp.ack_raw', 'tcp.checksum', 'tcp.connection.fin', 'tcp.connection.rst', 'tcp.connection.syn', 'tcp.connection.synack', 'tcp.dstport', 'tcp.flags.ack', 'tcp.len', 'udp.stream', 'udp.time_delta', 'dns.qry.qu', 'dns.qry.type', 'dns.retransmission', 'dns.retransmit_request', 'dns.retransmit_request_in', 'mqtt.conflag.cleansess', 'mqtt.hdrflags', 'mqtt.len', 'mqtt.msg_decoded_as', 'mbtcp.len', 'mbtcp.trans_id', 'mbtcp.unit_id']\n","\n","--- End of ModelBenchmarker (Part 1) Test ---\n","\n","✅ Section 2 (Benchmarking - `ModelBenchmarker` Class - Part 1: Init & Loading) is ready.\n"]}]},{"cell_type":"code","source":["# Imports from Section 1 should still be in effect.\n","# ModelBenchmarker class (Part 1) should be defined from the previous section.\n","# Ensure all necessary sklearn.metrics, psutil, gc, etc., are available.\n","\n","class ModelBenchmarker:\n","    \"\"\"\n","    Class for benchmarking and comparing different machine learning models\n","    for intrusion detection.\n","    (Includes methods from Part 1 and adds new methods from Part 2)\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.models = {}\n","        self.results = {}\n","        self.test_data = None\n","        self.X_columns = None # Original X_columns list passed to load_test_data\n","        self.y_column = None  # Target column name\n","        # print(\"ModelBenchmarker Initialized.\") # Verbose\n","\n","    def load_model(self, model_path, model_name=None):\n","        if not os.path.exists(model_path):\n","            print(f\"❌ Error: Model file not found at {model_path}\"); return False\n","        if model_name is None: model_name = os.path.basename(model_path).split('.')[0]\n","        try:\n","            model_type = 'unknown'; model_instance = None\n","            if model_path.endswith('.h5'):\n","                model_instance = keras_load_model(model_path); model_type = 'keras'\n","            elif model_path.endswith(('.pkl', '.joblib')):\n","                with open(model_path, 'rb') as f: loaded_object = pickle.load(f)\n","                if isinstance(loaded_object, dict) and 'model' in loaded_object:\n","                    model_instance = loaded_object['model']\n","                    model_type = 'sklearn_dict' if hasattr(model_instance, 'predict') else 'unknown_pickle_dict'\n","                else:\n","                    model_instance = loaded_object\n","                    model_type = 'sklearn' if hasattr(model_instance, 'predict') else 'unknown_pickle_object'\n","            else:\n","                with open(model_path, 'rb') as f: model_instance = pickle.load(f)\n","                model_type = 'sklearn_generic' if hasattr(model_instance, 'predict') else 'unknown_generic'\n","            if model_instance is None: print(f\"❌ Failed to load model instance from {model_path}.\"); return False\n","            self.models[model_name] = {'model': model_instance, 'path': model_path, 'type': model_type}\n","            # print(f\"✅ Successfully loaded model '{model_name}' (Type: {model_type}).\") # Verbose\n","            return True\n","        except Exception as e:\n","            print(f\"❌ Error loading model '{model_name}' from {model_path}: {e}\"); return False\n","\n","    def load_test_data(self, data_path, X_columns=None, y_column=None,\n","                       test_only=True, test_size_if_split=0.2,\n","                       scaler_path=None, reshape_for_lstm_load_time=False): # reshape_for_lstm_load_time not used here, benchmark_model handles it\n","        try:\n","            if data_path.endswith('.csv'): data_df_raw = pd.read_csv(data_path, low_memory=False)\n","            elif data_path.endswith(('.json', '.jsonl')): data_df_raw = pd.read_json(data_path, lines=data_path.endswith('.jsonl'))\n","            else: print(f\"❌ Unsupported test data file format: {data_path}\"); return False\n","\n","            data_df = data_df_raw.copy()\n","            cols_to_check_dropna = []\n","            if y_column is None: y_column = data_df.columns[-1]\n","            self.y_column = y_column\n","            if self.y_column not in data_df.columns: print(f\"❌ Target column '{self.y_column}' not found.\"); return False\n","            cols_to_check_dropna.append(self.y_column)\n","\n","            if X_columns is None: X_columns = [col for col in data_df.columns if col != self.y_column]\n","            self.X_columns = X_columns # Store original selected X feature names\n","            missing_X_cols = [col for col in self.X_columns if col not in data_df.columns]\n","            if missing_X_cols: print(f\"❌ Specified features not in data: {missing_X_cols}\"); return False\n","            cols_to_check_dropna.extend(self.X_columns)\n","\n","            data_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n","            data_df.dropna(subset=list(set(cols_to_check_dropna)), inplace=True)\n","            if data_df.empty: print(\"❌ Test data empty after NaN drop.\"); return False\n","\n","            current_data_for_processing = data_df\n","            if not test_only:\n","                stratify_col = current_data_for_processing[self.y_column] if len(current_data_for_processing[self.y_column].unique()) > 1 else None\n","                _, current_data_for_processing = train_test_split(current_data_for_processing, test_size=test_size_if_split, random_state=42, stratify=stratify_col)\n","\n","            y_test_raw = current_data_for_processing[self.y_column]\n","            X_test_raw_features = current_data_for_processing[self.X_columns].copy() # Select only specified X_columns\n","\n","            X_test_numeric = X_test_raw_features.select_dtypes(include=np.number)\n","            non_numeric_cols = X_test_raw_features.select_dtypes(exclude=np.number).columns.tolist()\n","            if non_numeric_cols: print(f\"⚠️ Warning: Non-numeric columns dropped from X_columns: {non_numeric_cols}\")\n","            if X_test_numeric.empty: print(\"❌ No numeric features available in X_test after selection.\"); return False\n","\n","            final_numeric_X_columns = X_test_numeric.columns.tolist() # Actual numeric columns being processed\n","            X_test_processed_df = X_test_numeric.copy() # Work with a DataFrame for now\n","\n","            if scaler_path and os.path.exists(scaler_path):\n","                try:\n","                    with open(scaler_path, 'rb') as f: scaler = pickle.load(f)\n","                    if hasattr(scaler, 'mean_') and scaler.mean_ is not None:\n","                         X_test_scaled_values = scaler.transform(X_test_processed_df)\n","                         X_test_processed = pd.DataFrame(X_test_scaled_values, columns=X_test_processed_df.columns, index=X_test_processed_df.index)\n","                    # else: print(f\"⚠️ Scaler from {scaler_path} not fitted. Using unscaled.\") # Verbose\n","                except Exception as e: print(f\"❌ Error applying scaler from {scaler_path}: {e}. Using unscaled.\")\n","            # elif scaler_path: print(f\"⚠️ Scaler not found at {scaler_path}. Using unscaled.\") # Verbose\n","            # else: print(\"ℹ️ No scaler_path. Using unscaled numeric data.\") # Verbose\n","\n","            label_encoder = LabelEncoder(); y_test_processed = label_encoder.fit_transform(y_test_raw)\n","\n","            # Store X as numpy array (2D), reshaping will be per-model in benchmark_model\n","            X_test_final_np = X_test_processed.values if isinstance(X_test_processed, pd.DataFrame) else np.array(X_test_processed)\n","\n","            self.test_data = {'X': X_test_final_np, 'y': y_test_processed,\n","                              'label_encoder': label_encoder,\n","                              'feature_names': final_numeric_X_columns } # Store actual numeric feature names used\n","            # print(f\"✅ Test data loaded & processed. Final X_test shape: {X_test_final_np.shape}\") # Verbose\n","            return True\n","        except Exception as e:\n","            print(f\"❌ Error in load_test_data: {e}\"); self.test_data=None; return False\n","\n","    # --- New methods for Section 3 ---\n","    def benchmark_model(self, model_name):\n","        if model_name not in self.models:\n","            print(f\"❌ Model '{model_name}' not found in benchmarker.\"); return None\n","        if self.test_data is None or 'X' not in self.test_data or 'y' not in self.test_data:\n","            print(f\"❌ Test data not properly loaded. Cannot benchmark '{model_name}'.\"); return None\n","\n","        model_info = self.models[model_name]\n","        model = model_info['model']\n","        model_type = model_info['type']\n","\n","        X_test_input = self.test_data['X']\n","        y_test_true = self.test_data['y']\n","\n","        X_test_for_model = np.array(X_test_input).copy() # Ensure it's a NumPy array\n","\n","        is_model_lstm_type = (model_type == 'keras' and hasattr(model, 'layers') and \\\n","                              any(isinstance(layer, tf.keras.layers.LSTM) for layer in model.layers))\n","\n","        # Reshape data if model is LSTM and current data is 2D\n","        if is_model_lstm_type and len(X_test_for_model.shape) == 2:\n","            num_samples, num_features = X_test_for_model.shape\n","            X_test_for_model = X_test_for_model.reshape(num_samples, 1, num_features)\n","        # If model is not LSTM but data is 3D (e.g. (samples, 1, features)), reshape to 2D\n","        elif not is_model_lstm_type and len(X_test_for_model.shape) == 3 and X_test_for_model.shape[1] == 1:\n","            X_test_for_model = X_test_for_model.reshape(X_test_for_model.shape[0], X_test_for_model.shape[2])\n","\n","        print(f\"\\nBenchmarking '{model_name}' (Type: {model_type}) on {X_test_for_model.shape[0]} samples. Input X shape: {X_test_for_model.shape}\")\n","\n","        start_time = time.time(); gc.collect(); process = psutil.Process(os.getpid()); memory_before = process.memory_info().rss/(1024*1024)\n","        y_pred_classes, y_pred_proba = None, None\n","\n","        try:\n","            if model_type == 'keras':\n","                y_pred_raw = model.predict(X_test_for_model, verbose=0)\n","                if len(y_pred_raw.shape) > 1 and y_pred_raw.shape[1] > 1:\n","                    y_pred_classes = np.argmax(y_pred_raw, axis=1); y_pred_proba = y_pred_raw\n","                else:\n","                    y_pred_classes = (y_pred_raw > 0.5).astype(int).ravel(); y_pred_proba = np.column_stack((1-y_pred_raw.ravel(), y_pred_raw.ravel()))\n","            elif model_type.startswith('sklearn'):\n","                y_pred_classes = model.predict(X_test_for_model)\n","                if hasattr(model, 'predict_proba'): y_pred_proba = model.predict_proba(X_test_for_model)\n","            else: print(f\"Unsupported model type '{model_type}'.\"); return None\n","        except ValueError as ve: # Catch feature mismatch errors specifically\n","             print(f\"❌ ValueError during prediction for '{model_name}': {ve}\")\n","             print(f\"   Model expected input based on its build: {model.input_shape if hasattr(model, 'input_shape') else 'N/A'}\")\n","             print(f\"   Data provided shape: {X_test_for_model.shape}\")\n","             return None\n","        except Exception as e: print(f\"❌ Error predicting for '{model_name}': {e}\"); return None\n","\n","        prediction_time=time.time()-start_time; gc.collect(); memory_after=process.memory_info().rss/(1024*1024); memory_used=memory_after-memory_before\n","        if y_test_true.ndim > 1 and y_test_true.shape[1]==1: y_test_true=y_test_true.ravel()\n","        if y_pred_classes.ndim > 1 and y_pred_classes.shape[1]==1: y_pred_classes=y_pred_classes.ravel()\n","\n","        unique_labels_in_data = self.test_data['label_encoder'].classes_ # Use all classes known to encoder\n","        accuracy=accuracy_score(y_test_true,y_pred_classes)\n","        precision=precision_score(y_test_true,y_pred_classes,average='weighted',zero_division=0,labels=np.arange(len(unique_labels_in_data)))\n","        recall=recall_score(y_test_true,y_pred_classes,average='weighted',zero_division=0,labels=np.arange(len(unique_labels_in_data)))\n","        f1=f1_score(y_test_true,y_pred_classes,average='weighted',zero_division=0,labels=np.arange(len(unique_labels_in_data)))\n","        cm=confusion_matrix(y_test_true,y_pred_classes,labels=np.arange(len(unique_labels_in_data)))\n","        report=classification_report(y_test_true,y_pred_classes,output_dict=True,zero_division=0,labels=np.arange(len(unique_labels_in_data)), target_names=[str(cls) for cls in unique_labels_in_data])\n","\n","        roc_auc_s, fpr_d, tpr_d = None, None, None\n","        if y_pred_proba is not None and len(unique_labels_in_data)==2 and y_pred_proba.ndim==2 and y_pred_proba.shape[1]==2:\n","             fpr_d,tpr_d,_=roc_curve(y_test_true,y_pred_proba[:,1]); roc_auc_s=auc(fpr_d,tpr_d)\n","\n","        res = {'model_name':model_name,'model_type':model_type,'accuracy':float(accuracy),'precision':float(precision),'recall':float(recall),'f1_score':float(f1),'confusion_matrix':cm.tolist(),'classification_report':report,'prediction_time_total_s':float(prediction_time),'prediction_time_per_sample_ms':float(prediction_time/len(X_test_for_model)*1000) if len(X_test_for_model)>0 else 0,'memory_increase_mb':float(memory_used),'roc_auc':float(roc_auc_s) if roc_auc_s is not None else None,'fpr':fpr_d.tolist() if fpr_d is not None else None,'tpr':tpr_d.tolist() if tpr_d is not None else None,'timestamp':datetime.now().isoformat(),'num_test_samples':len(X_test_for_model)}\n","        self.results[model_name]=res; print(f\"  Benchmarked '{model_name}': Acc={accuracy:.4f}, F1={f1:.4f}, Time/sample={res['prediction_time_per_sample_ms']:.2f}ms\"); return res\n","\n","    def benchmark_all_models(self):\n","        if not self.models: print(\"No models loaded to benchmark.\"); return {}\n","        if self.test_data is None: print(\"No test data loaded to benchmark on.\"); return {}\n","        print(f\"\\n--- Benchmarking All Loaded Models ({len(self.models)}) ---\")\n","        for model_name_key in list(self.models.keys()): self.benchmark_model(model_name_key)\n","        print(\"--- All Models Benchmarked ---\"); return self.results\n","\n","# --- Test Block for ModelBenchmarker (Part 2: Benchmarking) ---\n","if __name__ == \"__main__\" and 'google.colab' in sys.modules:\n","    print(\"\\n--- Testing ModelBenchmarker (Part 2: Benchmarking) ---\")\n","\n","    # Ensure benchmarker instance exists from Section 2, or re-initialize if this cell is run alone\n","    if 'benchmarker' not in globals() or not isinstance(benchmarker, ModelBenchmarker) or \\\n","       benchmarker.test_data is None or not benchmarker.models: # Check if data/models are loaded\n","        print(\"⚠️ ModelBenchmarker instance not found, or models/data not loaded from Section 2. \" \\\n","              \"Re-initializing and attempting to load for this test.\")\n","        benchmarker = ModelBenchmarker()\n","\n","        test_h5_model_path_s3 = \"/content/drive/MyDrive/Colab Notebooks/results/lstm_model.h5\"\n","        if os.path.exists(test_h5_model_path_s3):\n","            benchmarker.load_model(test_h5_model_path_s3, model_name=\"LSTM_IDS_Model\")\n","        else: print(f\"⚠️ Test Keras model for benchmark test not found: {test_h5_model_path_s3}\")\n","\n","        dummy_sklearn_model_path_s3 = os.path.join(MODEL_DIR if 'MODEL_DIR' in globals() else \".\", \"dummy_sklearn_model.pkl\")\n","        if os.path.exists(dummy_sklearn_model_path_s3):\n","            benchmarker.load_model(dummy_sklearn_model_path_s3, model_name=\"Dummy_Sklearn_Model\")\n","        else:\n","            try:\n","                from sklearn.linear_model import LogisticRegression\n","                temp_model_sk = LogisticRegression(); temp_model_sk.fit(np.random.rand(10,2), np.random.randint(0,2,10))\n","                os.makedirs(os.path.dirname(dummy_sklearn_model_path_s3), exist_ok=True)\n","                with open(dummy_sklearn_model_path_s3, 'wb') as f: pickle.dump(temp_model_sk, f)\n","                benchmarker.load_model(dummy_sklearn_model_path_s3, model_name=\"Dummy_Sklearn_Model\")\n","            except Exception as e_sk_create: print(f\"Error creating dummy sklearn for test: {e_sk_create}\")\n","\n","        test_data_csv_path_s3 = \"/content/drive/MyDrive/Colab Notebooks/datasets/ML-EdgeIIoT-dataset.csv\"\n","        test_scaler_path_s3 = \"/content/drive/MyDrive/Colab Notebooks/results/scaler.pkl\"\n","        edge_iiot_features_s3 = [\n","            'arp.hw.size', 'http.content_length', 'http.response', 'http.tls_port',\n","            'tcp.ack_raw', 'tcp.checksum', 'tcp.connection.fin', 'tcp.connection.rst',\n","            'tcp.connection.syn', 'tcp.connection.synack', 'tcp.dstport', 'tcp.flags.ack',\n","            'tcp.len', 'udp.stream', 'udp.time_delta', 'dns.qry.qu', 'dns.qry.type',\n","            'dns.retransmission', 'dns.retransmit_request', 'dns.retransmit_request_in',\n","            'mqtt.conflag.cleansess', 'mqtt.hdrflags', 'mqtt.len', 'mqtt.msg_decoded_as',\n","            'mbtcp.len', 'mbtcp.trans_id', 'mbtcp.unit_id']\n","        edge_iiot_target_s3 = 'Attack_label'\n","        if os.path.exists(test_data_csv_path_s3):\n","            print(\"Re-loading EdgeIIoT test data for benchmarking...\")\n","            benchmarker.load_test_data(\n","                test_data_csv_path_s3, X_columns=edge_iiot_features_s3, y_column=edge_iiot_target_s3,\n","                scaler_path=test_scaler_path_s3 if os.path.exists(test_scaler_path_s3) else None,\n","                test_only=True, reshape_for_lstm_load_time=False\n","            )\n","        else:\n","            print(f\"⚠️ EdgeIIoT Test data CSV ({test_data_csv_path_s3}) not found for benchmarking.\")\n","\n","    # Proceed to benchmark only if models AND data are successfully loaded into the instance\n","    if benchmarker.models and benchmarker.test_data:\n","        all_benchmark_results = benchmarker.benchmark_all_models()\n","        if all_benchmark_results:\n","            print(\"\\n--- Overall Benchmark Summary (from test block) ---\")\n","            for model_name_res, res_data in all_benchmark_results.items():\n","                print(f\"  Model: {model_name_res}, Accuracy: {res_data['accuracy']:.4f}, F1-score: {res_data['f1_score']:.4f}\")\n","    else:\n","        print(\"⚠️ Cannot run benchmarks: Models or test data not loaded into the benchmarker instance.\")\n","        if not benchmarker.models: print(\"   - No models found in benchmarker.\")\n","        if not benchmarker.test_data: print(\"   - No test data (self.test_data) found in benchmarker.\")\n","\n","    print(\"\\n--- End of ModelBenchmarker (Part 2) Test ---\")\n","\n","print(\"\\n✅ Section 3 (Benchmarking - `ModelBenchmarker` Class - Part 2: Benchmarking) is ready.\")"],"metadata":{"id":"2icNDCCGWshD","executionInfo":{"status":"ok","timestamp":1748480457496,"user_tz":-180,"elapsed":25390,"user":{"displayName":"Graduation project CIC 2025","userId":"06762630713984150762"}},"outputId":"b0f56b3b-141a-42ba-c782-29e9381ed31c","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"output_type":"stream","name":"stdout","text":["\n","--- Testing ModelBenchmarker (Part 2: Benchmarking) ---\n","⚠️ ModelBenchmarker instance not found, or models/data not loaded from Section 2. Re-initializing and attempting to load for this test.\n","Re-loading EdgeIIoT test data for benchmarking...\n","\n","--- Benchmarking All Loaded Models (2) ---\n","\n","Benchmarking 'LSTM_IDS_Model' (Type: keras) on 157800 samples. Input X shape: (157800, 1, 27)\n","  Benchmarked 'LSTM_IDS_Model': Acc=0.9729, F1=0.9727, Time/sample=0.13ms\n","\n","Benchmarking 'Dummy_Sklearn_Model' (Type: sklearn) on 157800 samples. Input X shape: (157800, 27)\n","❌ ValueError during prediction for 'Dummy_Sklearn_Model': X has 27 features, but LogisticRegression is expecting 2 features as input.\n","   Model expected input based on its build: N/A\n","   Data provided shape: (157800, 27)\n","--- All Models Benchmarked ---\n","\n","--- Overall Benchmark Summary (from test block) ---\n","  Model: LSTM_IDS_Model, Accuracy: 0.9729, F1-score: 0.9727\n","\n","--- End of ModelBenchmarker (Part 2) Test ---\n","\n","✅ Section 3 (Benchmarking - `ModelBenchmarker` Class - Part 2: Benchmarking) is ready.\n"]}]},{"cell_type":"code","source":["# Imports from Section 1 should still be in effect.\n","# ModelBenchmarker class (Parts 1 & 2) should be defined from previous sections.\n","# Ensure matplotlib.pyplot as plt and seaborn as sns are imported.\n","\n","class ModelBenchmarker:\n","    \"\"\"\n","    Class for benchmarking and comparing different machine learning models\n","    for intrusion detection.\n","    (Includes methods from Part 1 & 2, and adds new methods from Part 3)\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.models = {}\n","        self.results = {}\n","        self.test_data = None\n","        self.X_columns = None\n","        self.y_column = None\n","        # print(\"ModelBenchmarker Initialized.\")\n","\n","    def load_model(self, model_path, model_name=None):\n","        if not os.path.exists(model_path):\n","            print(f\"❌ Error: Model file not found at {model_path}\"); return False\n","        if model_name is None: model_name = os.path.basename(model_path).split('.')[0]\n","        try:\n","            model_type = 'unknown'; model_instance = None\n","            if model_path.endswith('.h5'):\n","                model_instance = keras_load_model(model_path); model_type = 'keras'\n","            elif model_path.endswith(('.pkl', '.joblib')):\n","                with open(model_path, 'rb') as f: loaded_object = pickle.load(f)\n","                if isinstance(loaded_object, dict) and 'model' in loaded_object: model_instance = loaded_object['model']\n","                else: model_instance = loaded_object\n","                model_type = 'sklearn' if hasattr(model_instance, 'predict') else 'unknown_pickle'\n","            else:\n","                with open(model_path, 'rb') as f: model_instance = pickle.load(f)\n","                model_type = 'sklearn_generic' if hasattr(model_instance, 'predict') else 'unknown_generic'\n","            if model_instance is None: print(f\"❌ Failed to load model instance from {model_path}.\"); return False\n","            self.models[model_name] = {'model': model_instance, 'path': model_path, 'type': model_type}\n","            # print(f\"✅ Successfully loaded model '{model_name}' (Type: {model_type}).\")\n","            return True\n","        except Exception as e:\n","            print(f\"❌ Error loading model '{model_name}' from {model_path}: {e}\"); return False\n","\n","    def load_test_data(self, data_path, X_columns=None, y_column=None, preprocess_for_lstm=False, scaler_path=None):\n","        try:\n","            if data_path.endswith('.csv'): data_df = pd.read_csv(data_path, low_memory=False)\n","            elif data_path.endswith(('.json', '.jsonl')): data_df = pd.read_json(data_path, lines=data_path.endswith('.jsonl'))\n","            else: print(f\"❌ Unsupported test data format: {data_path}\"); return False\n","            data_df.replace([np.inf, -np.inf], np.nan, inplace=True); data_df.dropna(inplace=True)\n","            if data_df.empty: print(\"❌ Test data empty after NaN drop.\"); return False\n","            if y_column is None: y_column = data_df.columns[-1]\n","            self.y_column = y_column\n","            if self.y_column not in data_df.columns: print(f\"❌ Target '{self.y_column}' not in test data.\"); return False\n","            y_test_raw = data_df[self.y_column]\n","            if X_columns is None: X_columns = [col for col in data_df.columns if col != self.y_column]\n","            self.X_columns = X_columns\n","            missing_X_cols = [col for col in self.X_columns if col not in data_df.columns]\n","            if missing_X_cols: print(f\"❌ Features not in test data: {missing_X_cols}\"); return False\n","            X_test_raw = data_df[self.X_columns].copy()\n","            X_test_numeric = X_test_raw.select_dtypes(include=np.number)\n","            if X_test_numeric.empty: print(\"❌ No numeric features in X_test.\"); return False\n","            X_test_processed = X_test_numeric.copy()\n","            if X_test_numeric.shape[1] < len(self.X_columns):\n","                self.X_columns = X_test_numeric.columns.tolist()\n","            if scaler_path and os.path.exists(scaler_path):\n","                try:\n","                    with open(scaler_path, 'rb') as f: scaler = pickle.load(f)\n","                    if hasattr(scaler, 'mean_') and scaler.mean_ is not None:\n","                         X_test_processed_scaled = scaler.transform(X_test_numeric)\n","                         X_test_processed = pd.DataFrame(X_test_processed_scaled, columns=X_test_numeric.columns, index=X_test_numeric.index)\n","                    else: print(f\"⚠️ Scaler from {scaler_path} not fitted. Using unscaled.\")\n","                except Exception as e: print(f\"❌ Error applying scaler from {scaler_path}: {e}. Using unscaled.\")\n","            elif scaler_path: print(f\"⚠️ Scaler not found at {scaler_path}. Using unscaled.\")\n","            label_encoder = LabelEncoder(); y_test_processed = label_encoder.fit_transform(y_test_raw)\n","            if preprocess_for_lstm:\n","                if isinstance(X_test_processed, pd.DataFrame): X_test_processed_values = X_test_processed.values\n","                else: X_test_processed_values = X_test_processed\n","                X_test_processed = X_test_processed_values.reshape((X_test_processed_values.shape[0], 1, X_test_processed_values.shape[1]))\n","            self.test_data = {'X': X_test_processed, 'y': y_test_processed, 'label_encoder': label_encoder, 'original_X_columns': self.X_columns }\n","            return True\n","        except Exception as e:\n","            print(f\"❌ Error loading/processing test data from {data_path}: {e}\"); self.test_data=None; return False\n","\n","    def benchmark_model(self, model_name):\n","        if model_name not in self.models: print(f\"❌ Model '{model_name}' not found.\"); return None\n","        if self.test_data is None or not isinstance(self.test_data, dict) or \\\n","           'X' not in self.test_data or 'y' not in self.test_data:\n","            print(f\"❌ Test data not properly loaded for '{model_name}'.\"); return None\n","        model_info=self.models[model_name]; model=model_info['model']; model_type=model_info['type']\n","        X_test_input = self.test_data['X']; y_test_input = self.test_data['y']\n","        if isinstance(X_test_input, pd.DataFrame): X_test_for_model = X_test_input.values\n","        else: X_test_for_model = np.array(X_test_input).copy()\n","        is_model_lstm_type = (model_type == 'keras' and hasattr(model, 'layers') and any(isinstance(layer, tf.keras.layers.LSTM) for layer in model.layers))\n","        if is_model_lstm_type and len(X_test_for_model.shape) == 2:\n","            X_test_for_model = X_test_for_model.reshape(X_test_for_model.shape[0], 1, X_test_for_model.shape[1])\n","        elif not is_model_lstm_type and len(X_test_for_model.shape) == 3:\n","            X_test_for_model = X_test_for_model.reshape(X_test_for_model.shape[0], -1)\n","        start_time=time.time(); gc.collect(); process=psutil.Process(os.getpid()); memory_before=process.memory_info().rss/(1024*1024)\n","        y_pred_classes, y_pred_proba = None, None\n","        try:\n","            if model_type == 'keras':\n","                y_pred_raw = model.predict(X_test_for_model, verbose=0)\n","                if len(y_pred_raw.shape) > 1 and y_pred_raw.shape[1] > 1: y_pred_classes = np.argmax(y_pred_raw, axis=1); y_pred_proba = y_pred_raw\n","                else: y_pred_classes = (y_pred_raw > 0.5).astype(int).flatten(); y_pred_proba = np.column_stack((1-y_pred_raw.flatten(), y_pred_raw.flatten()))\n","            elif model_type.startswith('sklearn'):\n","                y_pred_classes = model.predict(X_test_for_model)\n","                if hasattr(model, 'predict_proba'): y_pred_proba = model.predict_proba(X_test_for_model)\n","            else: return None\n","        except Exception as e: print(f\"❌ Error predicting for '{model_name}': {e}\"); return None\n","        prediction_time=time.time()-start_time; gc.collect(); memory_after=process.memory_info().rss/(1024*1024); memory_used=memory_after-memory_before\n","        if y_test_input.ndim > 1 and y_test_input.shape[1]==1: y_test_input=y_test_input.ravel()\n","        if y_pred_classes.ndim > 1 and y_pred_classes.shape[1]==1: y_pred_classes=y_pred_classes.ravel()\n","        unique_labels = np.union1d(np.unique(y_test_input), np.unique(y_pred_classes))\n","        accuracy=accuracy_score(y_test_input,y_pred_classes); precision=precision_score(y_test_input,y_pred_classes,average='weighted',zero_division=0,labels=unique_labels if len(unique_labels)>0 else None); recall=recall_score(y_test_input,y_pred_classes,average='weighted',zero_division=0,labels=unique_labels if len(unique_labels)>0 else None); f1=f1_score(y_test_input,y_pred_classes,average='weighted',zero_division=0,labels=unique_labels if len(unique_labels)>0 else None)\n","        cm=confusion_matrix(y_test_input,y_pred_classes,labels=unique_labels if len(unique_labels)>0 else None); report=classification_report(y_test_input,y_pred_classes,output_dict=True,zero_division=0,labels=unique_labels if len(unique_labels)>0 else None)\n","        roc_auc_s, fpr_d, tpr_d = None, None, None\n","        if y_pred_proba is not None and len(np.unique(y_test_input))==2 and y_pred_proba.ndim==2 and y_pred_proba.shape[1]==2:\n","             fpr_d,tpr_d,_=roc_curve(y_test_input,y_pred_proba[:,1]); roc_auc_s=auc(fpr_d,tpr_d)\n","        res = {'model_name':model_name,'model_type':model_type,'accuracy':float(accuracy),'precision':float(precision),'recall':float(recall),'f1_score':float(f1),'confusion_matrix':cm.tolist(),'classification_report':report,'prediction_time_total_s':float(prediction_time),'prediction_time_per_sample_ms':float(prediction_time/len(X_test_for_model)*1000) if len(X_test_for_model)>0 else 0,'memory_increase_mb':float(memory_used),'roc_auc':float(roc_auc_s) if roc_auc_s is not None else None,'fpr':fpr_d.tolist() if fpr_d is not None else None,'tpr':tpr_d.tolist() if tpr_d is not None else None,'timestamp':datetime.now().isoformat(),'num_test_samples':len(X_test_for_model)}\n","        self.results[model_name]=res; # print(f\"  Benchmarked '{model_name}': Acc={accuracy:.4f}, F1={f1:.4f}\") # Verbose\n","        return res\n","\n","    def benchmark_all_models(self):\n","        if not self.models: print(\"No models loaded.\"); return {}\n","        if self.test_data is None: print(\"No test data loaded.\"); return {}\n","        # print(f\"\\n--- Benchmarking All Loaded Models ({len(self.models)}) ---\") # Verbose\n","        for model_name in self.models.keys(): self.benchmark_model(model_name)\n","        # print(\"--- All Models Benchmarked ---\"); # Verbose\n","        return self.results\n","\n","    def compare_models(self):\n","        if not self.results: print(\"No benchmark results to compare.\"); return None\n","        metrics_to_compare = ['accuracy', 'precision', 'recall', 'f1_score', 'prediction_time_per_sample_ms', 'memory_increase_mb', 'roc_auc']\n","        comparison_data = {metric: {} for metric in metrics_to_compare}\n","        best_models_summary = {metric: {'model': None, 'value': None} for metric in metrics_to_compare}\n","        for model_name, result in self.results.items():\n","            for metric in metrics_to_compare:\n","                value = result.get(metric);\n","                if value is not None : comparison_data[metric][model_name] = value\n","        for metric in metrics_to_compare:\n","            if not comparison_data[metric]: continue\n","            if metric in ['prediction_time_per_sample_ms', 'memory_increase_mb']: best_model_name = min(comparison_data[metric], key=comparison_data[metric].get)\n","            else: best_model_name = max(comparison_data[metric], key=comparison_data[metric].get)\n","            best_models_summary[metric]['model'] = best_model_name; best_models_summary[metric]['value'] = comparison_data[metric][best_model_name]\n","        return {'metrics_data': comparison_data, 'best_models_summary': best_models_summary, 'timestamp': datetime.now().isoformat()}\n","\n","    def plot_comparison(self, metrics_to_plot=None, save_path=None, title_suffix=\"\"):\n","        if not self.results: print(\"No results to plot.\"); return None\n","        if metrics_to_plot is None: metrics_to_plot = ['accuracy', 'f1_score']\n","        df_data = []; valid_metrics_plotted = []\n","        for model_name, res in self.results.items():\n","            row = {'Model': model_name}\n","            for metric in metrics_to_plot:\n","                if res.get(metric) is not None: row[metric.replace('_', ' ').title()] = res.get(metric);\n","                if metric not in valid_metrics_plotted and res.get(metric) is not None : valid_metrics_plotted.append(metric) # Track successfully plotted metrics\n","            if len(row) > 1: df_data.append(row)\n","        if not df_data: print(f\"No data to plot for metrics: {metrics_to_plot}\"); return None\n","\n","        plot_df = pd.DataFrame(df_data).set_index('Model')\n","        # Use only columns that actually have data for plotting (derived from valid_metrics_plotted)\n","        valid_metric_titles = [m.replace('_',' ').title() for m in valid_metrics_plotted]\n","        plot_df = plot_df[[col for col in valid_metric_titles if col in plot_df.columns]] # Ensure column order and existence\n","        plot_df = plot_df.dropna(axis=1, how='all')\n","\n","        if plot_df.empty: print(f\"Not enough valid data for comparison plot for metrics: {valid_metrics_plotted}\"); return None\n","\n","        plot_df.plot(kind='bar', figsize=(8 + len(plot_df.index)*0.5, 6), rot=30, width=0.8) # Dynamic width\n","        plt.title(f\"Model Comparison: {', '.join(plot_df.columns)} {title_suffix}\"); plt.ylabel(\"Score / Value\"); plt.tight_layout();\n","        if save_path: plt.savefig(save_path); print(f\"Plot saved to {save_path}\"); plt.close(); return save_path\n","        else: plt.show(); return None\n","\n","    def plot_confusion_matrices(self, save_dir=None):\n","        if not self.results: print(\"No results for CMs.\"); return None\n","        saved_paths = [] # Initialize saved_paths at the beginning of the method\n","        for model_name, res in self.results.items():\n","            cm = np.array(res.get('confusion_matrix', []))\n","            if cm.size == 0: continue\n","            plt.figure(figsize=(5,4)); sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False); plt.title(f'CM: {model_name}'); plt.ylabel('True'); plt.xlabel('Predicted'); plt.tight_layout()\n","            if save_dir:\n","                path = os.path.join(save_dir, f\"cm_{model_name}.png\"); plt.savefig(path); saved_paths.append(path); plt.close()\n","            else: plt.show()\n","        return saved_paths if save_dir and saved_paths else None\n","\n","    def plot_roc_curves(self, save_path=None):\n","        if not self.results: print(\"No results for ROC.\"); return None\n","        plt.figure(figsize=(8,6)); any_roc_plotted = False\n","        for model_name, res in self.results.items():\n","            fpr, tpr, roc_auc = res.get('fpr'), res.get('tpr'), res.get('roc_auc')\n","            if fpr is not None and tpr is not None and roc_auc is not None:\n","                plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.3f})'); any_roc_plotted = True\n","        if not any_roc_plotted: print(\"No ROC curve data available in results.\"); plt.close(); return None\n","        plt.plot([0,1],[0,1],'k--'); plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate'); plt.title('ROC Curves'); plt.legend(loc='lower right'); plt.tight_layout()\n","        if save_path: plt.savefig(save_path); print(f\"ROC plot saved: {save_path}\"); plt.close(); return save_path\n","        else: plt.show(); return None\n","\n","    def plot_precision_recall_curves(self, save_dir=None):\n","        print(\"plot_precision_recall_curves: Needs y_scores. Placeholder.\")\n","        return None\n","\n","    def plot_latency_comparison(self, save_path=None):\n","        return self.plot_comparison(metrics_to_plot=['prediction_time_per_sample_ms'], save_path=save_path, title_suffix=\"- Latency (ms/sample)\")\n","\n","    def plot_memory_comparison(self, save_path=None):\n","        return self.plot_comparison(metrics_to_plot=['memory_increase_mb'], save_path=save_path, title_suffix=\"- Memory Increase (MB)\")\n","\n","# --- Test Block for ModelBenchmarker (Part 3: Plotting) ---\n","if __name__ == \"__main__\" and 'google.colab' in sys.modules:\n","    print(\"\\n--- Testing ModelBenchmarker (Part 3: Plotting) ---\")\n","\n","    if 'benchmarker' not in globals() or not isinstance(benchmarker, ModelBenchmarker):\n","        print(\"⚠️ ModelBenchmarker instance not found from Section 2. Re-initializing for this test.\")\n","        benchmarker = ModelBenchmarker()\n","        dummy_sklearn_model_path_s4 = os.path.join(MODEL_DIR if 'MODEL_DIR' in globals() else \".\", \"dummy_sklearn_model.pkl\")\n","        if \"Dummy_Sklearn_Model\" not in benchmarker.models:\n","            if not os.path.exists(dummy_sklearn_model_path_s4):\n","                from sklearn.linear_model import LogisticRegression\n","                temp_model_sk = LogisticRegression(); temp_model_sk.fit(np.random.rand(20,2), np.random.randint(0,2,20))\n","                with open(dummy_sklearn_model_path_s4, 'wb') as f: pickle.dump(temp_model_sk, f)\n","            benchmarker.load_model(dummy_sklearn_model_path_s4, \"Dummy_Sklearn_Model\")\n","\n","        if benchmarker.test_data is None:\n","            print(\"Plotting Test: No test data in benchmarker from Section 2. Creating minimal dummy data.\")\n","            dummy_X_plot_df = pd.DataFrame(np.random.rand(50, 2), columns=['dummy_feat1', 'dummy_feat2'])\n","            dummy_y_plot_raw = pd.Series(np.random.choice(['Normal', 'Attack'], 50))\n","            le_plot = LabelEncoder()\n","            benchmarker.test_data = {\n","                'X': dummy_X_plot_df.values,\n","                'y': le_plot.fit_transform(dummy_y_plot_raw.values),\n","                'label_encoder': le_plot, 'original_X_columns': ['dummy_feat1', 'dummy_feat2']\n","            }\n","            benchmarker.X_columns = ['dummy_feat1', 'dummy_feat2']; benchmarker.y_column = 'label'\n","            print(f\"Dummy test data created for plotting: X shape {benchmarker.test_data['X'].shape}, y shape {benchmarker.test_data['y'].shape}\")\n","\n","    if benchmarker.models and benchmarker.test_data:\n","        if not benchmarker.results:\n","            print(\"Plotting Test: No benchmark results found. Attempting to run benchmarks now...\")\n","            # Determine which models can run on current self.test_data['X']\n","            features_in_test_data = benchmarker.test_data['X'].shape[1]\n","\n","            if \"LSTM_IDS_Model\" in benchmarker.models and features_in_test_data == 27:\n","                print(\"Benchmarking LSTM_IDS_Model...\")\n","                benchmarker.benchmark_model(\"LSTM_IDS_Model\")\n","            elif \"LSTM_IDS_Model\" in benchmarker.models :\n","                 print(f\"Skipping LSTM benchmark for plotting test: test data has {features_in_test_data} features, LSTM might expect 27.\")\n","\n","\n","            if \"Dummy_Sklearn_Model\" in benchmarker.models:\n","                if features_in_test_data == 2: # If current test data is 2-feature dummy\n","                    print(\"Benchmarking Dummy_Sklearn_Model on current 2-feature test data...\")\n","                    benchmarker.benchmark_model(\"Dummy_Sklearn_Model\")\n","                else: # Create specific 2-feature test data for the dummy model\n","                     print(\"Current test data has >2 features. Creating specific 2-feature test for Dummy_Sklearn_Model.\")\n","                     original_test_data_backup = benchmarker.test_data # Backup original\n","                     X_dummy_test_for_plot = np.random.rand(30,2)\n","                     y_dummy_test_for_plot = np.random.randint(0,2,30)\n","                     benchmarker.test_data = {'X': X_dummy_test_for_plot, 'y': y_dummy_test_for_plot,\n","                                              'label_encoder': LabelEncoder().fit(y_dummy_test_for_plot)}\n","                     benchmarker.benchmark_model(\"Dummy_Sklearn_Model\")\n","                     benchmarker.test_data = original_test_data_backup # Restore\n","\n","        if benchmarker.results:\n","            print(\"\\n-- Generating Comparison Plots (using available results) --\")\n","            plots_save_dir = PLOTS_DIR if 'PLOTS_DIR' in globals() and os.path.exists(PLOTS_DIR) else \"benchmark_plots_output_s4\"\n","            os.makedirs(plots_save_dir, exist_ok=True)\n","\n","            benchmarker.plot_comparison(save_path=os.path.join(plots_save_dir, \"test_perf_compare.png\"))\n","            benchmarker.plot_latency_comparison(save_path=os.path.join(plots_save_dir, \"test_latency_comparison.png\"))\n","            benchmarker.plot_memory_comparison(save_path=os.path.join(plots_save_dir, \"test_memory_comparison.png\"))\n","            benchmarker.plot_confusion_matrices(save_dir=plots_save_dir)\n","            benchmarker.plot_roc_curves(save_path=os.path.join(plots_save_dir, \"test_roc_curves.png\"))\n","        else:\n","            print(\"⚠️ Plotting Test: Still no benchmark results after trying to run benchmarks. Plots will be skipped.\")\n","    else:\n","        print(\"⚠️ Plotting Test: Models or test data not available in benchmarker. Plots cannot be generated.\")\n","\n","    print(\"\\n--- End of ModelBenchmarker (Part 3) Test ---\")\n","\n","print(\"\\n✅ Section 4 (Benchmarking - `ModelBenchmarker` Class - Part 3: Comparison & Plotting) is ready.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6DjF95sec_BN","executionInfo":{"status":"ok","timestamp":1748480626749,"user_tz":-180,"elapsed":1356,"user":{"displayName":"Graduation project CIC 2025","userId":"06762630713984150762"}},"outputId":"7a89b076-7f29-4646-bfc5-c388680f27d6"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Testing ModelBenchmarker (Part 3: Plotting) ---\n","⚠️ ModelBenchmarker instance not found from Section 2. Re-initializing for this test.\n","Plotting Test: No test data in benchmarker from Section 2. Creating minimal dummy data.\n","Dummy test data created for plotting: X shape (50, 2), y shape (50,)\n","Plotting Test: No benchmark results found. Attempting to run benchmarks now...\n","Benchmarking Dummy_Sklearn_Model on current 2-feature test data...\n","\n","-- Generating Comparison Plots (using available results) --\n","Plot saved to /content/drive/MyDrive/IDS_AI_Suite/model_benchmarking_outputs/benchmark_outputs/plots/test_perf_compare.png\n","Plot saved to /content/drive/MyDrive/IDS_AI_Suite/model_benchmarking_outputs/benchmark_outputs/plots/test_latency_comparison.png\n","Plot saved to /content/drive/MyDrive/IDS_AI_Suite/model_benchmarking_outputs/benchmark_outputs/plots/test_memory_comparison.png\n","ROC plot saved: /content/drive/MyDrive/IDS_AI_Suite/model_benchmarking_outputs/benchmark_outputs/plots/test_roc_curves.png\n","\n","--- End of ModelBenchmarker (Part 3) Test ---\n","\n","✅ Section 4 (Benchmarking - `ModelBenchmarker` Class - Part 3: Comparison & Plotting) is ready.\n"]}]},{"cell_type":"code","source":["# Imports from Section 1 should still be in effect.\n","# ModelBenchmarker class (Parts 1, 2 & 3) should be defined from previous sections.\n","# Ensure os, json, datetime are available (should be from Section 1 imports).\n","\n","class ModelBenchmarker:\n","    \"\"\"\n","    Class for benchmarking and comparing different machine learning models\n","    for intrusion detection.\n","    (Includes methods from Parts 1, 2 & 3, and adds new methods from Part 4)\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.models = {}\n","        self.results = {}\n","        self.test_data = None\n","        self.X_columns = None\n","        self.y_column = None\n","        # print(\"ModelBenchmarker Initialized.\") # Keep less verbose\n","\n","    def load_model(self, model_path, model_name=None):\n","        if not os.path.exists(model_path):\n","            print(f\"❌ Error: Model file not found at {model_path}\"); return False\n","        if model_name is None: model_name = os.path.basename(model_path).split('.')[0]\n","        try:\n","            model_type = 'unknown'; model_instance = None\n","            if model_path.endswith('.h5'):\n","                model_instance = keras_load_model(model_path); model_type = 'keras'\n","            elif model_path.endswith(('.pkl', '.joblib')):\n","                with open(model_path, 'rb') as f: loaded_object = pickle.load(f)\n","                if isinstance(loaded_object, dict) and 'model' in loaded_object: model_instance = loaded_object['model']\n","                else: model_instance = loaded_object\n","                model_type = 'sklearn' if hasattr(model_instance, 'predict') else 'unknown_pickle'\n","            else:\n","                with open(model_path, 'rb') as f: model_instance = pickle.load(f)\n","                model_type = 'sklearn_generic' if hasattr(model_instance, 'predict') else 'unknown_generic'\n","            if model_instance is None: print(f\"❌ Failed to load model instance from {model_path}.\"); return False\n","            self.models[model_name] = {'model': model_instance, 'path': model_path, 'type': model_type}\n","            return True\n","        except Exception as e:\n","            print(f\"❌ Error loading model '{model_name}' from {model_path}: {e}\"); return False\n","\n","    def load_test_data(self, data_path, X_columns=None, y_column=None,\n","                       test_only=True, test_size_if_split=0.2,\n","                       scaler_path=None, reshape_for_lstm_load_time=False):\n","        try:\n","            if data_path.endswith('.csv'): data_df_raw = pd.read_csv(data_path, low_memory=False)\n","            elif data_path.endswith(('.json', '.jsonl')): data_df_raw = pd.read_json(data_path, lines=data_path.endswith('.jsonl'))\n","            else: print(f\"❌ Unsupported test data file format: {data_path}\"); return False\n","            data_df = data_df_raw.copy()\n","            cols_to_check_dropna = []\n","            if y_column is None: y_column = data_df.columns[-1]\n","            self.y_column = y_column\n","            if self.y_column not in data_df.columns: print(f\"❌ Target column '{self.y_column}' not found.\"); return False\n","            cols_to_check_dropna.append(self.y_column)\n","            if X_columns is None: X_columns = [col for col in data_df.columns if col != self.y_column]\n","            self.X_columns = X_columns\n","            missing_X_cols = [col for col in self.X_columns if col not in data_df.columns]\n","            if missing_X_cols: print(f\"❌ Features not found: {missing_X_cols}\"); return False\n","            cols_to_check_dropna.extend(self.X_columns)\n","            data_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n","            data_df.dropna(subset=list(set(cols_to_check_dropna)), inplace=True)\n","            if data_df.empty: print(\"❌ Test data empty after NaN drop.\"); return False\n","            current_data_for_processing = data_df\n","            if not test_only:\n","                stratify_col = current_data_for_processing[self.y_column] if len(current_data_for_processing[self.y_column].unique()) > 1 else None\n","                _, current_data_for_processing = train_test_split(current_data_for_processing, test_size=test_size_if_split, random_state=42, stratify=stratify_col)\n","            y_test_raw = current_data_for_processing[self.y_column]\n","            X_test_raw_features = current_data_for_processing[self.X_columns].copy()\n","            X_test_numeric = X_test_raw_features.select_dtypes(include=np.number)\n","            if X_test_numeric.empty: print(\"❌ No numeric features in X_test.\"); return False\n","            self.X_columns = X_test_numeric.columns.tolist()\n","            X_test_processed = X_test_numeric.copy()\n","            if scaler_path and os.path.exists(scaler_path):\n","                try:\n","                    with open(scaler_path, 'rb') as f: scaler = pickle.load(f)\n","                    if hasattr(scaler, 'mean_') and scaler.mean_ is not None:\n","                         X_test_scaled_values = scaler.transform(X_test_processed)\n","                         X_test_processed = pd.DataFrame(X_test_scaled_values, columns=X_test_processed.columns, index=X_test_processed.index)\n","                except Exception as e: print(f\"❌ Error applying scaler from {scaler_path}: {e}. Using unscaled.\")\n","            elif scaler_path: print(f\"⚠️ Scaler not found at {scaler_path}. Using unscaled.\")\n","            label_encoder = LabelEncoder(); y_test_processed = label_encoder.fit_transform(y_test_raw)\n","            X_test_final_np = X_test_processed.values if isinstance(X_test_processed, pd.DataFrame) else np.array(X_test_processed)\n","            if reshape_for_lstm_load_time: X_test_final_np = X_test_final_np.reshape((X_test_final_np.shape[0], 1, X_test_final_np.shape[1]))\n","            self.test_data = {'X': X_test_final_np, 'y': y_test_processed, 'label_encoder': label_encoder, 'feature_names': self.X_columns }\n","            return True\n","        except Exception as e:\n","            print(f\"❌ Error in load_test_data: {e}\"); self.test_data=None; return False\n","\n","    def benchmark_model(self, model_name):\n","        if model_name not in self.models: print(f\"❌ Model '{model_name}' not found.\"); return None\n","        if self.test_data is None: print(f\"❌ Test data not loaded for '{model_name}'.\"); return None\n","        model_info=self.models[model_name]; model=model_info['model']; model_type=model_info['type']\n","        X_test_input = self.test_data['X']; y_test_true = self.test_data['y']\n","        X_test_for_model = np.array(X_test_input).copy()\n","        is_model_lstm_type = (model_type == 'keras' and hasattr(model, 'layers') and any(isinstance(layer, tf.keras.layers.LSTM) for layer in model.layers))\n","        if is_model_lstm_type and len(X_test_for_model.shape) == 2: X_test_for_model = X_test_for_model.reshape(X_test_for_model.shape[0], 1, X_test_for_model.shape[1])\n","        elif not is_model_lstm_type and len(X_test_for_model.shape) == 3 and X_test_for_model.shape[1] == 1: X_test_for_model = X_test_for_model.reshape(X_test_for_model.shape[0], -1)\n","        start_time=time.time(); gc.collect(); process=psutil.Process(os.getpid()); memory_before=process.memory_info().rss/(1024*1024)\n","        y_pred_classes, y_pred_proba = None, None\n","        try:\n","            if model_type == 'keras':\n","                y_pred_raw = model.predict(X_test_for_model, verbose=0)\n","                if len(y_pred_raw.shape) > 1 and y_pred_raw.shape[1] > 1: y_pred_classes = np.argmax(y_pred_raw, axis=1); y_pred_proba = y_pred_raw\n","                else: y_pred_classes = (y_pred_raw > 0.5).astype(int).ravel(); y_pred_proba = np.column_stack((1-y_pred_raw.ravel(), y_pred_raw.ravel()))\n","            elif model_type.startswith('sklearn'):\n","                y_pred_classes = model.predict(X_test_for_model)\n","                if hasattr(model, 'predict_proba'): y_pred_proba = model.predict_proba(X_test_for_model)\n","            else: return None\n","        except ValueError as ve: print(f\"❌ ValueError predicting for '{model_name}': {ve}\"); return None\n","        except Exception as e: print(f\"❌ Error predicting for '{model_name}': {e}\"); return None\n","        prediction_time=time.time()-start_time; gc.collect(); memory_after=process.memory_info().rss/(1024*1024); memory_used=memory_after-memory_before\n","        if y_test_true.ndim > 1 and y_test_true.shape[1]==1: y_test_true=y_test_true.ravel()\n","        if y_pred_classes.ndim > 1 and y_pred_classes.shape[1]==1: y_pred_classes=y_pred_classes.ravel()\n","        unique_labels_in_data = self.test_data['label_encoder'].classes_\n","        accuracy=accuracy_score(y_test_true,y_pred_classes); precision=precision_score(y_test_true,y_pred_classes,average='weighted',zero_division=0,labels=np.arange(len(unique_labels_in_data))); recall=recall_score(y_test_true,y_pred_classes,average='weighted',zero_division=0,labels=np.arange(len(unique_labels_in_data))); f1=f1_score(y_test_true,y_pred_classes,average='weighted',zero_division=0,labels=np.arange(len(unique_labels_in_data)))\n","        cm=confusion_matrix(y_test_true,y_pred_classes,labels=np.arange(len(unique_labels_in_data))); report=classification_report(y_test_true,y_pred_classes,output_dict=True,zero_division=0,labels=np.arange(len(unique_labels_in_data)), target_names=[str(cls) for cls in unique_labels_in_data])\n","        roc_auc_s, fpr_d, tpr_d = None, None, None\n","        if y_pred_proba is not None and len(unique_labels_in_data)==2 and y_pred_proba.ndim==2 and y_pred_proba.shape[1]==2:\n","             fpr_d,tpr_d,_=roc_curve(y_test_true,y_pred_proba[:,1]); roc_auc_s=auc(fpr_d,tpr_d)\n","        res = {'model_name':model_name,'model_type':model_type,'accuracy':float(accuracy),'precision':float(precision),'recall':float(recall),'f1_score':float(f1),'confusion_matrix':cm.tolist(),'classification_report':report,'prediction_time_total_s':float(prediction_time),'prediction_time_per_sample_ms':float(prediction_time/len(X_test_for_model)*1000) if len(X_test_for_model)>0 else 0,'memory_increase_mb':float(memory_used),'roc_auc':float(roc_auc_s) if roc_auc_s is not None else None,'fpr':fpr_d.tolist() if fpr_d is not None else None,'tpr':tpr_d.tolist() if tpr_d is not None else None,'timestamp':datetime.now().isoformat(),'num_test_samples':len(X_test_for_model)}\n","        self.results[model_name]=res;\n","        return res\n","\n","    def benchmark_all_models(self):\n","        if not self.models: print(\"No models loaded.\"); return {}\n","        if self.test_data is None: print(\"No test data loaded.\"); return {}\n","        for model_name_key in list(self.models.keys()): self.benchmark_model(model_name_key)\n","        return self.results\n","\n","    def compare_models(self):\n","        if not self.results: print(\"No benchmark results to compare.\"); return None\n","        metrics_to_compare = ['accuracy', 'precision', 'recall', 'f1_score', 'prediction_time_per_sample_ms', 'memory_increase_mb', 'roc_auc']\n","        comparison_data = {metric: {} for metric in metrics_to_compare}\n","        best_models_summary = {metric: {'model': None, 'value': None} for metric in metrics_to_compare}\n","        for model_name, result in self.results.items():\n","            for metric in metrics_to_compare:\n","                value = result.get(metric);\n","                if value is not None : comparison_data[metric][model_name] = value\n","        for metric in metrics_to_compare:\n","            if not comparison_data[metric]: continue\n","            if metric in ['prediction_time_per_sample_ms', 'memory_increase_mb']: best_model_name = min(comparison_data[metric], key=comparison_data[metric].get)\n","            else: best_model_name = max(comparison_data[metric], key=comparison_data[metric].get)\n","            best_models_summary[metric]['model'] = best_model_name; best_models_summary[metric]['value'] = comparison_data[metric][best_model_name]\n","        return {'metrics_data': comparison_data, 'best_models_summary': best_models_summary, 'timestamp': datetime.now().isoformat()}\n","\n","    def plot_comparison(self, metrics_to_plot=None, save_path=None, title_suffix=\"\"):\n","        if not self.results: print(\"No results to plot.\"); return None\n","        if metrics_to_plot is None: metrics_to_plot = ['accuracy', 'f1_score']\n","        df_data = []; valid_metrics_plotted = []\n","        for model_name, res in self.results.items():\n","            row = {'Model': model_name}\n","            for metric in metrics_to_plot:\n","                if res.get(metric) is not None: row[metric.replace('_', ' ').title()] = res.get(metric);\n","                if metric not in valid_metrics_plotted and res.get(metric) is not None : valid_metrics_plotted.append(metric)\n","            if len(row) > 1: df_data.append(row)\n","        if not df_data: print(f\"No data to plot for metrics: {metrics_to_plot}\"); return None\n","        plot_df = pd.DataFrame(df_data).set_index('Model');\n","        valid_metric_titles = [m.replace('_',' ').title() for m in valid_metrics_plotted if m.replace('_',' ').title() in plot_df.columns]\n","        if not valid_metric_titles: print(f\"No valid metric titles for plot: {metrics_to_plot}\"); return None # Added check\n","        plot_df = plot_df[valid_metric_titles]\n","        plot_df = plot_df.dropna(axis=1, how='all')\n","        if plot_df.empty: print(f\"Not enough valid data for comparison plot for metrics: {valid_metrics_plotted}\"); return None\n","        plot_df.plot(kind='bar', figsize=(8 + len(plot_df.index)*0.5, 6), rot=30, width=0.8)\n","        plt.title(f\"Model Comparison: {', '.join(plot_df.columns)} {title_suffix}\"); plt.ylabel(\"Score / Value\"); plt.tight_layout();\n","        if save_path: plt.savefig(save_path); print(f\"Plot saved to {save_path}\"); plt.close(); return save_path\n","        else: plt.show(); return None\n","\n","    def plot_confusion_matrices(self, save_dir=None):\n","        if not self.results: print(\"No results for CMs.\"); return None\n","        saved_paths = []\n","        for model_name, res in self.results.items():\n","            cm = np.array(res.get('confusion_matrix', []))\n","            if cm.size == 0: continue\n","            plt.figure(figsize=(6,5)); sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False); plt.title(f'CM: {model_name}'); plt.ylabel('True'); plt.xlabel('Predicted'); plt.tight_layout()\n","            if save_dir: path = os.path.join(save_dir, f\"cm_{model_name}.png\"); plt.savefig(path); saved_paths.append(path); plt.close()\n","            else: plt.show()\n","        return saved_paths if save_dir and saved_paths else None\n","\n","    def plot_roc_curves(self, save_path=None):\n","        if not self.results: print(\"No results for ROC.\"); return None\n","        plt.figure(figsize=(8,6)); any_roc_plotted = False\n","        for model_name, res in self.results.items():\n","            fpr_list, tpr_list, roc_auc_val = res.get('fpr'), res.get('tpr'), res.get('roc_auc')\n","            if isinstance(fpr_list, list) and isinstance(tpr_list, list) and isinstance(roc_auc_val, float):\n","                plt.plot(fpr_list, tpr_list, label=f'{model_name} (AUC = {roc_auc_val:.3f})'); any_roc_plotted = True\n","        if not any_roc_plotted: print(\"No valid ROC curve data in results.\"); plt.close(); return None\n","        plt.plot([0,1],[0,1],'k--'); plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate'); plt.title('ROC Curves'); plt.legend(loc='lower right'); plt.tight_layout()\n","        if save_path: plt.savefig(save_path); print(f\"ROC plot saved: {save_path}\"); plt.close(); return save_path\n","        else: plt.show(); return None\n","\n","    def plot_precision_recall_curves(self, save_dir=None): # Placeholder from original script\n","        print(\"plot_precision_recall_curves: Functionality to plot actual curves requires storing y_scores \" \\\n","              \"from each model's prediction probabilities. Currently, only average precision might be available via classification_report if calculated.\")\n","        return None\n","\n","    def plot_latency_comparison(self, save_path=None):\n","        return self.plot_comparison(metrics_to_plot=['prediction_time_per_sample_ms'], save_path=save_path, title_suffix=\"- Latency (ms/sample)\")\n","\n","    def plot_memory_comparison(self, save_path=None):\n","        return self.plot_comparison(metrics_to_plot=['memory_increase_mb'], save_path=save_path, title_suffix=\"- Memory Increase (MB)\")\n","\n","    # --- New methods for Section 5 ---\n","    def generate_comparison_report(self, output_dir=None):\n","        \"\"\"\n","        Generate a comprehensive comparison report with plots and metrics.\n","        Saves JSON results and calls plotting functions.\n","        \"\"\"\n","        if not self.results:\n","            print(\"No benchmark results available to generate a report.\")\n","            return None\n","\n","        if output_dir is None:\n","            report_time_str = datetime.now().strftime('%Y%m%d_%H%M%S')\n","            base_results_dir = RESULTS_DIR if 'RESULTS_DIR' in globals() and os.path.exists(RESULTS_DIR) else \".\"\n","            output_dir = os.path.join(base_results_dir, f\"model_benchmark_report_{report_time_str}\")\n","\n","        os.makedirs(output_dir, exist_ok=True)\n","        current_report_plots_dir = os.path.join(output_dir, \"plots\")\n","        os.makedirs(current_report_plots_dir, exist_ok=True)\n","\n","        print(f\"\\nGenerating comparison report in: {output_dir}\")\n","\n","        comparison_summary = self.compare_models()\n","\n","        plot_paths = {}\n","        plot_paths['performance'] = self.plot_comparison(\n","            metrics_to_plot=['accuracy', 'precision', 'recall', 'f1_score'],\n","            save_path=os.path.join(current_report_plots_dir, \"performance_metrics_comparison.png\")\n","        )\n","        plot_paths['latency'] = self.plot_latency_comparison(\n","            save_path=os.path.join(current_report_plots_dir, \"latency_comparison.png\")\n","        )\n","        plot_paths['memory'] = self.plot_memory_comparison(\n","            save_path=os.path.join(current_report_plots_dir, \"memory_usage_comparison.png\")\n","        )\n","        plot_paths['confusion_matrices_list'] = self.plot_confusion_matrices(save_dir=current_report_plots_dir)\n","        plot_paths['roc_curves_combined'] = self.plot_roc_curves(save_path=os.path.join(current_report_plots_dir, \"roc_curves_comparison.png\"))\n","\n","        all_results_path = os.path.join(output_dir, \"all_benchmark_results.json\")\n","        try:\n","            with open(all_results_path, 'w') as f: json.dump(self.results, f, indent=2, cls=NpEncoder)\n","            print(f\"Detailed benchmark results saved to: {all_results_path}\")\n","        except Exception as e: print(f\"❌ Error saving detailed benchmark results: {e}\")\n","\n","        comparison_summary_path = None\n","        if comparison_summary:\n","            comparison_summary_path = os.path.join(output_dir, \"comparison_summary_metrics.json\")\n","            try:\n","                with open(comparison_summary_path, 'w') as f: json.dump(comparison_summary, f, indent=2, cls=NpEncoder)\n","                print(f\"Comparison summary metrics saved to: {comparison_summary_path}\")\n","            except Exception as e: print(f\"❌ Error saving comparison summary: {e}\")\n","\n","        html_report_path = self.generate_html_report(\n","            comparison_summary_data=comparison_summary,\n","            all_results_data=self.results,\n","            plot_paths_dict=plot_paths,\n","            base_report_dir=output_dir\n","        )\n","\n","        final_report_info = {\n","            'report_directory': output_dir, 'all_results_json': all_results_path,\n","            'comparison_summary_json': comparison_summary_path,\n","            'plot_paths_dict': plot_paths, 'html_report_path': html_report_path\n","        }\n","        print(f\"\\n✅ Comprehensive benchmark report generated successfully in {output_dir}\")\n","        return final_report_info\n","\n","    def generate_html_report(self, comparison_summary_data, all_results_data, plot_paths_dict, base_report_dir):\n","        if not all_results_data:\n","            print(\"Cannot generate HTML: No all_results_data provided.\"); return None\n","\n","        output_html_path = os.path.join(base_report_dir, \"benchmark_report.html\")\n","\n","        html_content = f\"\"\"<!DOCTYPE html><html lang=\"en\"><head><meta charset=\"UTF-8\"><title>IDS-AI Model Benchmarking Report</title>\n","        <style> body {{ font-family: Arial, sans-serif; margin: 20px; line-height: 1.6; color: #333; }} h1, h2, h3 {{ color: #2c3e50; border-bottom: 1px solid #bdc3c7; padding-bottom: 5px; }} h1 {{ text-align: center; }} table {{ border-collapse: collapse; width: 95%; margin: 20px auto; box-shadow: 0 0 5px #ccc; }} th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }} th {{ background-color: #3498db; color: white; }} tr:nth-child(even) {{ background-color: #f8f9f9; }} .plot-container {{ text-align: center; margin:15px 0; padding:10px; border:1px solid #eee; border-radius:4px;}} .plot {{ max-width:80%; height:auto; border:1px solid #ccc;}} .section {{ margin-bottom:30px; padding:15px; background-color:#f4f6f6; border-radius:5px;}} .highlight {{ background-color: #fff3cd; font-weight: bold; }} details > summary {{ cursor: pointer; font-weight: bold; color: #2980b9; margin-bottom: 5px;}}</style></head><body>\n","            <h1>IDS-AI Model Benchmarking Report</h1><p>Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\"\"\"\n","\n","        if comparison_summary_data and 'metrics_data' in comparison_summary_data:\n","            html_content += \"<div class='section'><h2>Overall Performance Metrics</h2><table><tr><th>Model</th>\"\n","            metric_headers = sorted(list(comparison_summary_data['metrics_data'].keys()))\n","            for mh in metric_headers: html_content += f\"<th>{mh.replace('_',' ').title()}</th>\"\n","            html_content += \"</tr>\"\n","            model_names_sorted = sorted(list(all_results_data.keys()))\n","            for mn in model_names_sorted:\n","                html_content += f\"<tr><td>{mn}</td>\"\n","                for mh in metric_headers:\n","                    val = comparison_summary_data['metrics_data'][mh].get(mn, 'N/A')\n","                    fmt_val = f\"{val:.4f}\" if isinstance(val,float) and mh not in ['prediction_time_per_sample_ms','memory_increase_mb'] else (f\"{val:.2f}\" if isinstance(val,float) else str(val))\n","                    is_best = comparison_summary_data.get('best_models_summary',{}).get(mh,{}).get('model') == mn\n","                    html_content += f\"<td class='{'highlight' if is_best else ''}'>{fmt_val}</td>\"\n","                html_content += \"</tr>\"\n","            html_content += \"</table></div>\"\n","\n","        plot_display_order_map = {'performance': (\"Performance Metrics Comparison\", plot_paths_dict.get('performance')), 'latency': (\"Prediction Latency Comparison\", plot_paths_dict.get('latency')), 'memory': (\"Memory Usage Increase Comparison\", plot_paths_dict.get('memory')), 'roc_curves_combined': (\"ROC Curves (Combined)\", plot_paths_dict.get('roc_curves_combined')), 'confusion_matrices_list': (\"Confusion Matrices\", plot_paths_dict.get('confusion_matrices_list'))}\n","        for plot_key, (title, plot_item) in plot_display_order_map.items():\n","            if plot_item:\n","                html_content += f\"<div class='section'><h2>{title}</h2>\"\n","                if plot_key == 'confusion_matrices_list' and isinstance(plot_item, list):\n","                    for cm_abs_path in plot_item:\n","                        if cm_abs_path and os.path.exists(cm_abs_path):\n","                            cm_filename = os.path.basename(cm_abs_path); model_name_from_cm = cm_filename.replace('cm_', '').replace('.png', '').replace('_', ' ').title()\n","                            html_content += f\"<h3>{model_name_from_cm}</h3><div class='plot-container'><img class='plot' src='plots/{cm_filename}' alt='CM {model_name_from_cm}'></div>\"\n","                elif isinstance(plot_item, str) and os.path.exists(plot_item):\n","                    plot_filename = os.path.basename(plot_item)\n","                    html_content += f\"<div class='plot-container'><img class='plot' src='plots/{plot_filename}' alt='{title}'></div>\"\n","                html_content += \"</div>\"\n","\n","        html_content += \"<div class='section'><h2>Detailed Classification Reports</h2>\"\n","        for model_name, res_data in all_results_data.items():\n","            html_content += f\"<details><summary>{model_name}</summary><table><tr><th>Class/Metric</th><th>Precision</th><th>Recall</th><th>F1-Score</th><th>Support</th></tr>\"\n","            if 'classification_report' in res_data and isinstance(res_data['classification_report'], dict):\n","                for lbl, m_dict in res_data['classification_report'].items():\n","                    if isinstance(m_dict, dict): html_content+=f\"<tr><td>{lbl}</td><td>{m_dict.get('precision',0.0):.4f}</td><td>{m_dict.get('recall',0.0):.4f}</td><td>{m_dict.get('f1-score',0.0):.4f}</td><td>{m_dict.get('support','')}</td></tr>\"\n","            html_content += \"</table></details>\"\n","        html_content += \"</div></body></html>\"\n","        try:\n","            with open(output_html_path, 'w', encoding='utf-8') as f: f.write(html_content)\n","            print(f\"✅ HTML report saved to {output_html_path}\"); return output_html_path\n","        except Exception as e: print(f\"❌ Error saving HTML report: {e}\"); return None\n","\n","# Helper class for JSON encoding\n","if 'NpEncoder' not in globals(): # Define only if not already defined (e.g. if running cells out of order)\n","    class NpEncoder(json.JSONEncoder):\n","        def default(self, obj):\n","            if isinstance(obj, np.integer): return int(obj)\n","            if isinstance(obj, np.floating): return float(obj)\n","            if isinstance(obj, np.ndarray): return obj.tolist()\n","            return super(NpEncoder, self).default(obj)\n","\n","# --- Test Block for ModelBenchmarker (Part 4: Report Generation) ---\n","if __name__ == \"__main__\" and 'google.colab' in sys.modules:\n","    print(\"\\n--- Testing ModelBenchmarker (Part 4: Report Generation) ---\")\n","\n","    if 'benchmarker' not in globals() or not isinstance(benchmarker, ModelBenchmarker):\n","        print(\"⚠️ ModelBenchmarker instance not found from previous sections. Re-initializing for this test.\")\n","        benchmarker = ModelBenchmarker()\n","        dummy_model_path_s5 = os.path.join(MODEL_DIR if 'MODEL_DIR' in globals() else \".\", \"dummy_sklearn_model.pkl\")\n","        if not os.path.exists(dummy_model_path_s5): # Ensure dummy model pkl exists for test\n","            from sklearn.linear_model import LogisticRegression\n","            temp_model_sk = LogisticRegression(); temp_model_sk.fit(np.random.rand(10,2), np.random.randint(0,2,10))\n","            os.makedirs(os.path.dirname(dummy_model_path_s5), exist_ok=True)\n","            with open(dummy_model_path_s5, 'wb') as f: pickle.dump(temp_model_sk, f)\n","            print(f\"Created dummy model for report test: {dummy_model_path_s5}\")\n","        benchmarker.load_model(dummy_model_path_s5, \"DummyModelForReport\")\n","\n","        # Create minimal dummy results if none exist, for the loaded dummy model\n","        if not benchmarker.results and \"DummyModelForReport\" in benchmarker.models:\n","            print(\"Creating dummy benchmark results for 'DummyModelForReport' for report generation test...\")\n","            # Ensure self.test_data is set up with compatible (e.g., 2-feature) data\n","            dummy_X_report = np.random.rand(20,2)\n","            dummy_y_report_raw = np.random.choice(['Normal', 'Attack'], 20)\n","            le_rep = LabelEncoder()\n","            benchmarker.test_data = {\n","                'X': dummy_X_report,\n","                'y': le_rep.fit_transform(dummy_y_report_raw),\n","                'label_encoder': le_rep,\n","                'feature_names': ['dummy_feat1', 'dummy_feat2']\n","            }\n","            benchmarker.benchmark_model(\"DummyModelForReport\") # Benchmark this specific model\n","\n","    if benchmarker.results:\n","        print(\"\\n-- Generating Full Comparison Report --\")\n","        # Ensure RESULTS_DIR is defined and exists\n","        report_output_base_dir = RESULTS_DIR if 'RESULTS_DIR' in globals() and os.path.exists(RESULTS_DIR) else \"benchmark_report_output_s5_default\"\n","        os.makedirs(report_output_base_dir, exist_ok=True) # Ensure base for timestamped folder exists\n","\n","        # generate_comparison_report will create its own timestamped subdirectory within report_output_base_dir\n","        report_info = benchmarker.generate_comparison_report(output_dir=None) # Let it create timestamped dir\n","\n","        if report_info and report_info.get('html_report_path'):\n","            print(f\"Full report generated in directory: {report_info['report_directory']}\")\n","            print(f\"HTML report should be viewable at: {report_info['html_report_path']}\")\n","            # To display HTML in Colab (optional):\n","            # from IPython.display import HTML, display\n","            # if os.path.exists(report_info['html_report_path']):\n","            #    display(HTML(filename=report_info['html_report_path']))\n","            # else:\n","            #    print(f\"HTML file not found at {report_info['html_report_path']} for display.\")\n","        else:\n","            print(\"⚠️ Report generation failed or produced no HTML output path.\")\n","    else:\n","        print(\"⚠️ No benchmark results available in benchmarker. Cannot generate report.\")\n","\n","    print(\"\\n--- End of ModelBenchmarker (Part 4) Test ---\")\n","\n","print(\"\\n✅ Section 5 (Benchmarking - `ModelBenchmarker` Class - Part 4: Report Generation) is ready.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kj5BvfoSfODu","executionInfo":{"status":"ok","timestamp":1748480648456,"user_tz":-180,"elapsed":3742,"user":{"displayName":"Graduation project CIC 2025","userId":"06762630713984150762"}},"outputId":"6c11b2c0-99ac-464f-e160-967d35660f24"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Testing ModelBenchmarker (Part 4: Report Generation) ---\n","⚠️ ModelBenchmarker instance not found from previous sections. Re-initializing for this test.\n","Creating dummy benchmark results for 'DummyModelForReport' for report generation test...\n","\n","-- Generating Full Comparison Report --\n","\n","Generating comparison report in: /content/drive/MyDrive/IDS_AI_Suite/model_benchmarking_outputs/benchmark_outputs/model_benchmark_report_20250529_010407\n","Plot saved to /content/drive/MyDrive/IDS_AI_Suite/model_benchmarking_outputs/benchmark_outputs/model_benchmark_report_20250529_010407/plots/performance_metrics_comparison.png\n","Plot saved to /content/drive/MyDrive/IDS_AI_Suite/model_benchmarking_outputs/benchmark_outputs/model_benchmark_report_20250529_010407/plots/latency_comparison.png\n","Plot saved to /content/drive/MyDrive/IDS_AI_Suite/model_benchmarking_outputs/benchmark_outputs/model_benchmark_report_20250529_010407/plots/memory_usage_comparison.png\n","ROC plot saved: /content/drive/MyDrive/IDS_AI_Suite/model_benchmarking_outputs/benchmark_outputs/model_benchmark_report_20250529_010407/plots/roc_curves_comparison.png\n","Detailed benchmark results saved to: /content/drive/MyDrive/IDS_AI_Suite/model_benchmarking_outputs/benchmark_outputs/model_benchmark_report_20250529_010407/all_benchmark_results.json\n","Comparison summary metrics saved to: /content/drive/MyDrive/IDS_AI_Suite/model_benchmarking_outputs/benchmark_outputs/model_benchmark_report_20250529_010407/comparison_summary_metrics.json\n","✅ HTML report saved to /content/drive/MyDrive/IDS_AI_Suite/model_benchmarking_outputs/benchmark_outputs/model_benchmark_report_20250529_010407/benchmark_report.html\n","\n","✅ Comprehensive benchmark report generated successfully in /content/drive/MyDrive/IDS_AI_Suite/model_benchmarking_outputs/benchmark_outputs/model_benchmark_report_20250529_010407\n","Full report generated in directory: /content/drive/MyDrive/IDS_AI_Suite/model_benchmarking_outputs/benchmark_outputs/model_benchmark_report_20250529_010407\n","HTML report should be viewable at: /content/drive/MyDrive/IDS_AI_Suite/model_benchmarking_outputs/benchmark_outputs/model_benchmark_report_20250529_010407/benchmark_report.html\n","\n","--- End of ModelBenchmarker (Part 4) Test ---\n","\n","✅ Section 5 (Benchmarking - `ModelBenchmarker` Class - Part 4: Report Generation) is ready.\n"]}]},{"cell_type":"code","source":["# Imports from Section 1 should still be in effect.\n","# ModelBenchmarker class (all parts) should be defined from previous sections.\n","# Global directories (BASE_DIR, RESULTS_DIR, etc.) should also be defined.\n","\n","def main():\n","    \"\"\"\n","    Main function to run the model comparison and benchmarking.\n","    Handles argument parsing (simulated for Colab) and uses ModelBenchmarker.\n","    \"\"\"\n","    # For Colab, we simulate command-line arguments.\n","    # In a standalone .py script, argparse would parse them.\n","    class SimulatedBenchmarkingArgs:\n","        def __init__(self):\n","            # --- Paths to your trained models ---\n","            # You'll need to provide paths to the models you want to compare.\n","            # Example: Your LSTM model and the dummy sklearn model\n","            self.models = [\n","                \"/content/drive/MyDrive/Colab Notebooks/results/lstm_model.h5\", # Path to your LSTM model\n","                # Add paths to other models, e.g., the dummy sklearn model we created:\n","                # os.path.join(MODEL_DIR, \"dummy_sklearn_model.pkl\") # MODEL_DIR from Section 1\n","            ]\n","            # For testing, ensure dummy_sklearn_model.pkl exists in MODEL_DIR or provide its full path\n","\n","            # --- Path to your test dataset ---\n","            self.data = \"/content/drive/MyDrive/Colab Notebooks/datasets/ML-EdgeIIoT-dataset.csv\"\n","\n","            # --- Feature columns and target column for your dataset ---\n","            self.X_columns = [ # Based on your MLP_LSTM.ipynb\n","                'arp.hw.size', 'http.content_length', 'http.response', 'http.tls_port',\n","                'tcp.ack_raw', 'tcp.checksum', 'tcp.connection.fin', 'tcp.connection.rst',\n","                'tcp.connection.syn', 'tcp.connection.synack', 'tcp.dstport', 'tcp.flags.ack',\n","                'tcp.len', 'udp.stream', 'udp.time_delta', 'dns.qry.qu', 'dns.qry.type',\n","                'dns.retransmission', 'dns.retransmit_request', 'dns.retransmit_request_in',\n","                'mqtt.conflag.cleansess', 'mqtt.hdrflags', 'mqtt.len', 'mqtt.msg_decoded_as',\n","                'mbtcp.len', 'mbtcp.trans_id', 'mbtcp.unit_id'\n","            ]\n","            self.y_column = 'Attack_label' # Target column for ML-EdgeIIoT-dataset.csv\n","\n","            # --- Path to your saved scaler (from training the models) ---\n","            self.scaler_path = \"/content/drive/MyDrive/Colab Notebooks/results/scaler.pkl\"\n","\n","            # --- Output directory for results ---\n","            # If None, ModelBenchmarker.generate_comparison_report will create a timestamped one\n","            self.output_dir = None\n","            # Example: self.output_dir = os.path.join(RESULTS_DIR, \"my_benchmark_run_1\")\n","\n","            # --- Flags ---\n","            # The original script had --compare-fl and related args.\n","            # This simplified main function focuses on the ModelBenchmarker's primary role.\n","            # If FL vs CL comparison is needed here, it would require integrating\n","            # the compare_federated_vs_centralized function and its specific args.\n","            # For now, we focus on benchmarking a list of pre-trained models.\n","\n","    args = SimulatedBenchmarkingArgs()\n","\n","    print(\"--- Model Benchmarking System ---\")\n","    print(f\"Models to benchmark: {args.models}\")\n","    print(f\"Test dataset: {args.data}\")\n","    print(f\"Output directory for report (if None, timestamped dir created): {args.output_dir or os.path.join(RESULTS_DIR, 'timestamped_report')}\")\n","\n","    benchmarker = ModelBenchmarker()\n","\n","    # 1. Load Test Data\n","    if not args.data or not os.path.exists(args.data):\n","        print(f\"❌ Test data file not found at '{args.data}'. Cannot proceed with benchmarking.\")\n","        return\n","\n","    data_loaded = benchmarker.load_test_data(\n","        data_path=args.data,\n","        X_columns=args.X_columns,\n","        y_column=args.y_column,\n","        scaler_path=args.scaler_path if args.scaler_path and os.path.exists(args.scaler_path) else None,\n","        test_only=True # Assume the provided data is a dedicated test set\n","    )\n","    if not data_loaded or benchmarker.test_data is None:\n","        print(\"❌ Failed to load or process test data. Aborting benchmark.\")\n","        return\n","\n","    # 2. Load Models\n","    models_loaded_count = 0\n","    if args.models:\n","        for model_path_arg in args.models:\n","            if model_path_arg and os.path.exists(model_path_arg):\n","                benchmarker.load_model(model_path_arg) # Model name will be derived from filename\n","                if os.path.basename(model_path_arg).split('.')[0] in benchmarker.models:\n","                    models_loaded_count+=1\n","            else:\n","                print(f\"⚠️ Model path specified but not found: {model_path_arg}\")\n","\n","    if models_loaded_count == 0: # Check if any models were actually loaded\n","        print(\"❌ No models were successfully loaded. Aborting benchmark.\")\n","        # Try to create and load the dummy model as a fallback for testing the script structure\n","        print(\"Attempting to load/create a dummy model for structural testing...\")\n","        dummy_model_path_main = os.path.join(MODEL_DIR if 'MODEL_DIR' in globals() else \".\", \"dummy_main_sklearn_model.pkl\")\n","        if not os.path.exists(dummy_model_path_main):\n","            try:\n","                from sklearn.linear_model import LogisticRegression\n","                temp_model_sk = LogisticRegression(); temp_model_sk.fit(np.random.rand(10,2), np.random.randint(0,2,10))\n","                with open(dummy_model_path_main, 'wb') as f: pickle.dump(temp_model_sk, f)\n","            except Exception as e_create_dummy: print(f\"Error creating dummy model: {e_create_dummy}\")\n","\n","        if os.path.exists(dummy_model_path_main):\n","            benchmarker.load_model(dummy_model_path_main, \"DummySklearnForMain\")\n","            if \"DummySklearnForMain\" in benchmarker.models:\n","                 print(f\"Loaded DummySklearnForMain. Test data features might not match its training (2 features).\")\n","                 # For the dummy model, if the main test_data has many features, benchmarking will fail for it.\n","                 # This script assumes all models benchmarked can use the same self.test_data.\n","                 # For a robust dummy test here, we'd need to create specific 2-feature test data if only dummy is loaded.\n","            else:\n","                print(\"Could not load/create even a dummy model. Benchmark cannot proceed.\")\n","                return\n","        else:\n","            print(\"Could not load/create even a dummy model. Benchmark cannot proceed.\")\n","            return\n","\n","\n","    # 3. Benchmark All Loaded Models\n","    if benchmarker.models and benchmarker.test_data:\n","        print(\"\\n--- Starting Benchmarking Process ---\")\n","        benchmarker.benchmark_all_models()\n","    else:\n","        print(\"❌ Cannot start benchmarking: No models loaded or no test data available.\")\n","        return\n","\n","    # 4. Generate Comparison Report\n","    if benchmarker.results:\n","        print(\"\\n--- Generating Final Comparison Report ---\")\n","        benchmarker.generate_comparison_report(output_dir=args.output_dir)\n","    else:\n","        print(\"ℹ️ No benchmark results were generated. Report cannot be created.\")\n","\n","    print(\"\\n🏁 Model Benchmarking Main Execution Finished.\")\n","\n","# This block ensures main() is called when the cell is run in Colab\n","if __name__ == \"__main__\" and 'google.colab' in sys.modules:\n","    print(\"\\n--- Running main() for Model Comparison and Benchmarking ---\")\n","\n","    essential_items = ['ModelBenchmarker', 'BASE_DIR', 'RESULTS_DIR', 'MODEL_DIR', 'PLOTS_DIR', 'DATA_DIR']\n","    all_defined = True\n","    for item_name in essential_items:\n","        if item_name not in globals():\n","            print(f\"🔴 CRITICAL ERROR for main(): Required item '{item_name}' is not defined. \"\n","                  \"Ensure all previous code sections (1-5) ran successfully.\")\n","            all_defined = False\n","            break\n","\n","    if all_defined:\n","        main()\n","    else:\n","        print(\"\\n❌ Main function execution aborted due to missing definitions from previous sections.\")\n","\n","print(\"\\n✅ Section 6 (Benchmarking - Main Execution Block) is ready.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hU0GrRMGf92S","executionInfo":{"status":"ok","timestamp":1748480670607,"user_tz":-180,"elapsed":16411,"user":{"displayName":"Graduation project CIC 2025","userId":"06762630713984150762"}},"outputId":"d10e7bd3-2f57-42de-9aa3-273c0ad1e23c"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Running main() for Model Comparison and Benchmarking ---\n","--- Model Benchmarking System ---\n","Models to benchmark: ['/content/drive/MyDrive/Colab Notebooks/results/lstm_model.h5']\n","Test dataset: /content/drive/MyDrive/Colab Notebooks/datasets/ML-EdgeIIoT-dataset.csv\n","Output directory for report (if None, timestamped dir created): /content/drive/MyDrive/IDS_AI_Suite/model_benchmarking_outputs/benchmark_outputs/timestamped_report\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"output_type":"stream","name":"stdout","text":["\n","--- Starting Benchmarking Process ---\n","\n","--- Generating Final Comparison Report ---\n","\n","Generating comparison report in: /content/drive/MyDrive/IDS_AI_Suite/model_benchmarking_outputs/benchmark_outputs/model_benchmark_report_20250529_010430\n","Plot saved to /content/drive/MyDrive/IDS_AI_Suite/model_benchmarking_outputs/benchmark_outputs/model_benchmark_report_20250529_010430/plots/performance_metrics_comparison.png\n","Plot saved to /content/drive/MyDrive/IDS_AI_Suite/model_benchmarking_outputs/benchmark_outputs/model_benchmark_report_20250529_010430/plots/latency_comparison.png\n","Plot saved to /content/drive/MyDrive/IDS_AI_Suite/model_benchmarking_outputs/benchmark_outputs/model_benchmark_report_20250529_010430/plots/memory_usage_comparison.png\n","ROC plot saved: /content/drive/MyDrive/IDS_AI_Suite/model_benchmarking_outputs/benchmark_outputs/model_benchmark_report_20250529_010430/plots/roc_curves_comparison.png\n","Detailed benchmark results saved to: /content/drive/MyDrive/IDS_AI_Suite/model_benchmarking_outputs/benchmark_outputs/model_benchmark_report_20250529_010430/all_benchmark_results.json\n","Comparison summary metrics saved to: /content/drive/MyDrive/IDS_AI_Suite/model_benchmarking_outputs/benchmark_outputs/model_benchmark_report_20250529_010430/comparison_summary_metrics.json\n","✅ HTML report saved to /content/drive/MyDrive/IDS_AI_Suite/model_benchmarking_outputs/benchmark_outputs/model_benchmark_report_20250529_010430/benchmark_report.html\n","\n","✅ Comprehensive benchmark report generated successfully in /content/drive/MyDrive/IDS_AI_Suite/model_benchmarking_outputs/benchmark_outputs/model_benchmark_report_20250529_010430\n","\n","🏁 Model Benchmarking Main Execution Finished.\n","\n","✅ Section 6 (Benchmarking - Main Execution Block) is ready.\n"]}]}]}