{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V28","mount_file_id":"1aUJ5xZqt9d8YitfIH41swoAG1D-cnqSi","authorship_tag":"ABX9TyNkbmJojW9OkIMr+vtAh/MA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","source":["!pip install imbalanced-learn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RfncCAVGM3oV","executionInfo":{"status":"ok","timestamp":1749301938765,"user_tz":-180,"elapsed":1330,"user":{"displayName":"Graduation project CIC 2025","userId":"06762630713984150762"}},"outputId":"a207f3ff-ae9c-455c-a969-c7a71491b656"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n","Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (2.0.2)\n","Requirement already satisfied: scipy<2,>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.15.3)\n","Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.6.1)\n","Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n","Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.5.1)\n","Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (3.6.0)\n"]}]},{"cell_type":"code","source":["# 1. Setup: Import Libraries and Mount Google Drive\n","import pandas as pd\n","import numpy as np\n","import os\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.impute import SimpleImputer\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from imblearn.over_sampling import SMOTE\n","from sklearn.preprocessing import LabelEncoder # Explicitly import LabelEncoder\n","\n","# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# 2. Load DataFrames and Align Columns\n","print(\"ðŸ”„ Loading datasets...\")\n","data_path = '/content/drive/MyDrive/Colab Notebooks/datasets/'\n","file1_path = os.path.join(data_path, 'DNN-EdgeIIoT-dataset.csv')\n","file2_path = os.path.join(data_path, 'ML-EdgeIIoT-dataset.csv')\n","\n","df1 = pd.read_csv(file1_path, low_memory=False)\n","df2 = pd.read_csv(file2_path, low_memory=False)\n","\n","common_cols = list(set(df1.columns).intersection(set(df2.columns)))\n","df = pd.concat([df1[common_cols], df2[common_cols]], ignore_index=True)\n","df.drop_duplicates(inplace=True)\n","print(f\"âœ… Data loaded and duplicates dropped. Shape: {df.shape}\")\n","\n","# 3. Drop Unnecessary/Identifier Columns\n","columns_to_drop = [\n","    'frame.time', 'ip.src_host', 'ip.dst_host', 'arp.src.proto_ipv4',\n","    'arp.dst.proto_ipv4', 'http.file_data', 'http.request.uri.query',\n","    'http.referer', 'http.request.full_uri', 'tcp.options', 'tcp.payload',\n","    'dns.qry.name', 'dns.qry.name.len', 'mqtt.msg', 'mqtt.topic', 'mbtcp.trans_id'\n","]\n","df.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n","print(f\"âœ… Dropped identifier columns. New shape: {df.shape}\")\n","\n","# 4. Separate Features (X) and Target (y)\n","y = df['Attack_type'].copy()\n","X = df.drop(columns=[col for col in ['Attack_type', 'Attack_label'] if col in df.columns])\n","print(f\"âœ… Features (X) and target (y) separated.\")\n","\n","# 5. Encode the Target Variable (y)\n","print(\"\\n--- Encoding Target Variable 'Attack_type' ---\")\n","label_encoder = LabelEncoder()\n","y_encoded = label_encoder.fit_transform(y)\n","mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n","print(\"Attack Type Label Mapping:\")\n","for class_name, idx in mapping.items():\n","    print(f\"  - '{class_name}' -> {idx}\")\n","\n","# 6. Define Preprocessing Pipeline with ColumnTransformer\n","print(\"\\n--- Defining Preprocessing Logic ---\")\n","\n","# Identify column types based on their content\n","numeric_features = []\n","categorical_features = []\n","\n","for col in X.columns:\n","    if X[col].dtype == 'object':\n","        # If a column has few unique string values, treat it as categorical\n","        if X[col].nunique(dropna=True) <= 20:\n","            categorical_features.append(col)\n","        else:\n","            # Otherwise, attempt to convert it to a number (it's likely a messy numeric column)\n","            X[col] = pd.to_numeric(X[col], errors='coerce')\n","            numeric_features.append(col)\n","    else:\n","        numeric_features.append(col)\n","\n","print(f\"Identified {len(numeric_features)} numeric features.\")\n","print(f\"Identified {len(categorical_features)} categorical features.\")\n","\n","# Create the preprocessing pipelines for numeric and categorical data\n","numeric_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='median')),\n","    ('scaler', StandardScaler())\n","])\n","\n","categorical_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n","    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n","])\n","\n","# Combine preprocessing steps into a single ColumnTransformer\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', numeric_transformer, numeric_features),\n","        ('cat', categorical_transformer, categorical_features)\n","    ],\n","    remainder='passthrough' # Keep other columns (if any)\n",")\n","\n","print(\"âœ… Preprocessing pipeline defined successfully.\")\n","\n","# 7. Split Data into Train/Validation/Test (70/15/15)\n","print(\"\\n--- Splitting Data (70% train, 15% val, 15% test) ---\")\n","X_train, X_temp, y_train, y_temp = train_test_split(\n","    X, y_encoded, test_size=0.30, random_state=42, stratify=y_encoded\n",")\n","X_val, X_test, y_val, y_test = train_test_split(\n","    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",")\n","print(\"âœ… Data split complete.\")\n","\n","# 8. Apply the Preprocessing Pipeline\n","print(\"\\n--- Applying Preprocessing Pipeline to Data Splits ---\")\n","# Fit the preprocessor on the training data and transform it\n","X_train_processed = preprocessor.fit_transform(X_train)\n","\n","# Transform the validation and test data using the FITTED preprocessor\n","X_val_processed = preprocessor.transform(X_val)\n","X_test_processed = preprocessor.transform(X_test)\n","\n","# Get the new column names after transformation for readability (optional but recommended)\n","new_cols = preprocessor.get_feature_names_out()\n","X_train_scaled = pd.DataFrame(X_train_processed, columns=new_cols)\n","X_val_scaled = pd.DataFrame(X_val_processed, columns=new_cols)\n","X_test_scaled = pd.DataFrame(X_test_processed, columns=new_cols)\n","\n","print(f\"âœ… Scaling and encoding complete. Final shapes:\")\n","print(f\"  - X_train_scaled: {X_train_scaled.shape}\")\n","print(f\"  - X_val_scaled:   {X_val_scaled.shape}\")\n","print(f\"  - X_test_scaled:  {X_test_scaled.shape}\")\n","\n","# --- Final Preprocessing Notebook Code ---\n","\n","# --- 1. Imports and Setup (and all previous cells) ---\n","# ... (all your previous cells for loading, ColumnTransformer, splitting, etc. should be here) ...\n","# The script below represents the FINAL cell of your preprocessing notebook.\n","\n","# --- Final Step: Create, Verify, and Save a Light, Balanced Dataset ---\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import StratifiedShuffleSplit\n","from imblearn.pipeline import Pipeline as ImblearnPipeline\n","from imblearn.over_sampling import SMOTE\n","from imblearn.under_sampling import RandomUnderSampler\n","import joblib\n","import os\n","\n","print(\"\\n--- Applying Hybrid Sampling ---\")\n","\n","# This assumes X_train_scaled, y_train are available from previous cells\n","majority_class_label = 7\n","undersampler = RandomUnderSampler(sampling_strategy={majority_class_label: 200000}, random_state=42)\n","oversampler = SMOTE(random_state=42)\n","hybrid_pipeline = ImblearnPipeline(steps=[('under', undersampler), ('over', oversampler)])\n","\n","# The output of the imblearn pipeline can be a DataFrame/Series if the input was\n","X_train_resampled, y_train_resampled = hybrid_pipeline.fit_resample(X_train_scaled, y_train)\n","print(f\"âœ… Intermediate balanced dataset created with shape: {X_train_resampled.shape}\")\n","\n","print(\"\\n--- Creating the final light dataset ---\")\n","n_samples_light = 300000\n","sss = StratifiedShuffleSplit(n_splits=1, test_size=n_samples_light, random_state=42)\n","for _, light_index in sss.split(X_train_resampled, y_train_resampled):\n","    # --- THIS IS THE CORRECTED PART ---\n","    # Use .iloc for integer-location based indexing on pandas objects\n","    if isinstance(X_train_resampled, pd.DataFrame):\n","        X_train_final = X_train_resampled.iloc[light_index]\n","    else: # If it's a numpy array\n","        X_train_final = X_train_resampled[light_index]\n","\n","    if isinstance(y_train_resampled, pd.Series):\n","        y_train_final = y_train_resampled.iloc[light_index]\n","    else: # If it's a numpy array\n","        y_train_final = y_train_resampled[light_index]\n","\n","print(f\"âœ… Final light dataset created with shape: {X_train_final.shape}\")\n","\n","\n","# --- Data Validation Step ---\n","print(\"\\n--- Verifying Data Integrity Before Saving ---\")\n","is_all_nan = pd.DataFrame(X_train_final).isnull().all().all()\n","\n","if is_all_nan:\n","    raise ValueError(\"CRITICAL ERROR: The generated training data is all NaN. Halting before saving corrupted files.\")\n","else:\n","    print(\"âœ… Data integrity check passed. Proceeding to save.\")\n","\n","# --- Save Final Datasets ---\n","save_path = '/content/drive/MyDrive/Colab Notebooks/datasets/processed/'\n","os.makedirs(save_path, exist_ok=True)\n","print(f\"\\nðŸ’¾ Saving final processed data files to {save_path}...\")\n","# Note: We save X_val and X_test from the original split, not the resampled ones.\n","# The variable names here should match the ones from your data splitting cell.\n","# I am assuming they are X_val_scaled and X_test_scaled (numpy arrays).\n","X_val_to_save = globals().get('X_val_scaled', None)\n","X_test_to_save = globals().get('X_test_scaled', None)\n","\n","joblib.dump(X_train_final, os.path.join(save_path, 'X_train_resampled.joblib'))\n","joblib.dump(y_train_final, os.path.join(save_path, 'y_train_resampled.joblib'))\n","joblib.dump(X_val_to_save, os.path.join(save_path, 'X_val_scaled.joblib'))\n","joblib.dump(y_val, os.path.join(save_path, 'y_val.joblib'))\n","joblib.dump(X_test_to_save, os.path.join(save_path, 'X_test_scaled.joblib'))\n","joblib.dump(y_test, os.path.join(save_path, 'y_test.joblib'))\n","joblib.dump(label_encoder, os.path.join(save_path, 'label_encoder.joblib'))\n","\n","print(\"âœ… All files saved successfully.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8a4qF2m1KdiE","executionInfo":{"status":"ok","timestamp":1749302060622,"user_tz":-180,"elapsed":121841,"user":{"displayName":"Graduation project CIC 2025","userId":"06762630713984150762"}},"outputId":"19375863-44ca-48af-be82-434bb0d36f7a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","ðŸ”„ Loading datasets...\n","âœ… Data loaded and duplicates dropped. Shape: (2218387, 63)\n","âœ… Dropped identifier columns. New shape: (2218387, 47)\n","âœ… Features (X) and target (y) separated.\n","\n","--- Encoding Target Variable 'Attack_type' ---\n","Attack Type Label Mapping:\n","  - 'Backdoor' -> 0\n","  - 'DDoS_HTTP' -> 1\n","  - 'DDoS_ICMP' -> 2\n","  - 'DDoS_TCP' -> 3\n","  - 'DDoS_UDP' -> 4\n","  - 'Fingerprinting' -> 5\n","  - 'MITM' -> 6\n","  - 'Normal' -> 7\n","  - 'Password' -> 8\n","  - 'Port_Scanning' -> 9\n","  - 'Ransomware' -> 10\n","  - 'SQL_injection' -> 11\n","  - 'Uploading' -> 12\n","  - 'Vulnerability_scanner' -> 13\n","  - 'XSS' -> 14\n","\n","--- Defining Preprocessing Logic ---\n","Identified 41 numeric features.\n","Identified 4 categorical features.\n","âœ… Preprocessing pipeline defined successfully.\n","\n","--- Splitting Data (70% train, 15% val, 15% test) ---\n","âœ… Data split complete.\n","\n","--- Applying Preprocessing Pipeline to Data Splits ---\n","âœ… Scaling and encoding complete. Final shapes:\n","  - X_train_scaled: (1552870, 78)\n","  - X_val_scaled:   (332758, 78)\n","  - X_test_scaled:  (332759, 78)\n","\n","--- Applying Hybrid Sampling ---\n","âœ… Intermediate balanced dataset created with shape: (3000000, 78)\n","\n","--- Creating the final light dataset ---\n","âœ… Final light dataset created with shape: (300000, 78)\n","\n","--- Verifying Data Integrity Before Saving ---\n","âœ… Data integrity check passed. Proceeding to save.\n","\n","ðŸ’¾ Saving final processed data files to /content/drive/MyDrive/Colab Notebooks/datasets/processed/...\n","âœ… All files saved successfully.\n"]}]}]}