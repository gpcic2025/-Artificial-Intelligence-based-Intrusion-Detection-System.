{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V28","mount_file_id":"1aUJ5xZqt9d8YitfIH41swoAG1D-cnqSi","authorship_tag":"ABX9TyMfjPUrhjZejvpW3D0figS6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","source":["!pip install imbalanced-learn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RfncCAVGM3oV","executionInfo":{"status":"ok","timestamp":1749251923384,"user_tz":-180,"elapsed":2026,"user":{"displayName":"Graduation project CIC 2025","userId":"06762630713984150762"}},"outputId":"b4dae470-f5da-43c3-9326-db5d54375edb"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting imbalanced-learn\n","  Downloading imbalanced_learn-0.13.0-py3-none-any.whl.metadata (8.8 kB)\n","Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (2.0.2)\n","Requirement already satisfied: scipy<2,>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.15.3)\n","Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.6.1)\n","Collecting sklearn-compat<1,>=0.1 (from imbalanced-learn)\n","  Downloading sklearn_compat-0.1.3-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.5.1)\n","Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (3.6.0)\n","Downloading imbalanced_learn-0.13.0-py3-none-any.whl (238 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.4/238.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sklearn_compat-0.1.3-py3-none-any.whl (18 kB)\n","Installing collected packages: sklearn-compat, imbalanced-learn\n","Successfully installed imbalanced-learn-0.13.0 sklearn-compat-0.1.3\n"]}]},{"cell_type":"code","source":["# 1. Setup: Import Libraries and Mount Google Drive\n","import pandas as pd\n","import numpy as np\n","import os\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.impute import SimpleImputer\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from imblearn.over_sampling import SMOTE\n","from sklearn.preprocessing import LabelEncoder # Explicitly import LabelEncoder\n","\n","# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# 2. Load DataFrames and Align Columns\n","print(\"🔄 Loading datasets...\")\n","data_path = '/content/drive/MyDrive/Colab Notebooks/datasets/'\n","file1_path = os.path.join(data_path, 'DNN-EdgeIIoT-dataset.csv')\n","file2_path = os.path.join(data_path, 'ML-EdgeIIoT-dataset.csv')\n","\n","df1 = pd.read_csv(file1_path, low_memory=False)\n","df2 = pd.read_csv(file2_path, low_memory=False)\n","\n","common_cols = list(set(df1.columns).intersection(set(df2.columns)))\n","df = pd.concat([df1[common_cols], df2[common_cols]], ignore_index=True)\n","df.drop_duplicates(inplace=True)\n","print(f\"✅ Data loaded and duplicates dropped. Shape: {df.shape}\")\n","\n","# 3. Drop Unnecessary/Identifier Columns\n","columns_to_drop = [\n","    'frame.time', 'ip.src_host', 'ip.dst_host', 'arp.src.proto_ipv4',\n","    'arp.dst.proto_ipv4', 'http.file_data', 'http.request.uri.query',\n","    'http.referer', 'http.request.full_uri', 'tcp.options', 'tcp.payload',\n","    'dns.qry.name', 'dns.qry.name.len', 'mqtt.msg', 'mqtt.topic', 'mbtcp.trans_id'\n","]\n","df.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n","print(f\"✅ Dropped identifier columns. New shape: {df.shape}\")\n","\n","# 4. Separate Features (X) and Target (y)\n","y = df['Attack_type'].copy()\n","X = df.drop(columns=[col for col in ['Attack_type', 'Attack_label'] if col in df.columns])\n","print(f\"✅ Features (X) and target (y) separated.\")\n","\n","# 5. Encode the Target Variable (y)\n","print(\"\\n--- Encoding Target Variable 'Attack_type' ---\")\n","label_encoder = LabelEncoder()\n","y_encoded = label_encoder.fit_transform(y)\n","mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n","print(\"Attack Type Label Mapping:\")\n","for class_name, idx in mapping.items():\n","    print(f\"  - '{class_name}' -> {idx}\")\n","\n","# 6. Define Preprocessing Pipeline with ColumnTransformer\n","print(\"\\n--- Defining Preprocessing Logic ---\")\n","\n","# Identify column types based on their content\n","numeric_features = []\n","categorical_features = []\n","\n","for col in X.columns:\n","    if X[col].dtype == 'object':\n","        # If a column has few unique string values, treat it as categorical\n","        if X[col].nunique(dropna=True) <= 20:\n","            categorical_features.append(col)\n","        else:\n","            # Otherwise, attempt to convert it to a number (it's likely a messy numeric column)\n","            X[col] = pd.to_numeric(X[col], errors='coerce')\n","            numeric_features.append(col)\n","    else:\n","        numeric_features.append(col)\n","\n","print(f\"Identified {len(numeric_features)} numeric features.\")\n","print(f\"Identified {len(categorical_features)} categorical features.\")\n","\n","# Create the preprocessing pipelines for numeric and categorical data\n","numeric_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='median')),\n","    ('scaler', StandardScaler())\n","])\n","\n","categorical_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n","    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n","])\n","\n","# Combine preprocessing steps into a single ColumnTransformer\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', numeric_transformer, numeric_features),\n","        ('cat', categorical_transformer, categorical_features)\n","    ],\n","    remainder='passthrough' # Keep other columns (if any)\n",")\n","\n","print(\"✅ Preprocessing pipeline defined successfully.\")\n","\n","# 7. Split Data into Train/Validation/Test (70/15/15)\n","print(\"\\n--- Splitting Data (70% train, 15% val, 15% test) ---\")\n","X_train, X_temp, y_train, y_temp = train_test_split(\n","    X, y_encoded, test_size=0.30, random_state=42, stratify=y_encoded\n",")\n","X_val, X_test, y_val, y_test = train_test_split(\n","    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",")\n","print(\"✅ Data split complete.\")\n","\n","# 8. Apply the Preprocessing Pipeline\n","print(\"\\n--- Applying Preprocessing Pipeline to Data Splits ---\")\n","# Fit the preprocessor on the training data and transform it\n","X_train_processed = preprocessor.fit_transform(X_train)\n","\n","# Transform the validation and test data using the FITTED preprocessor\n","X_val_processed = preprocessor.transform(X_val)\n","X_test_processed = preprocessor.transform(X_test)\n","\n","# Get the new column names after transformation for readability (optional but recommended)\n","new_cols = preprocessor.get_feature_names_out()\n","X_train_scaled = pd.DataFrame(X_train_processed, columns=new_cols)\n","X_val_scaled = pd.DataFrame(X_val_processed, columns=new_cols)\n","X_test_scaled = pd.DataFrame(X_test_processed, columns=new_cols)\n","\n","print(f\"✅ Scaling and encoding complete. Final shapes:\")\n","print(f\"  - X_train_scaled: {X_train_scaled.shape}\")\n","print(f\"  - X_val_scaled:   {X_val_scaled.shape}\")\n","print(f\"  - X_test_scaled:  {X_test_scaled.shape}\")\n","\n","\n","# 9. Final Preprocessing: Create and Save a Light, Balanced Dataset\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import StratifiedShuffleSplit\n","from imblearn.pipeline import Pipeline as ImblearnPipeline\n","from imblearn.over_sampling import SMOTE\n","from imblearn.under_sampling import RandomUnderSampler\n","import joblib\n","import os\n","\n","print(\"\\n--- Applying Hybrid Sampling ---\")\n","\n","# Define and apply the hybrid sampling pipeline\n","majority_class_label = 7 # 'Normal' class\n","undersampler = RandomUnderSampler(sampling_strategy={majority_class_label: 200000}, random_state=42)\n","oversampler = SMOTE(random_state=42)\n","hybrid_pipeline = ImblearnPipeline(steps=[('under', undersampler), ('over', oversampler)])\n","X_train_resampled, y_train_resampled = hybrid_pipeline.fit_resample(X_train_scaled, y_train)\n","print(f\"✅ Intermediate balanced dataset created with shape: {X_train_resampled.shape}\")\n","\n","\n","# --- Create the Final \"Light\" Version ---\n","print(\"\\n--- Creating the final light dataset (300,000 samples) ---\")\n","n_samples_light = 300000\n","sss = StratifiedShuffleSplit(n_splits=1, test_size=n_samples_light, random_state=42)\n","\n","for _, light_index in sss.split(X_train_resampled, y_train_resampled):\n","    X_train_final = X_train_resampled.iloc[light_index] if isinstance(X_train_resampled, pd.DataFrame) else X_train_resampled[light_index]\n","    y_train_final = y_train_resampled[light_index]\n","\n","print(f\"✅ Final light dataset created with shape: {X_train_final.shape}\")\n","\n","\n","# --- Free Up Memory ---\n","# Delete the large intermediate dataset to free up RAM immediately\n","del X_train_resampled\n","del y_train_resampled\n","print(\"✅ Memory freed by deleting the large intermediate dataset.\")\n","\n","\n","# --- Save Final Datasets ---\n","# The light version will be saved as the main file, replacing any older, heavy versions.\n","save_path = '/content/drive/MyDrive/Colab Notebooks/datasets/processed/'\n","os.makedirs(save_path, exist_ok=True)\n","print(f\"\\n💾 Saving final processed data files to {save_path}...\")\n","\n","# Save the LIGHT training data with the standard filename\n","joblib.dump(X_train_final, os.path.join(save_path, 'X_train_resampled.joblib'))\n","joblib.dump(y_train_final, os.path.join(save_path, 'y_train_resampled.joblib'))\n","\n","# Save the validation, test, and encoder files as before\n","joblib.dump(X_val_scaled, os.path.join(save_path, 'X_val_scaled.joblib'))\n","joblib.dump(y_val, os.path.join(save_path, 'y_val.joblib'))\n","joblib.dump(X_test_scaled, os.path.join(save_path, 'X_test_scaled.joblib'))\n","joblib.dump(y_test, os.path.join(save_path, 'y_test.joblib'))\n","joblib.dump(label_encoder, os.path.join(save_path, 'label_encoder.joblib'))\n","\n","print(\"✅ All files saved successfully. The main training data is now the lighter version.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8a4qF2m1KdiE","executionInfo":{"status":"ok","timestamp":1749252065979,"user_tz":-180,"elapsed":139191,"user":{"displayName":"Graduation project CIC 2025","userId":"06762630713984150762"}},"outputId":"ad9f518b-6422-4307-f2dc-66390fd08673"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","🔄 Loading datasets...\n","✅ Data loaded and duplicates dropped. Shape: (2218387, 63)\n","✅ Dropped identifier columns. New shape: (2218387, 47)\n","✅ Features (X) and target (y) separated.\n","\n","--- Encoding Target Variable 'Attack_type' ---\n","Attack Type Label Mapping:\n","  - 'Backdoor' -> 0\n","  - 'DDoS_HTTP' -> 1\n","  - 'DDoS_ICMP' -> 2\n","  - 'DDoS_TCP' -> 3\n","  - 'DDoS_UDP' -> 4\n","  - 'Fingerprinting' -> 5\n","  - 'MITM' -> 6\n","  - 'Normal' -> 7\n","  - 'Password' -> 8\n","  - 'Port_Scanning' -> 9\n","  - 'Ransomware' -> 10\n","  - 'SQL_injection' -> 11\n","  - 'Uploading' -> 12\n","  - 'Vulnerability_scanner' -> 13\n","  - 'XSS' -> 14\n","\n","--- Defining Preprocessing Logic ---\n","Identified 41 numeric features.\n","Identified 4 categorical features.\n","✅ Preprocessing pipeline defined successfully.\n","\n","--- Splitting Data (70% train, 15% val, 15% test) ---\n","✅ Data split complete.\n","\n","--- Applying Preprocessing Pipeline to Data Splits ---\n","✅ Scaling and encoding complete. Final shapes:\n","  - X_train_scaled: (1552870, 78)\n","  - X_val_scaled:   (332758, 78)\n","  - X_test_scaled:  (332759, 78)\n","\n","--- Applying Hybrid Sampling ---\n","✅ Intermediate balanced dataset created with shape: (3000000, 78)\n","\n","--- Creating the final light dataset (300,000 samples) ---\n","✅ Final light dataset created with shape: (300000, 78)\n","✅ Memory freed by deleting the large intermediate dataset.\n","\n","💾 Saving final processed data files to /content/drive/MyDrive/Colab Notebooks/datasets/processed/...\n","✅ All files saved successfully. The main training data is now the lighter version.\n"]}]},{"cell_type":"code","source":["import joblib\n","\n","# Define the path on your Google Drive where you want to save the files\n","save_path = '/content/drive/MyDrive/Colab Notebooks/datasets/processed/'\n","\n","# Create the directory if it doesn't exist\n","os.makedirs(save_path, exist_ok=True)\n","joblib.dump(X_train_final, os.path.join(save_path, 'X_train_resampled.joblib'))\n","joblib.dump(y_train_final, os.path.join(save_path, 'y_train_resampled.joblib'))\n","\n","print(f\"💾 Saving processed data to {save_path}...\")\n","joblib.dump(X_val_scaled, os.path.join(save_path, 'X_val_scaled.joblib'))\n","joblib.dump(y_val, os.path.join(save_path, 'y_val.joblib'))\n","joblib.dump(X_test_scaled, os.path.join(save_path, 'X_test_scaled.joblib'))\n","joblib.dump(y_test, os.path.join(save_path, 'y_test.joblib'))\n","joblib.dump(label_encoder, os.path.join(save_path, 'label_encoder.joblib'))\n","\n","print(\"✅ All files saved successfully. The main training data is now the lighter version.\")\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OtkCntkdPhpS","executionInfo":{"status":"ok","timestamp":1749252289442,"user_tz":-180,"elapsed":5586,"user":{"displayName":"Graduation project CIC 2025","userId":"06762630713984150762"}},"outputId":"c084b9ab-d0f5-45cb-db0b-39085708c01d"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["💾 Saving processed data to /content/drive/MyDrive/Colab Notebooks/datasets/processed/...\n","✅ All files saved successfully. The main training data is now the lighter version.\n"]}]}]}