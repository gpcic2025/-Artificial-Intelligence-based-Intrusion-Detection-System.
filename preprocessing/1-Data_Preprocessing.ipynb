{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V28","mount_file_id":"1aUJ5xZqt9d8YitfIH41swoAG1D-cnqSi","authorship_tag":"ABX9TyNtt/QBWRRw5LJQO/1P1Idb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","source":["!pip install imbalanced-learn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RfncCAVGM3oV","executionInfo":{"status":"ok","timestamp":1749246290667,"user_tz":-180,"elapsed":1927,"user":{"displayName":"Graduation project CIC 2025","userId":"06762630713984150762"}},"outputId":"fcdcb231-d820-4ab8-8f1f-a05cf49e4366"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting imbalanced-learn\n","  Downloading imbalanced_learn-0.13.0-py3-none-any.whl.metadata (8.8 kB)\n","Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (2.0.2)\n","Requirement already satisfied: scipy<2,>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.15.3)\n","Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.6.1)\n","Collecting sklearn-compat<1,>=0.1 (from imbalanced-learn)\n","  Downloading sklearn_compat-0.1.3-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.5.1)\n","Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (3.6.0)\n","Downloading imbalanced_learn-0.13.0-py3-none-any.whl (238 kB)\n","\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/238.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”\u001b[0m \u001b[32m204.8/238.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m238.4/238.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sklearn_compat-0.1.3-py3-none-any.whl (18 kB)\n","Installing collected packages: sklearn-compat, imbalanced-learn\n","Successfully installed imbalanced-learn-0.13.0 sklearn-compat-0.1.3\n"]}]},{"cell_type":"code","source":["# 1. Setup: Import Libraries and Mount Google Drive\n","import pandas as pd\n","import numpy as np\n","import os\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.impute import SimpleImputer\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from imblearn.over_sampling import SMOTE\n","from sklearn.preprocessing import LabelEncoder # Explicitly import LabelEncoder\n","\n","# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# 2. Load DataFrames and Align Columns\n","print(\"ðŸ”„ Loading datasets...\")\n","data_path = '/content/drive/MyDrive/Colab Notebooks/datasets/'\n","file1_path = os.path.join(data_path, 'DNN-EdgeIIoT-dataset.csv')\n","file2_path = os.path.join(data_path, 'ML-EdgeIIoT-dataset.csv')\n","\n","df1 = pd.read_csv(file1_path, low_memory=False)\n","df2 = pd.read_csv(file2_path, low_memory=False)\n","\n","common_cols = list(set(df1.columns).intersection(set(df2.columns)))\n","df = pd.concat([df1[common_cols], df2[common_cols]], ignore_index=True)\n","df.drop_duplicates(inplace=True)\n","print(f\"âœ… Data loaded and duplicates dropped. Shape: {df.shape}\")\n","\n","# 3. Drop Unnecessary/Identifier Columns\n","columns_to_drop = [\n","    'frame.time', 'ip.src_host', 'ip.dst_host', 'arp.src.proto_ipv4',\n","    'arp.dst.proto_ipv4', 'http.file_data', 'http.request.uri.query',\n","    'http.referer', 'http.request.full_uri', 'tcp.options', 'tcp.payload',\n","    'dns.qry.name', 'dns.qry.name.len', 'mqtt.msg', 'mqtt.topic', 'mbtcp.trans_id'\n","]\n","df.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n","print(f\"âœ… Dropped identifier columns. New shape: {df.shape}\")\n","\n","# 4. Separate Features (X) and Target (y)\n","y = df['Attack_type'].copy()\n","X = df.drop(columns=[col for col in ['Attack_type', 'Attack_label'] if col in df.columns])\n","print(f\"âœ… Features (X) and target (y) separated.\")\n","\n","# 5. Encode the Target Variable (y)\n","print(\"\\n--- Encoding Target Variable 'Attack_type' ---\")\n","label_encoder = LabelEncoder()\n","y_encoded = label_encoder.fit_transform(y)\n","mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n","print(\"Attack Type Label Mapping:\")\n","for class_name, idx in mapping.items():\n","    print(f\"  - '{class_name}' -> {idx}\")\n","\n","# 6. Define Preprocessing Pipeline with ColumnTransformer\n","print(\"\\n--- Defining Preprocessing Logic ---\")\n","\n","# Identify column types based on their content\n","numeric_features = []\n","categorical_features = []\n","\n","for col in X.columns:\n","    if X[col].dtype == 'object':\n","        # If a column has few unique string values, treat it as categorical\n","        if X[col].nunique(dropna=True) <= 20:\n","            categorical_features.append(col)\n","        else:\n","            # Otherwise, attempt to convert it to a number (it's likely a messy numeric column)\n","            X[col] = pd.to_numeric(X[col], errors='coerce')\n","            numeric_features.append(col)\n","    else:\n","        numeric_features.append(col)\n","\n","print(f\"Identified {len(numeric_features)} numeric features.\")\n","print(f\"Identified {len(categorical_features)} categorical features.\")\n","\n","# Create the preprocessing pipelines for numeric and categorical data\n","numeric_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='median')),\n","    ('scaler', StandardScaler())\n","])\n","\n","categorical_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n","    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n","])\n","\n","# Combine preprocessing steps into a single ColumnTransformer\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', numeric_transformer, numeric_features),\n","        ('cat', categorical_transformer, categorical_features)\n","    ],\n","    remainder='passthrough' # Keep other columns (if any)\n",")\n","\n","print(\"âœ… Preprocessing pipeline defined successfully.\")\n","\n","# 7. Split Data into Train/Validation/Test (70/15/15)\n","print(\"\\n--- Splitting Data (70% train, 15% val, 15% test) ---\")\n","X_train, X_temp, y_train, y_temp = train_test_split(\n","    X, y_encoded, test_size=0.30, random_state=42, stratify=y_encoded\n",")\n","X_val, X_test, y_val, y_test = train_test_split(\n","    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",")\n","print(\"âœ… Data split complete.\")\n","\n","# 8. Apply the Preprocessing Pipeline\n","print(\"\\n--- Applying Preprocessing Pipeline to Data Splits ---\")\n","# Fit the preprocessor on the training data and transform it\n","X_train_processed = preprocessor.fit_transform(X_train)\n","\n","# Transform the validation and test data using the FITTED preprocessor\n","X_val_processed = preprocessor.transform(X_val)\n","X_test_processed = preprocessor.transform(X_test)\n","\n","# Get the new column names after transformation for readability (optional but recommended)\n","new_cols = preprocessor.get_feature_names_out()\n","X_train_scaled = pd.DataFrame(X_train_processed, columns=new_cols)\n","X_val_scaled = pd.DataFrame(X_val_processed, columns=new_cols)\n","X_test_scaled = pd.DataFrame(X_test_processed, columns=new_cols)\n","\n","print(f\"âœ… Scaling and encoding complete. Final shapes:\")\n","print(f\"  - X_train_scaled: {X_train_scaled.shape}\")\n","print(f\"  - X_val_scaled:   {X_val_scaled.shape}\")\n","print(f\"  - X_test_scaled:  {X_test_scaled.shape}\")\n","\n","\n","# 9. Apply SMOTE on Training Data Only\n","print(\"\\n--- Applying SMOTE for Class Imbalance on Training Set ---\")\n","print(\"Class distribution before SMOTE:\")\n","unique_train, counts_train = np.unique(y_train, return_counts=True)\n","for cls_idx, count in zip(unique_train, counts_train):\n","    cls_name = label_encoder.inverse_transform([cls_idx])[0]\n","    print(f\"  - {cls_name}: {count} samples\")\n","\n","smote = SMOTE(random_state=42)\n","X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n","\n","print(\"\\nClass distribution after SMOTE:\")\n","unique_res, counts_res = np.unique(y_train_resampled, return_counts=True)\n","for cls_idx, count in zip(unique_res, counts_res):\n","    cls_name = label_encoder.inverse_transform([cls_idx])[0]\n","    print(f\"  - {cls_name}: {count} samples\")\n","\n","print(f\"\\nâœ… Preprocessing Complete!\")\n","print(\"You now have:\")\n","print(\"  â€¢ X_train_resampled   (resampled & scaled training features)\")\n","print(\"  â€¢ y_train_resampled   (resampled training labels)\")\n","print(\"  â€¢ X_val_scaled        (scaled validation features)\")\n","print(\"  â€¢ y_val               (validation labels)\")\n","print(\"  â€¢ X_test_scaled       (scaled test features)\")\n","print(\"  â€¢ y_test              (test labels)\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8a4qF2m1KdiE","executionInfo":{"status":"ok","timestamp":1749246475210,"user_tz":-180,"elapsed":162954,"user":{"displayName":"Graduation project CIC 2025","userId":"06762630713984150762"}},"outputId":"9772df61-88fc-4c25-f2fa-24cb65381ad9"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","ðŸ”„ Loading datasets...\n","âœ… Data loaded and duplicates dropped. Shape: (2218387, 63)\n","âœ… Dropped identifier columns. New shape: (2218387, 47)\n","âœ… Features (X) and target (y) separated.\n","\n","--- Encoding Target Variable 'Attack_type' ---\n","Attack Type Label Mapping:\n","  - 'Backdoor' -> 0\n","  - 'DDoS_HTTP' -> 1\n","  - 'DDoS_ICMP' -> 2\n","  - 'DDoS_TCP' -> 3\n","  - 'DDoS_UDP' -> 4\n","  - 'Fingerprinting' -> 5\n","  - 'MITM' -> 6\n","  - 'Normal' -> 7\n","  - 'Password' -> 8\n","  - 'Port_Scanning' -> 9\n","  - 'Ransomware' -> 10\n","  - 'SQL_injection' -> 11\n","  - 'Uploading' -> 12\n","  - 'Vulnerability_scanner' -> 13\n","  - 'XSS' -> 14\n","\n","--- Defining Preprocessing Logic ---\n","Identified 41 numeric features.\n","Identified 4 categorical features.\n","âœ… Preprocessing pipeline defined successfully.\n","\n","--- Splitting Data (70% train, 15% val, 15% test) ---\n","âœ… Data split complete.\n","\n","--- Applying Preprocessing Pipeline to Data Splits ---\n","âœ… Scaling and encoding complete. Final shapes:\n","  - X_train_scaled: (1552870, 78)\n","  - X_val_scaled:   (332758, 78)\n","  - X_test_scaled:  (332759, 78)\n","\n","--- Applying SMOTE for Class Imbalance on Training Set ---\n","Class distribution before SMOTE:\n","  - Backdoor: 17403 samples\n","  - DDoS_HTTP: 34938 samples\n","  - DDoS_ICMP: 81505 samples\n","  - DDoS_TCP: 35043 samples\n","  - DDoS_UDP: 85097 samples\n","  - Fingerprinting: 701 samples\n","  - MITM: 280 samples\n","  - Normal: 1130950 samples\n","  - Password: 35107 samples\n","  - Port_Scanning: 15795 samples\n","  - Ransomware: 7648 samples\n","  - SQL_injection: 35842 samples\n","  - Uploading: 26344 samples\n","  - Vulnerability_scanner: 35077 samples\n","  - XSS: 11140 samples\n","\n","Class distribution after SMOTE:\n","  - Backdoor: 1130950 samples\n","  - DDoS_HTTP: 1130950 samples\n","  - DDoS_ICMP: 1130950 samples\n","  - DDoS_TCP: 1130950 samples\n","  - DDoS_UDP: 1130950 samples\n","  - Fingerprinting: 1130950 samples\n","  - MITM: 1130950 samples\n","  - Normal: 1130950 samples\n","  - Password: 1130950 samples\n","  - Port_Scanning: 1130950 samples\n","  - Ransomware: 1130950 samples\n","  - SQL_injection: 1130950 samples\n","  - Uploading: 1130950 samples\n","  - Vulnerability_scanner: 1130950 samples\n","  - XSS: 1130950 samples\n","\n","âœ… Preprocessing Complete!\n","You now have:\n","  â€¢ X_train_resampled   (resampled & scaled training features)\n","  â€¢ y_train_resampled   (resampled training labels)\n","  â€¢ X_val_scaled        (scaled validation features)\n","  â€¢ y_val               (validation labels)\n","  â€¢ X_test_scaled       (scaled test features)\n","  â€¢ y_test              (test labels)\n"]}]},{"cell_type":"code","source":["import joblib\n","\n","# Define the path on your Google Drive where you want to save the files\n","save_path = '/content/drive/MyDrive/Colab Notebooks/datasets/processed/'\n","\n","# Create the directory if it doesn't exist\n","os.makedirs(save_path, exist_ok=True)\n","\n","print(f\"ðŸ’¾ Saving processed data to {save_path}...\")\n","\n","# Save each variable to a separate file using joblib\n","# We use joblib because it's highly efficient for large numpy arrays\n","joblib.dump(X_train_resampled, os.path.join(save_path, 'X_train_resampled.joblib'))\n","joblib.dump(y_train_resampled, os.path.join(save_path, 'y_train_resampled.joblib'))\n","joblib.dump(X_val_scaled, os.path.join(save_path, 'X_val_scaled.joblib'))\n","joblib.dump(y_val, os.path.join(save_path, 'y_val.joblib'))\n","joblib.dump(X_test_scaled, os.path.join(save_path, 'X_test_scaled.joblib'))\n","joblib.dump(y_test, os.path.join(save_path, 'y_test.joblib'))\n","joblib.dump(label_encoder, os.path.join(save_path, 'label_encoder.joblib')) # Also save the encoder!\n","\n","print(\"âœ… All processed data files have been saved successfully.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OtkCntkdPhpS","executionInfo":{"status":"ok","timestamp":1749247021473,"user_tz":-180,"elapsed":48741,"user":{"displayName":"Graduation project CIC 2025","userId":"06762630713984150762"}},"outputId":"8e7b1945-419b-4d6a-86ec-a89cc79211a5"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["ðŸ’¾ Saving processed data to /content/drive/MyDrive/Colab Notebooks/datasets/processed/...\n","âœ… All processed data files have been saved successfully.\n"]}]}]}